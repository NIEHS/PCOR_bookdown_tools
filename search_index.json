[["index.html", "The CHORDS Toolkit for Health and Geospatial Exposures Research Introduction About CHORDS About This Toolkit", " The CHORDS Toolkit for Health and Geospatial Exposures Research Last Modified: August 08, 2024 (Version 1.0) Introduction Researchers interested in studying the health impacts of climate change and climate related disasters need access to relevant, timely, and harmonized data on environmental exposures, social determinants of health variables, and health outcomes. However, such datasets are often developed for different purposes, reside in multiple locations, and require linkage. The Climate and Health Outcomes Research Data Systems (CHORDS) Project seeks to connect researchers to environmental and health datasets with a toolkit that provides guides, tutorials, and example code to improve integration of geospatial data-based exposures and health data and records into their research. About CHORDS The Climate and Health Outcomes Research Data Systems (CHORDS) program provides resources aimed at making it easier for researchers to study the effects of place-based environmental exposures on health outcomes. The CHORDS resources include a web-based data catalog, standardized data sets, and this toolkit. About This Toolkit This toolkit provides guides, tools, and example code in R. The CHORDS Toolkit GitHub repository provides the underlying code and data for this book. The CHORDS toolkit has been developed to support different types of users, such as students, clinicians, and data managers. Please see the User Profile Appendix for descriptions and suggested relevant toolkit chapters for each user profile. This toolkit requires familiarity with the R programming language and the use of R for working with, visualizing, and analyzing scientific data. Please see the Getting Started chapter for a list of resources for getting started with R for geospatial data analysis in environmental health research. The toolkit consists of a series of chapters organized into the following units: Geospatial Data Foundations: This unit provides background, guidance and example code for working with different types of geospatial data common in environmental health research. This unit is intended as a starting point for users with less familiarity with geospatial data and geospatial analysis methods in environmental health using R. Wildfire Data: This unit provides guidance for working with different types of wildfire-related data in climate change and health research. Other Environmental Data: This unit provides guidance and code for working with specific sources of environmental data common in environmental health research for characterizing environmental exposures as well as social determinants of health. Health Data Integration: This unit provides guidance and code for integrating environmental and health data in climate change and health research, both at the individual level and at the population level. Case Studies: This unit provides example case studies that analyze integrated wildfire-related data and other environmental exposures data with health outcomes data. The CHORDS Toolkit is a work in progress. Please see the CHORDS Toolkit GitHub repository for a list of chapters currently in development. This is a BETA Release. Please let us know of any improvements we can make. Funding This resource was supported by the Department of Health and Human Services (DHHS) Office of the Assistant Secretary for Planning and Evaluation (ASPE) Office of the Secretary’s Patient Centered Outcomes Research Trust Fund (OS-PCORTF) and by the National Institutes of Health (NIH) from the National Institute of Environmental Health Sciences (NIEHS) and the NIH Office of Data Science Strategy (ODSS). "],["unit-geospatial-foundations.html", "Geospatial Data Foundations", " Geospatial Data Foundations This unit provides background, guidance and example code for working with different types of geospatial data common in environmental health research. This unit is intended as a starting point for users with less familiarity with geospatial data and geospatial analysis methods in environmental health using R. "],["chapter-getting-started-spatial.html", "1 Getting Started 1.1 Introduction 1.2 Concepts and Terminology 1.3 Datasets 1.4 R Packages 1.5 Resources", " 1 Getting Started Getting Started with Geospatial Data Analysis in Environmental Health Using R Date Modified: June 25, 2024 Authors: Mitchell Manware , Lara P. Clark , Kyle P. Messier Key Terms: Environmental Health, Geospatial Data Programming Language: R 1.1 Introduction 1.1.1 Motivation Environmental health research relies on various types of data to accurately measure, model, and predict exposures. Environmental data are often spatial (related to the surface of the Earth), temporal (related to a specific time or period of time), or spatio-temporal (related to the surface of the Earth for a specific time or period of time). Here, the term geospatial will be used to refer to spatial and spatio-temporal data. These data are at the core of environmental health research, but the steps between identifying a geospatial data set or variable and using it to help answer a research question can be challenging. 1.1.2 Objectives The objectives of this chapter are to: Introduce concepts and terminology used throughout the CHORDS Toolkit for geospatial data and geospatial analysis methods. Describe the geospatial datasets and R packages used in the following tutorials. Provide a list of useful resources for getting started with R and for further exploration of geospatial data analysis methods in environmental health. The following chapters in this unit will demonstrate how to use R to access, prepare, and analyze different types of geospatial data that are commonly used in environmental health applications. The tutorials will focus primarily on spatial data, but some aspects of temporal and spatio-temporal data will also be discussed. 1.2 Concepts and Terminology 1.2.1 Spatial Geometry The spatial geometry of a geospatial dataset is an important consideration in data analysis pipelines. There are three main spatial geometry types: point, line, and area (i.e., polygon or grid). Points are represented by geographic coordinates (latitude and longitude pairs), lines by a series of connected points, and polygons by a series of connected points that completely enclose and define an area. In contrast to polygons, which can define irregular or non-uniform areas, grids define regular and uniform areas (e.g., such that each grid cell has the same area). Point, line, and polygon data is referred to as vector data, and grid data is referred to as raster data. For detailed descriptions of vector data, raster data, and the differences between them, respectively, see (1), (2) and (3). The following table illustrates common examples of each spatial geometry type used in environmental health applications. Spatial Geometry Types Type Illustration Examples Tutorials Point (Vector) Air pollution monitors, Weather stations, Patient geocoded addresses, Healthcare facility coordinates Point Data Line (Vector) Roads, Commute routes Polygon (Vector) Wildfire smoke plumes, Census boundaries Polygon Data Grid (Raster) Land cover imagery from satellites, Meteorological model output, Gridded population counts Raster Data The tutorials linked in the table above demonstrate exploratory analyses with each spatial geometry data type. 1.2.2 Coordinate Reference Systems and Projections Coordinate reference systems (CRS) are important for spatial analyses as they define how spatial data align with the Earth’s surface (4). Transforming (projecting) the data to a different CRS may be necessary when combining multiple datasets or creating visuals for particular areas of interest. It is important to note that transforming spatial data can cause distortions in its area, direction, distance, or shape (4). The direction and magnitude of these distortions vary depending on the chosen CRS, area of interest, and type of data (5). For guidance on selecting an appropriate CRS based on the data, area of interest, and analysis goals, see (6,7). 1.3 Datasets The tutorials in this unit demonstrate the use of geospatial data using the following publicly available datasets: Data Provider Dataset Type Environmental Protection Agency (EPA) PM2.5 Daily Observations Point National Oceanic and Atmospheric Administration (NOAA) Wildfire Smoke Plumes Polygon United States Census Bureau United States Cartographic Boundary Polygon National Oceanic and Atmospheric Administration (NOAA) Land Surface Temperature Raster 1.4 R Packages The tutorials in this unit demonstrate the use of the following R packages: Spatial Analysis: sf (1,8), terra (9), tidyterra (10) Data Visualization: ggplot2 (11), ggpubr (12) Data Processing: dplyr (13) The following code installs and imports the packages used in this unit: Installing and importing new packages may required R to restart. vignette_packages &lt;- c( &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;ggpubr&quot;, &quot;sf&quot;, &quot;terra&quot;, &quot;tidyterra&quot;, &quot;utils&quot; ) for (v in seq_along(vignette_packages)) { if (vignette_packages[v] %in% installed.packages() == FALSE) { install.packages(vignette_packages[v]) } } library(dplyr) library(ggplot2) library(ggpubr) library(sf) library(terra) library(tidyterra) library(utils) 1.5 Resources This section highlights resources for getting started with R, geospatial data analysis, and/or climate change and human health related research methods. The BUSPH-HSPH Climate Change and Health Research Coordinating Center (CAFÉ) provides training and education materials for climate change and human health research in different formats for various types of users. The Climate CAFÉ Tutorials and Code Walkthroughs demonstrate geospatial data management and analysis in climate change and human health research using R. CAFÉ also provides a series of video tutorials demonstrating the use of geographic information systems (GIS) in environmental health and a list of educational materials on climate and health. The inTelligence And Machine lEarning (TAME) Toolkit provides tutorials for data generation, management, and analysis in environmental health research using R. The TAME Toolkit Chapter 1 includes a guide for installing and getting started with R and an introduction to data science methods in R. The TAME Toolkit also includes tutorials with R code demonstrating geospatial data analysis methods in environmental health (e.g., Chapter 3.3). The IPUMS DHS Climate Change and Health Research Hub provides tutorials with code in R demonstrating use of various climate and health datasets and analysis methods. IPUMS also provides a guide to installing and setting up R for use in climate change and health research. The book Geocomputation with R provides resources for geospatial data analysis, visualization, and modeling with R. This book provides tutorials and examples from various disciplines that use geospatial data (e.g., transportation, ecology). This book covers introductory through advanced topics. References "],["chapter-point-data.html", "2 Point Data 2.1 Introduction 2.2 Access, Download, and Unzip 2.3 Data Preparation 2.4 Exploratory Analysis", " 2 Point Data Point Data Access, Preparation, and Exploratory Analysis in R Date Modified: May 6, 2024 Authors: Mitchell Manware , Kyle P. Messier Key Terms: Air Pollution, Geospatial Data, Particulate Matter Programming Language: R 2.1 Introduction Air pollution monitoring data from the United States Environmental Protection Agency (EPA) will be used to demonstrate using point data with the sf package (1). This tutorial will demonstrate the following steps with point data in R: Downloading data from a URL Importing data Checking data type, structure, and class Reclassifying data Computing summary and zonal statistics Plotting individual and multiple data sets The exploratory analyses in this unit are designed for educational purposes only. The results of the following analyses are not peer-reviewed findings, nor are they based on any hypotheses. 2.2 Access, Download, and Unzip To download data with the utils::download.file() function, define two variables. One variable to store the website URL where the data exists and a second to store the file path for where the file should be saved. Multiple chunks of code in this tutorial will contain / YOUR FILE PATH /. To run the code on your machine, substitute / YOUR FILE PATH / with the file path where you would like to store the tutorial data. url_epa &lt;- &quot;https://aqs.epa.gov/aqsweb/airdata/daily_88101_2021.zip&quot; destination_epa &lt;- &quot;/ YOUR FILE PATH /epa_data.zip&quot; download.file( url_epa, destination_epa ) The file downloaded from the EPA website is a zip file. Zip files need to be unzipped (decompressed) in order to access the data within. Unzip the EPA air pollution file with utils::unzip(). Unzipping a .zip file will decompress the contents within. Spatial data sets can be very large (i.e., &gt; 1 GB ), so check the size of the data before unzipping on your machine. The numeric value size of the file is listed under Length. unzip(&quot;/ YOUR FILE PATH /epa_data.zip&quot;, list = TRUE ) After inspecting the file size, unzip epa_data.zip. unzip(&quot;/ YOUR FILE PATH /epa_data.zip&quot;) Inspecting the file with utils::unzip(list = TRUE) returned the size of the file, but also the name of the data file of interest. The desired data file can also be identified with list.files(). Other file names may be returned if / YOUR FILE PATH / is a directory with other contents (e.g., Desktop or Documents folder). list.files(&quot;/ YOUR FILE PATH /&quot;) 2.3 Data Preparation 2.3.1 Import Now that the contents of the zip file have been saved on your machine and the data file of interest has been identified, import the data with sf::st_read(). pm &lt;- st_read(&quot;/ YOUR FILE PATH /daily_88101_2021.csv&quot;) The previous chunk of code returned a Warning: message. This warning informs the user that the imported data does not have native spatial features, so the data was imported as a data.frame. 2.3.2 Inspect Structure Inspect the structure of pm to see its class, column names, column classes, and the first two (specified by vec.len = 2) data points from each column. str(pm, vec.len = 2 ) 2.3.3 Subset Checking the data structure shows that pm is a very large data set. Each of the variables convey important information related to air pollution monitoring, but not all will be utilized in these exploratory analyses. The data set can be reduced to include only the variables of interest with the subset() function. The select = argument indicates which variables to be retained in the new data set. pm &lt;- subset(pm, select = c( State.Code, County.Code, Site.Num, Latitude, Longitude, State.Name, Date.Local, Arithmetic.Mean )) Re-running str(pm) after subsetting the data set shows that all observations (n = 590,208) of the variables of interest (n = 8) have been retained. str(pm, vec.len = 2 ) 2.3.4 Reclassify The str() function showed the class of each variable within the data set. All of the retained variables are of class character, indicated by : chr and the quotations around each observation (\"01\" \"01\" ...). The class of a variable depends on the information conveyed by the data stored within that variable. For example, character is an appropriate class for the pm$State.Name variable because each observation is a character string labeling in which state the monitor was located. Alternatively, character is not appropriate for the pm$Arithmetic.Mean or pm$Date.Local variables because each observation is a numeric decimal or time-referenced date, respectively. The as. functions can be used for reclassifying data. Reclassify pm$Arithmetic.Mean as a number, and pm$Date.Local as a date. pm$Arithmetic.Mean &lt;- as.numeric(pm$Arithmetic.Mean) pm$Date.Local &lt;- as.Date(pm$Date.Local) After running the as. functions, ensure that the two variables have been reclassified to the desired classes. class(pm$Arithmetic.Mean) class(pm$Date.Local) 2.3.5 Convert to sf Object With the variables of interest selected and reclassified, pm can be converted to spatially defined sf object. The sf::st_as_sf() function creates a $geometry field based on the latitude and longitude coordinates contained within pm. The coords = c() argument specifies the column names which contain the coordinate values. The columns containing coordinate values will not always be “Latitude” and “Longitude”. Use str() to see column names and identify which contain the coordinate values. pm_sf &lt;- st_as_sf(pm, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;) ) Inspect the classes of pm_sf and pm_sf$geometry to see their differences, and how each are different than class(pm). class(pm_sf) class(pm_sf$geometry) class(pm_sf) returned both \"sf\" and \"data.frame\", indicating that it contains both spatial and non-spatial data. 2.3.6 Coordinate Reference System and Projection The coordinate reference system of an sf object can be checked with sf::st_crs(). st_crs(pm_sf) The previous chunk of code shows that pm_sf does not have a native coordinate reference system. The same function, sf::st_crs(), can be used to assign a coordinate reference system to an sf object. For this example, the World Geodetic System 1984 (WGS 84) will be used (EPSG code: 4326). st_crs(pm_sf) &lt;- 4326 st_crs(pm_sf) An sf object with a coordinate reference system can be transformed (projected) into a different coordinate reference system with sf::st_transform(). The area of interest for these exploratory analyses is the conterminous United States, so the Albers Equal Area projected coordinate system will be used (EPSG code: 5070). For a detailed description of coordinate reference systems, and how to select the best system for your analyses, see Section Coordinate Reference Systems and Projections in the Chapter Getting Started. An sf object without an assigned coordinate reference system cannot be transformed. sf::st_crs() must be used to assign a coordinate reference system to an sf object that does not have one. pm_sf &lt;- st_transform( pm_sf, 5070 ) 2.4 Exploratory Analysis 2.4.1 Plot Plotting spatial data is important for visualizing and analyzing patterns in the data. Initialize a plot for the locations of each air pollution monitoring station with ggplot2::ggplot(). Identifying the data set to be plotted within the geom_sf() argument informs the function that the data is an sf object. ggplot() + geom_sf(data = pm_sf) + ggtitle(&quot;Air Pollution Monitor Locations&quot;) + theme_pubr() + theme(plot.title = element_text(hjust = 0.5)) + grids() The plot shows the distribution of monitoring locations, and roughly depicts the outline of the United States due to the large number of monitors. The plot does not, however, convey any information about the concentration of PM2.5 measured by each monitor. Inspect the summary statistics of the PM2.5 measurements before creating any plots to visualize the data. summary(pm_sf$Arithmetic.Mean) sd(pm_sf$Arithmetic.Mean) After inspecting the summary statistics, create a histogram of the PM2.5 concentration measurements to visualize the distribution of the data. The histogram is not a spatially defined plot, so the data set to be plotted is identified within ggplot(). ggplot( data = pm_sf, aes(Arithmetic.Mean) ) + geom_histogram( fill = &quot;blue&quot;, binwidth = 5 ) + ggtitle( expression(&quot;PM&quot;[2.5] * &quot; Concentration Measurements&quot;) ) + xlab(expression(&quot;PM&quot;[2.5] * &quot; Concentration (µg/m&quot;^3 * &quot;)&quot;)) + ylab(&quot;Number of Measurements&quot;) + theme_pubr() + theme(plot.title = element_text(hjust = 0.5)) + grids() 2.4.2 Calculate Annual Mean A common summary statistic of interest to researchers is the mean over a certain period of time. For this example, we will calculate the mean PM2.5 concentration for each monitoring location for the year 2021. To do this, a unique identification code must be created for each monitoring location. The pm_sf$Monitor.ID variable can be created by concatenating each monitor’s state code, county code, and site number together into a single character string. pm_sf$Monitor.ID &lt;- paste0( pm_sf$State.Code, pm_sf$County.Code, pm_sf$Site.Num ) Each unique monitor identification code should be matched with a unique monitor location geometry. To ensure that each monitor location has a unique identification code, check that there are an equal number of unique geometries and identification codes. length(unique(pm_sf$Monitor.ID)) == length(unique(pm_sf$geometry)) Now that each monitor location has a unique identification code, we can calculate the mean PM2.5 concentration measured at each monitoring location. Functions and syntax from the dplyr package will be used to do this. For more on the dplyr package, please see Introduction to dplyr. The group_by(Monitor.ID, ) argument specifies that an annual mean should be calculated for each unique Monitor.ID. Including State.Name in this argument retains the column in the new pm_mean data set, but does not influence the calculation of the annual mean. pm_mean &lt;- pm_sf %&gt;% group_by(Monitor.ID, State.Name) %&gt;% summarise(Annual.Mean = mean(Arithmetic.Mean)) Inspect the summary statistics of pm_mean. summary(pm_mean$Annual.Mean) sd(pm_mean$Annual.Mean) Create a plot which shows the distribution of monitoring locations, and color each point according to the monitor’s annual mean concentration of PM2.5. ggplot() + geom_sf( data = pm_mean, aes(color = Annual.Mean) ) + scale_color_viridis_b( expression(&quot;PM&quot;[2.5] * &quot; Concentration (µg/m&quot;^3 * &quot;)&quot;) ) + ggtitle( expression(&quot;Annual Mean PM&quot;[2.5] * &quot; Concentration&quot;) ) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() Now the plot depicts both spatial and non-spatial data. 2.4.3 Compare Highest Annual Means A close visual inspection of the previous plot shows a few monitoring locations in the southwestern region of the United States with very high (&gt; 20 µm/m3) annual mean concentrations of PM2.5. To investigate the differences between the monitors with the highest and lowest annual mean concentrations, create a subset of pm_sf with only the three highest and lowest monitors. To do this, first identify the monitors with the highest and lowest annual mean concentrations. min_monitors &lt;- pm_mean %&gt;% arrange(Annual.Mean) %&gt;% head(n = 3) max_monitors &lt;- pm_mean %&gt;% arrange(Annual.Mean) %&gt;% tail(n = 3) Next, create a variable storing only the unique identification codes of these six monitors. min_max_monitors_id &lt;- c( min_monitors$Monitor.ID, max_monitors$Monitor.ID ) Finally, subset the pm_sf data set according to the monitor identification codes stored in min_max_monitors_id. pm_min_max &lt;- subset(pm_sf, subset = Monitor.ID == min_max_monitors_id ) The resulting pm_min_max data set contains data for only six monitoring locations. Check the unique monitor identification codes that constitute the new data set. unique(pm_min_max$Monitor.ID) The temporal trend of PM2.5 concentrations measured at each of these locations in 2021 can be depicted with ggplot::geom_line(). ggplot( data = pm_min_max, aes( x = Date.Local, y = Arithmetic.Mean, group = Monitor.ID, color = Monitor.ID ) ) + geom_line() + ggtitle(&quot;Minimum and Maximum Monitors&quot;) + xlab(&quot;Date&quot;) + ylab(expression(&quot;PM&quot;[2.5] * &quot; Concentrations (µg/m&quot;^3 * &quot;)&quot;)) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) Alternatively, the ggplot2::geom_boxplot() function compares the median, interquartile range, and outliers of the monitors’ measurements. ggplot( data = pm_min_max, aes( x = Monitor.ID, y = Arithmetic.Mean, fill = Monitor.ID ) ) + geom_boxplot() + xlab(&quot;Monitor ID&quot;) + ylab(expression(&quot;PM&quot;[2.5] * &quot; Concentrations (µg/m&quot;^3 * &quot;)&quot;)) + theme_pubr(legend = &quot;none&quot;) References "],["chapter-polygon-data.html", "3 Polygon Data 3.1 Introduction 3.2 Access, Download, and Unzip 3.3 Data Preparation with sf 3.4 Exploratory Analysis with sf 3.5 Data Preparation with terra 3.6 Exploratory Analysis with terra", " 3 Polygon Data Polygon Data Access, Preparation, and Exploratory Analysis in R Date Modified: May 6, 2024 Authors: Mitchell Manware , Kyle P. Messier Key Terms: Geospatial Data, Wildfire Programming Language: R 3.1 Introduction Wildfire smoke plume coverage data from the United States National Oceanic and Atmospheric Administration (NOAA) will be used to demonstrate using polygon data. This tutorial will cover polygon data with both the sf (1) and terra (9) packages separately, but the steps for accessing, downloading, and unzipping the data is the same for both packages. This tutorial will demonstrate the following steps with polygon data in R: Downloading data from a URL Importing data Checking data type, structure, and class Reclassifying data Computing summary and zonal statistics Plotting individual and multiple data sets The exploratory analyses in this unit are designed for educational purposes only. The results of the following analyses are not peer-reviewed findings, nor are they based on any hypotheses. 3.2 Access, Download, and Unzip The website URL where the NOAA wildfire smoke plume data exists is date-specific, meaning there is a unique URL for each daily data set. For the purpose of these exploratory analyses, wildfire smoke plume data from September 1, 2023 will be used. Define three variables for day, month, and year according to the date of interest. day &lt;- &quot;01&quot; month &lt;- &quot;09&quot; year &lt;- &quot;2023&quot; The utils::download.file() function downloads the file according to the defined URL and destination file. url_noaa &lt;- paste0( &quot;https://satepsanone.nesdis.noaa.gov/pub/FIRE/web/HMS/Smoke_Polygons&quot;, &quot;/Shapefile/&quot;, year, &quot;/&quot;, month, &quot;/hms_smoke&quot;, year, month, day, &quot;.zip&quot; ) destination_noaa &lt;- paste0( &quot;/ YOUR FILE PATH /noaa_smoke&quot;, year, month, day, &quot;.zip&quot; ) download.file( url_noaa, destination_noaa ) The file downloaded from the NOAA website is a .zip file. Zip files need to be unzipped (decompressed) in order to access the data within. Unzip the NOAA wildfire smoke plume coverage file with utils::unzip(). Unzipping a .zip file will decompress the contents within. Spatial data sets can be very large (i.e., &gt; 1 GB), so check the size of the data before unzipping on your machine. The numeric value size of each file is listed under Length. unzip(&quot;/ YOUR FILE PATH /noaa_smoke20230901.zip&quot;, list = TRUE ) After inspecting the file sizes, unzip noaa_smoke20230901.zip. unzip(&quot;/ YOUR FILE PATH /noaa_smoke20230901.zip&quot;) Inspecting the file with utils::unzip(list = TRUE) returned the size of the file, but also the name of the data file of interest. The desired data file can also be identified with list.files(). list.files(&quot;/ YOUR FILE PATH /&quot;) Listing the contents of the unzipped file reveals four individual files. The data to be imported is stored in the hms_smoke20230901.shp, but the other files contain important information for the .shp file. Deleting any of the supporting files (i.e., *.dbf, *.prj, or *.shx) will disrupt the data import. 3.3 Data Preparation with sf This section will focus on exploratory analyses with polygon data using the sf package. 3.3.1 Import Now that the contents of the zip file have been saved on your machine and the data file of interest has been identified, import the data with sf::st_read(). Although the supporting files are required to import a shapefile, only the file ending in .shp needs to be imported smoke_sf &lt;- st_read(&quot;/ YOUR FILE PATH /hms_smoke20230901.shp&quot;) Importing hms_smoke20230901.shp does not return a Warning: message because the data set has native spatial features, and is therefore imported as an sf object. 3.3.2 Inspect Structure Inspect the structure of smoke_sf to see its class, column names, column classes, and the first two (specified by vec.len = 2) data points. str(smoke_sf, vec.len = 2 ) As mentioned previously, the smoke_sf data set has native spatial features. These are reflected by the data set having classes of sf and data.frame, and the $geometry feature. class(smoke_sf) 3.3.3 Reclassify The main parameter of interest in this data set is $Density, which discretely categorizes each wildfire smoke plume as “Light”, “Medium”, or “Heavy”. Checking its class shows that $Density is class character. class(smoke_sf$Density) Nominal data, data without fixed order or rank system, can be stored as class character (i.e., State names). However, it is best to store ordinal data as class factor for conducting analyses in R. Converting data from class character to class factor can be done with factor(). The levels = c() argument in the function specifies both the level names and the ranked order of the levels. smoke_sf$Density &lt;- factor(smoke_sf$Density, levels = c(&quot;Light&quot;, &quot;Medium&quot;, &quot;Heavy&quot;) ) Check the class of $Density again to ensure proper reclassification. class(smoke_sf$Density) 3.3.4 Coordinate Reference System and Projection Check the coordinate reference system of an sf object with sf::st_crs(). st_crs(smoke_sf) smoke_sf has a native coordinate reference system which was imported during the sf::st_read() step. The area of interest for these exploratory analyses is the conterminous United States, so we can transform smoke_sf to the Albers Equal Area projected coordinate system (EPSG code: 5070). smoke_sf &lt;- st_transform( smoke_sf, 5070 ) 3.4 Exploratory Analysis with sf 3.4.1 Plot (Single) With the data prepared, plot the wildfire smoke plume polygons with ggplot2::ggplot(). Now that the parameters of interest and coordinate reference system have been prepared, create a plot with ggplot2::ggplot(). Identifying the data set to be plotted within the geom_sf() argument informs the function that the data is an sf object. ggplot() + geom_sf( data = smoke_sf, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + ggtitle(&quot;Wildfire Smoke Plumes&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() The wildfire smoke plume polygons are clearly visible and colored according to their individual smoke density classification. This plot, however, is difficult to interpret for two reasons. First, there are multiple polygons for each smoke density classification. Multiple borders and overlapping polygons with the same smoke density type can be confusing. To make the polygons more clear, individual polygons for each smoke density classification can be combined. For the purposes of these exploratory analyses, the satellite travelling direction and time of collection will be ignored. 3.4.2 Union Individual polygons can be unioned (combined) into one multi-part polygon with sf::st_union. The group_by(Density) argument specifies that the polygons should be combined based on the value stored in $Density. Adding the Date = paste0(... argument within the dplyr::summarise() function creates a parameter to store the date based on the year, month, and day of the data. smoke_sf_density &lt;- smoke_sf %&gt;% group_by(Density) %&gt;% summarise( geometry = st_union(geometry), Date = paste0( year, month, day ) ) The resulting data set contains three multi-polygons, a column for the smoke plume classification, and a column for the date. smoke_sf_density Creating a new plot with smoke_sf_density. ggplot() + geom_sf( data = smoke_sf_density, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + ggtitle(&quot;Wildfire Smoke Plumes (unioned)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() The plot is still difficult to interpret because it lacks geospatial context. The grid lines provide latitude and longitude references, but physical or geopolitical boundaries can help show where the wildfire smoke plumes are relative to the study area of interest. To provide geospatial context to the wildfire smoke plume polygons, we can add the United States state boundary polygons to the plot. 3.4.3 Plot (Multiple) The steps taken to access, download, unzip, and import the United States state boundary data are the same as those taken for the wildfire smoke plume coverage data. Refer to Sections Access, Download, and Unzip and Import for detailed descriptions. url_states &lt;- &quot;https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip&quot; destination_states &lt;- &quot;/ YOUR FILE PATH /states.zip&quot; download.file( url_states, destination_states ) unzip(&quot;/ YOUR FILE PATH /states.zip&quot;, list = TRUE ) unzip(&quot;/ YOUR FILE PATH /states.zip&quot;) list.files(&quot;/ YOUR FILE PATH /&quot;) states &lt;- st_read(&quot;/ YOUR FILE PATH /cb_2018_us_state.shp&quot;) Inspect the structure of states_sf. str(states_sf, vec.len = 2 ) For the purpose of these exploratory analyses, only the conterminous United States (CONUS) will be used. Subset states_sf to remove Alaska, Hawaii, and United States territories. remove &lt;- c( &quot;Alaska&quot;, &quot;Hawaii&quot;, &quot;Puerto Rico&quot;, &quot;United States Virgin Islands&quot;, &quot;Commonwealth of the Northern Mariana Islands&quot;, &quot;Guam&quot;, &quot;American Samoa&quot; ) conus_sf &lt;- subset( states_sf, !NAME %in% remove ) Check the coordinate reference system. st_crs(conus_sf) When analyzing multiple spatial data sets together, all data sets must have the same coordinate reference system or projected coordinate system. Transform conus_sf to match the projected coordinate system of the smoke_sf_density data set. conus_sf &lt;- st_transform( conus_sf, st_crs(smoke_sf_density) ) Plot the conterminous United States state boundaries. ggplot() + geom_sf(data = conus_sf) + ggtitle(&quot;Coterminous United States&#39; State Boundaries&quot;) + theme_pubr() + theme(plot.title = element_text(hjust = 0.5)) + grids() With the wildfire smoke plume and conterminous United States polygons imported and prepared, ensure that they have the same coordinate reference system. st_crs(smoke_sf_density) == st_crs(conus_sf) Create a plot which shows the distribution of wildfire smoke plumes over the conterminous United States state boundaries. ggplot() + geom_sf( data = smoke_sf_density, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + geom_sf( data = conus_sf, fill = &quot;transparent&quot; ) + ggtitle(&quot;Wildfire Smoke Plumes (with states)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() This plot provides important geospatial context for understanding where the wildfire smoke plumes are in relation to the study area of interest. 3.4.4 Crop The sf::st_crop() function can be used to reduce the extent of a set of polygons to a specific rectangle, typically the bounding box of another spatial data set. In this example we can crop the smoke_sf_density polygons to the bounding box surrounding the conterminous United States. smoke_sf_crop &lt;- st_crop( smoke_sf_density, conus_sf ) Plot the cropped wildfire smoke plume polygons and the conterminous United States state boundaries. ggplot() + geom_sf( data = smoke_sf_crop, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + geom_sf( data = conus_sf, fill = &quot;transparent&quot; ) + ggtitle(&quot;Wildfire Smoke Plumes (cropped)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() 3.5 Data Preparation with terra This section will focus on exploratory analyses with polygon data using the terra package (9). 3.5.1 Import Now that the contents of the zip files have been saved on your machine and the data files of interest have been identified, import both the wildfire smoke plume coverage data and the United States state boundary data with terra::vect(). See Sections Access, Download, and Unzip and Plot (Multiple) for obtaining the wildfire smoke plume coverage and United States state boundary data sets, respectively. smoke_t &lt;- vect(&quot;/ YOUR FILE PATH /hms_smoke20230901.shp&quot;) states_t &lt;- vect(&quot;/ YOUR FILE PATH /cb_2018_us_state_500k.shp&quot;) 3.5.2 Inspect Structure Inspect the structures of smoke_t and states_t to see their classes, column names, and column classes. smoke_t states_t Both smoke_t and states_t have native spatial features. These are reflected by the type of spatial data in geometry:, and the spatial attributes extent: and coord. ref.: 3.5.3 Reclassify The main parameter of interest in this data set is $Density, which discretely categorizes each wildfire smoke plume as “Light”, “Medium”, or “Heavy”. Checking its class shows that $Density is class character. class(smoke_t$Density) Nominal data, data without fixed order or rank system, can be stored as class character (i.e., State names). However, it is best to store ordinal data as class factor for conducting analyses in R. Converting data from class character to class factor can be done with factor(). The levels = c() argument in the function specifies both the level names and the ranked order of the levels. smoke_t$Density &lt;- factor(smoke_t$Density, levels = c(&quot;Light&quot;, &quot;Medium&quot;, &quot;Heavy&quot;) ) Check the class of $Density again to ensure proper reclassification. class(smoke_t$Density) For the purpose of these exploratory analyses, only the conterminous United States (CONUS) will be used. Subset states_t to remove Alaska, Hawaii, and the United States territories. remove &lt;- c( &quot;Alaska&quot;, &quot;Hawaii&quot;, &quot;Puerto Rico&quot;, &quot;United States Virgin Islands&quot;, &quot;Commonwealth of the Northern Mariana Islands&quot;, &quot;Guam&quot;, &quot;American Samoa&quot; ) conus_t &lt;- subset( states_t, !states_t$NAME %in% remove ) 3.5.4 Coordinate Reference System and Projection Check the coordinate reference systems of SpatVector objects with terra::crs(). crs(smoke_t, describe = TRUE ) crs(conus_t, describe = TRUE ) Both data sets have native coordinate reference systems which were imported during the terra::vect() step. The two data sets, however, have different coordinate reference systems from each other. The area of interest for these exploratory analyses is the conterminous United States, so we can project smoke_t and conus_t to the Albers Equal Area projected coordinate system (EPSG code: 5070). smoke_t &lt;- project( smoke_t, &quot;EPSG:5070&quot; ) conus_t &lt;- project( conus_t, &quot;EPSG:5070&quot; ) Although both data sets were transformed to the same projected coordinate system, it is important to ensure that all data sets have the same coordinate reference system or projected coordinate system. crs(smoke_t) == crs(conus_t) 3.6 Exploratory Analysis with terra 3.6.1 Plot (Multiple) Plot both data sets together in one plot with ggplot2::ggplot(). Now that the parameters of interest and coordinate reference systems have been prepared, create a plot with ggplot2::ggplot(). Identifying the data sets to be plotted within the tidyterra::geom_spatvector() arguments informs the function that the data are SpatVector objects (10). ggplot() + geom_spatvector( data = smoke_t, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot; ) + ggtitle(&quot;Wildfire Smoke Plumes (with states)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() The wildfire smoke plume polygons are clearly visible and colored according to their individual smoke density classification. The plot, however, is difficult to interpret because there are multiple polygons for each smoke density classification. Multiple borders and overlapping polygons with the same smoke density type can be confusing. To make the polygons more clear, individual polygons for each smoke density classification can be combined. For the purposes of these exploratory analyses, the satellite travelling direction and time of collection will be ignored. 3.6.2 Aggregate Individual polygons an be aggregated (combined) into one multi-part polygon with terra::aggregate(). The by = \"Density\" argument specifies that the polygons should be combined based on the value stored in $Density. smoke_t_density &lt;- terra::aggregate(smoke_t, by = &quot;Density&quot;, dissolve = TRUE ) Aggregating the polygons based on the values stored in the $Density column can result in the other columns containing NA values. To remove these columns, subset smoke_t_density to remove $Satellite, $Start, and $End. smoke_t_density &lt;- smoke_t_density[ seq_len(nrow(smoke_t_density)), c(&quot;Density&quot;, &quot;agg_n&quot;) ] The resulting data set contains three multi-polygons, a column for the smoke plume classification, and a count of the number of individual polygons that were aggregated to create the multi-polygon. This last column, $agg_n is automatically calculated by the terra::aggregate() function. smoke_t_density Create a new plot with smoke_t_density. ggplot() + geom_spatvector( data = smoke_t_density, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot; ) + ggtitle(&quot;Wildfire Smoke Plumes (aggregated)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() 3.6.3 Crop The terra::crop() function can be used to reduce SpatVector to an area determined by another SpatVector. In this example, we can crop the smoke_t_density polygons to the conterminous United States state boundaries. smoke_t_crop &lt;- terra::crop( smoke_t_density, conus_t ) Plot the cropped wildfire smoke plume polygons and the conterminous United States state boundaries. ggplot() + geom_spatvector( data = smoke_t_crop, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot; ) + ggtitle(&quot;Wildfire Smoke Plumes (cropped)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.line = element_blank(), axis.ticks = element_blank(), axis.text = element_blank() ) 3.6.4 Zonal Statistics Looking closely at the previous plot, it is clear that wildfire smoke plumes cover each state differently. The terra package can be used to identify which states are covered by each classification of wildfire smoke plumes. The terra::relate() function can be used to identify spatial relationships between two SpatVector objects. The relation = \"intersects\" argument logically identifies if any portion of each state is or is not covered by each of the three wildfire smoke plume classification multi-polygons. The output of terra::relate() is a wide matrix. The nested data.frame() and t() wrappers convert the output from a wide matrix to a long data frame, which is required to combine the results with the conus_t data set. conus_smoke &lt;- data.frame( t( relate(smoke_t_density, conus_t, relation = &quot;intersects&quot; ) ) ) Set the column names of conus_smoke to match the smoke density classifications. The order of the columns in conus_smoke are based on the ordered factor levels in smoke_t_density$Density (see Section Reclassify). colnames(conus_smoke) &lt;- c(&quot;Light&quot;, &quot;Medium&quot;, &quot;Heavy&quot;) Combine the wildfire smoke plume indicator data frame with the the conterminous United States state boundaries data. conus_t &lt;- cbind( conus_t, conus_smoke ) The conus_t data set now contains separate columns indicating the presence or absence of “Light”, “Medium”, and “Heavy” wildfire smoke plumes for each conterminous state. names(conus_t) 3.6.5 Plot (for Loop) A for loop can be used to create indicator plots for each wildfire smoke plume classification. The layout of a for loop can look complicated, but it simply applies the same set of functions to a given list of inputs. The list of inputs must first be created. As the goal is to plot each of the wildfire smoke plume density classifications, create a character vector of the three classification names. This “list of inputs” must first be created. Store the three wildfire smoke plume classifications in a vector of class character. dens_c &lt;- c(&quot;Light&quot;, &quot;Medium&quot;, &quot;Heavy&quot;) Create a for loop that creates a plot for each wildfire smoke plume density stored within dens_c. Code line 1 tells the for loop to apply the following functions to each observation in dens_c. Code lines 3 through 9 define the plotting colors based on the wildfire smoke plume classification (dens_c[d]). As in previous plots, “Light” smoke plumes will be colored green, “Medium” smoke plumes will be covered yellow, and “Heavy” smoke plumes will be colored red. Code lines 12 through 32 create the plot based on the wildfire smoke plume classification (dens_c[d]), and previously defined plotting colors (color_values). for (d in seq_along(dens_c)) { # define color palette based on smoke density if (dens_c[d] == &quot;Light&quot;) { color_values &lt;- c(&quot;lightgrey&quot;, &quot;lightgreen&quot;) } else if (dens_c[d] == &quot;Medium&quot;) { color_values &lt;- c(&quot;lightgrey&quot;, &quot;lightgoldenrod&quot;) } else if (dens_c[d] == &quot;Heavy&quot;) { color_values &lt;- c(&quot;lightgrey&quot;, &quot;tomato&quot;) } # create plot print( ggplot() + geom_spatvector( data = conus_t, aes_string(fill = dens_c[d]) ) + scale_fill_manual( paste0( dens_c[d], &quot; Smoke Plume Coverage Present&quot; ), values = color_values ) + theme_pubr(legend = &quot;bottom&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.line = element_blank(), axis.ticks = element_blank(), axis.text = element_blank() ) ) } References "],["chapter-raster-data.html", "4 Raster Data 4.1 Introduction 4.2 Access and Download 4.3 Data Preparation 4.4 Exploratory Analysis", " 4 Raster Data Raster Data Access, Preparation, and Exploratory Analysis in R Date Modified: May 6, 2024 Authors: Mitchell Manware , Kyle P. Messier Key Terms: Geospatial Data, Weather Programming Language: R 4.1 Introduction Air temperature data from the United States National Oceanic and Atmospheric Administration’s (NOAA) North American Regional Reanalysis (NARR) data set will be used to demonstrate using raster (grid) data with the terra package (9). This tutorial will demonstrate the following steps with raster data in R: Downloading data from a URL Importing data Checking data type, structure, and class Reclassifying data Computing summary and zonal statistics Plotting individual and multiple data sets The exploratory analyses in this unit are designed for educational purposes only. The results of the following analyses are not peer-reviewed findings, nor are they based on any hypotheses. 4.2 Access and Download The website URL where the NOAA NARR exists is year-specific, meaning there is a unique URL for each annual data set. For the purpose of these exploratory analyses, air temperature data from the year 2021 will be used Define the variable year according to the year of interest. year &lt;- &quot;2021&quot; The utils::download.file() function downloads the file according to the defined URL and destination file. # specify the URL where data is stored based on year variable of interest url_narr &lt;- paste0( &quot;https://downloads.psl.noaa.gov//Datasets/NARR/Dailies/monolevel/air.2m.&quot;, year, &quot;.nc&quot; ) # specify where to save downloaded data destination_narr &lt;- paste0( &quot;/ YOUR FILE PATH /narr_air2m_&quot;, year, &quot;.nc&quot; ) # download the data download.file( url_narr, destination_narr ) Identify the desired data file with utils::list.files() list.files(&quot;/ YOUR FILE PATH /&quot;) The file downloaded from NOAA’s NARR data set is an .nc, or netCDF, file. NetCDF files are common for raster data, and do not need to be unzipped. 4.3 Data Preparation 4.3.1 Import Now that the data file of interest has been downloaded and identified, import the data with terra::rast(). narr &lt;- rast(paste0( &quot;/ YOUR FILE PATH /narr_air2m_&quot;, year, &quot;.nc&quot; )) 4.3.2 Inspect Structure Inspect the structure of narr to see its class, dimensions, variables, and layer names. narr When working with raster data, the dimensions of the raster refer to the number of rows (nrow) and columns (ncol) of grid cells that make up the raster. Similarly, the number of layers in the raster object (nlyr) represents the number of observations of the data. These can also be inspected individually with nrow(), ncol(), and nlyr(), respectively. nrow(narr) ncol(narr) nlyr(narr) 4.3.3 Rename The narr data set contains 365 layers, one for each daily observation of air temperature in 2021. When working with raster data that contains multiple layers, it is important to know and recognize the naming structure of each layer. In this case, the layer names are air_ followed by the day of the year (i.e., January 1 = air_1). names(narr)[1:5] Renaming raster layers can be useful for calculating summary statistics or when combining rasters with potentially identical layer names. Using the time() and gsub() functions, the layers can be renamed according to their date. names(narr) &lt;- paste0( &quot;air_&quot;, gsub( &quot;-&quot;, &quot;&quot;, as.character(time(narr)) ) ) Check the layer names again to ensure proper renaming. names(narr)[1:5] 4.3.4 Coordinate Reference System and Projection Check the coordinate reference system of a SpatRaster object with terra::crs(). crs(narr, describe = TRUE ) narr has a native coordinate reference system, but it is unnamed and was not identifiable by terra::rast(). The area of interest for these exploratory analyses is the conterminous United States, so we can project narr to the Albers Equal Area projected coordinate system (EPSG code: 5070). narr &lt;- project( narr, &quot;EPSG:5070&quot; ) We want to create plots with and analyze the air temperature data as it relates to the conterminous United States state boundaries. Subset states_t to remove Alaska, Hawaii, and the United States territories. See the Polygon Data Chapter Section Plot (Multiple) for the steps for obtaining the states_t data set. remove &lt;- c( &quot;Alaska&quot;, &quot;Hawaii&quot;, &quot;Puerto Rico&quot;, &quot;United States Virgin Islands&quot;, &quot;Commonwealth of the Northern Mariana Islands&quot;, &quot;Guam&quot;, &quot;American Samoa&quot; ) conus_t &lt;- subset( states_t, !states_t$NAME %in% remove ) Project conus_t to the Albers Equal Area projected coordinate system (EPSG code: 5070). conus_t &lt;- project( conus_t, &quot;EPSG:5070&quot; ) Ensure that both data sets have the same coordinate reference system. crs(conus_t) == crs(narr) 4.4 Exploratory Analysis 4.4.1 Plot (Multiple) Now that the data sets and coordinate reference systems have been prepared, create a plot with ggplot2::gglot(). Identifying the data sets to be plotted within the tidyterra::geom_spatraster() and tidyterra::geom_spatvector() argument informs the function that the narr and conus_t data sets are SpatRaster and SpatVector objects, respectively (10). Only the first layer of the narr data set will be plotted. ggplot() + geom_spatraster(data = narr$air_20210101) + scale_fill_continuous( type = &quot;viridis&quot;, na.value = NA, &quot;Temperature (°K)&quot; ) + ggtitle(&quot;Air Temperature&quot;) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot;, color = &quot;black&quot; ) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() 4.4.2 Crop The terra::crop() function can be used to reduce a SpatRaster to an area determined by SpatVector polygons. The mask = TRUE argument crops to the border of the polygons, whereas mask = FALSE crops to the bounding box surrounding the polygons. In this example, crop the narr data to the conterminous United States state boundaries. narr_crop &lt;- crop(narr, conus_t, mask = TRUE ) Plot the cropped temperature data and the conterminous United States state boundaries. Only the first layer of the narr data set will be plotted. ggplot() + geom_spatraster(data = narr_crop$air_20210101) + scale_fill_continuous( type = &quot;viridis&quot;, na.value = NA, &quot;Temperature (°K)&quot; ) + ggtitle(&quot;Air Temperature (cropped)&quot;) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot;, color = &quot;black&quot; ) + theme_pubr(legend = &quot;bottom&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.line = element_blank(), axis.ticks = element_blank(), axis.text = element_blank() ) 4.4.3 Units The previous plot depicts the 2 m air temperature data in degrees Kelvin. To convert the units to degrees Celsius, simply subtract the values in narr_crop by 273.15. This subtraction will be applied to every grid cell within every layer of narr_crop. narr_crop_c &lt;- narr_crop - 273.15 4.4.4 Summary Statistics Similar to mathematical operations, calculating summary statistics is very straight forward with raster data. For this example, calculate the annual mean 2m air temperature and range of 2m air temperatures at each grid cell, and save these as a new layer in narr_crop_c narr_crop_c$mean &lt;- mean(narr_crop_c) narr_crop_c$range &lt;- max(narr_crop_c) - min(narr_crop_c) Inspect the results of the mean and range calculations. summary(narr_crop_c$mean) summary(narr_crop_c$range) With the summary statistic layers prepared, create a plot with ggplot2::ggplot(). Identifying the data sets to be plotted within the tidyterra::geom_spatraster() and tidyterra::geom_spatvector() argument informs the function that the narr_crop_c and conus_t data sets are SpatRaster and SpatVector objects, respectively. Additionally, the facet_wrap(~lyr) argument creates a plot for each layer specified in geom_spatraster(data = narr_crop_c[[c(\"mean\", \"range\")]]). ggplot() + geom_spatraster(data = narr_crop_c[[c(&quot;mean&quot;, &quot;range&quot;)]]) + scale_fill_continuous( type = &quot;viridis&quot;, na.value = NA, &quot;Temperature (°C)&quot; ) + facet_wrap(~lyr) + ggtitle(&quot;Air Temperature&quot;) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot;, color = &quot;black&quot; ) + theme_pubr(legend = &quot;bottom&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.line = element_blank(), axis.ticks = element_blank(), axis.text = element_blank() ) 4.4.5 Zonal Statistics Looking closely at the previous plot, it is clear that the annual mean and range of temperatures differ between states. The terra package can be used to calculate zonal statistics of a SpatRaster object based on SpatVector polygons. The terra::zonal() function can be used to calculate the average annual temperature in each state based on the annual grid cell temperatures stored in narr_crop_c$mean. conus_t$MEAN &lt;- zonal(narr_crop_c$mean, conus_t, fun = &quot;mean&quot; ) Plot the state-specific annual mean temperatures with ggplot2::ggplot(). Identifying the data set to be plotted within tidyterra::geom_spatvector() argument informs the function that the data is a SpatVector object. ggplot() + geom_spatvector( data = conus_t, aes(fill = MEAN) ) + scale_fill_continuous( type = &quot;viridis&quot;, na.value = NA, &quot;Temperature (°C)&quot; ) + ggtitle(&quot;Air Temperature (state mean)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.line = element_blank(), axis.ticks = element_blank(), axis.text = element_blank() ) 4.4.6 Reclassify Raster data is most often continuous numeric data. Sometimes, however, it is important to classify the continuous numeric raster data into discrete classes. For this example, we will reclassify the annual mean temperature data into three discrete classes: &lt;10°C, 10°C - 20°C, &gt;20°C. The first step in the reclassification process is to create a matrix storing the “from”, “to”, and “becomes” values. As the names imply, the “from” and “to” values identify the discrete ranges to be reclassified, and “becomes” is the new value that data within this range will take (i.e., “from” 0 “to” 5 “becomes” 1 means that values ranging from 0 to 5 will be reclassified as 1). Create the reclassification matrix. from &lt;- c( -Inf, 10, 20 ) to &lt;- c( 10, 20, Inf ) becomes &lt;- 1:3 reclass &lt;- matrix( c( from, to, becomes ), ncol = 3 ) Now that the reclassification matrix has been prepared, reclassify the annual mean temperatures. The right = TRUE argument indicates that intervals are open on the left and closed on the right (i.e., (0,10] becomes 1). narr_reclass &lt;- classify(narr_crop_c$mean, rcl = reclass, right = TRUE ) Although narr_reclass contains the reclassified mean air temperature data, the data is still continuously numeric. The following chunk of code converts the narr_reclass$mean layer from numeric to character based on the defined levels. narr_reclass contains the reclassified mean air temperature data, but the data is still numeric. The terra::set.cats() function assigns categories to numeric data based on values stored in a data frame. level_values &lt;- data.frame( c(1:3), c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;) ) colnames(level_values) &lt;- c(&quot;mean_continuous&quot;, &quot;mean_discrete&quot;) set.cats(narr_reclass, layer = &quot;mean&quot;, value = level_values ) Plot the discretely reclassified mean temperature data. ggplot() + geom_spatraster(data = narr_reclass$mean_discrete) + scale_fill_viridis_d(&quot;&quot;, labels = c( &quot;≤10°&quot;, &quot;10°&lt; &amp; ≤20°&quot;, &quot;20°&lt;&quot;, &quot;&quot; ), na.value = NA ) + ggtitle(&quot;Air Temperature (reclassified)&quot;) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot;, color = &quot;black&quot; ) + theme_pubr(legend = &quot;bottom&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.line = element_blank(), axis.ticks = element_blank(), axis.text = element_blank() ) References "],["wildfire-data.html", "Wildfire Data", " Wildfire Data This unit provides guidance and code for working with different types of wildfire-related data in climate change and health research. Please note that the CHORDS Toolkit is a work in progress. This unit is currently in development. "],["other-environmental-data.html", "Other Environmental Data", " Other Environmental Data This unit provides guidance and code for working with various sources of environmental data common in environmental health research. "],["chapter-nasa-earthdata.html", "5 NASA EarthData Download 5.1 Introduction 5.2 NASA EarthData Account 5.3 Generate Prerequisite Files 5.4 Download Data", " 5 NASA EarthData Download Using NASA EarthData Account to Download Data in R Date Modified: June 28, 2024 Author: Mitchell Manware Key Terms: Geospatial Data, Remote Sensing Programming Languages: R, Bash 5.1 Introduction 5.1.1 Motivation NASA’s Earth Observing System Data and Information System (EOSDIS) and its twelve Distributed Active Archive Centers (DAAC) are home to a wide range of open access Earth science data. Alaska Satellite Facility (ASF) DAAC Atmospheric Science Data Center (ASDC) Crustal Dynamics Data Information System (CDDIS) Global Hydrometeorology Resource Center (GHRC) Goddard Earth Sciences Data and Information Services Center (GES DISC) Land Processes DAAC (LP DAAC) Level 1 and Atmosphere Archive and Distribution System (LAADS) DAAC National Snow and Ice Data Center (NSIDC) DAAC Oak Ridge National Laboratory (ORNL) DAAC Ocean Biology DAAC (OB.DAAC) Physical Oceanography DAAC (PO.DAAC) Socioeconomic Data and Applications Center (SEDAC) See https://www.earthdata.nasa.gov/eosdis/daacs for more information (14). Many of the NASA EOSDIS data sets are relevant to environmental health research, and accessing the data from within R is important for reproducability and repeatable analyses. Although the data is openly accessible, users are required to register for an EarthData account and/or be logged in if you have an account. The NASA EarthData Account prerequisite files can be generated directly from the command line or using a text editor (notepad in Windows). See How to Generate Earthdata Prerequisite Files for detailed instructions on creating prerequisite files with both the command line and Python (15). 5.1.2 Objectives Users will: Register for or log into a NASA EarthData Account Generate NASA EarthData Account prerequisite files using R Practice downloading data from a URL 5.2 NASA EarthData Account 5.2.1 Register or Log In Visit https://urs.earthdata.nasa.gov/ to register for or log into a NASA EarthData account. NASA EarthData Account Landing Page 5.2.2 Approved Applications After creating an account, navigate to “My Profile”(https://urs.earthdata.nasa.gov/profile), and then to “Applications &gt; Authorized Apps”. This “Authorized Apps” page specifies which NASA EarthData applications can use your login credentials. Authorize the applications from which you will be downloading data. NASA EarthData Approved Applications 5.3 Generate Prerequisite Files Downloading password-protected data from a URL requires user credentials. Without prerequisite files containing user credentials, the data will not be downloaded correctly. Without the prerequisite files the download step run without error, but trying to open the zip file will return an error. To demonstrate, try to download population density data (16) from NASA’s Socioeconomic Data and Applications Center (SEDAC) archive center. NASA SEDAC Population Density Data Characteristics Metric Population Density Year 2020 Resolution ~5 km Format GeoTiff URL https://sedac.ciesin.columbia.edu/data/set/gpw-v4-population-density-rev11 Define the data URL and destination file. url &lt;- paste0( &quot;https://sedac.ciesin.columbia.edu/downloads/data/gpw-v4/gpw-v4-population-&quot;, &quot;density-rev11/gpw-v4-population-density-rev11_2020_2pt5_min_tif.zip&quot; ) destfile &lt;- paste0( &quot;/ YOUR FILE PATH /sedac_population_2020_5km.zip&quot; ) Run the download command using system() and unzip the file with unzip(). system( command = paste0( &quot;curl -n -c -b -LJ -o &quot;, destfile, &quot; --url &quot;, url ) ) unzip(destfile) As expected, the data was not downloaded. To download the password protected data with command line commands, we must generate the .netrc, .urs_cookies, and .dodsrc prerequisite files. The following steps return errors for Windows system users. File generation on Windows is currently in development. 5.3.1 .netrc The following commands create the .netrc file, which contains your NASA EarthData Account credentials. First, set your working directory to the home directory. Setting a working directory differs between Mac/Linux and Windows machines. if (.Platform$OS.type == &quot;unix&quot;) { setwd(&quot;~/&quot;) } else if (.Platform$OS.type == &quot;windows&quot;) { setwd(&quot;C:/&quot;) } Create a file named .netrc with file.create(). file.create(&quot;.netrc&quot;) Open a connection to .netrc with sink(). Write the line machine urs... replacing YOUR_USERNAME and YOUR_PASSWORD with your NASA EarthData username and password, respectively. After writing the line, close the connection with sink() again. sink(&quot;.netrc&quot;) writeLines( &quot;machine urs.earthdata.nasa.gov login YOUR_USERNAME password YOUR_PASSWORD&quot; ) sink() Edit the settings so only you, the owner of the file, can read and write .netrc. system(&quot;chmod 0600 .netrc&quot;) After, check to ensure the file was created properly. file.exists(&quot;.netrc&quot;) TRUE readLines(&quot;.netrc&quot;) paste0( &quot;machine urs.earthdata.nasa.gov login YOUR_USERNAME password YOUR_PASSWORD&quot; ) 5.3.2 .urs_cookies The following commands create the .urs_cookies file. First, set your working directory to the home directory. if (.Platform$OS.type == &quot;unix&quot;) { setwd(&quot;~/&quot;) } else if (.Platform$OS.type == &quot;windows&quot;) { setwd(&quot;C:/&quot;) } Create a file named .netrc with file.create(). file.create(&quot;.urs_cookies&quot;) After, check to ensure the file was created properly. file.exists(&quot;.urs_cookies&quot;) TRUE 5.3.3 .dodsrc The following commands create the .dodsrc file. First, set your working directory to the home directory. if (.Platform$OS.type == &quot;unix&quot;) { setwd(&quot;~/&quot;) } else if (.Platform$OS.type == &quot;windows&quot;) { setwd(&quot;C:/&quot;) } Create a file named “.dodsrc” with file.create() file.create(&quot;.dodsrc&quot;) Open a connection to .dodsrc with sink(). Write the lines beginning with HTTP., replacing YOUR_USERNAME and YOUR_PASSWORD with your NASA EarthData username and password, respectively. After writing the line, close the connection with sink() again. sink(&quot;.dodsrc&quot;) writeLines( paste0( &quot;HTTP.NETRC=YOUR_HOME_DIRECTORY/.netrc\\n&quot;, &quot;HTTP.COOKIE.JAR=YOUR_HOME_DIRECTORY/.urs_cookies&quot; ) ) sink() After, check to ensure the file was created properly. file.exists(&quot;.dodsrc&quot;) TRUE readLines(&quot;.dodsrc&quot;) paste0( c( &quot;HTTP.NETRC=YOUR_HOME_DIRECTORY/.netrc&quot;, &quot;HTTP.COOKIE.JAR=YOUR_HOME_DIRECTORY/.urs_cookies&quot; ) ) If working on a Windows machine, copy the .dodsrc file to the project working directory. Replace YOUR_WORKING_DIRECTORY with the absolute path to the project working directory. if (.Platform$OS.type == &quot;windows&quot;) { file.copy( &quot;C:/.dodsrc&quot;, &quot;YOUR_WORKING_DIRECTORY/.dodsrc&quot; ) } Enter these commands, as well as your username, password, and home directory, without error. Even a single misplaced character can disrupt the verification of your EarthData credentials. 5.4 Download Data With the prerequisite files generated, try to download the SEDAC population density data again. Be sure to authorize the “SEDAC” applications at “My Profile”(https://urs.earthdata.nasa.gov/profile) under “Applications &gt; Authorized Apps” before running the following command. system( command = paste0( &quot;curl -n -c ~/.urs_cookies -b .urs_cookies -LJ -o &quot;, destfile, &quot; --url &quot;, url ) ) unzip(destfile) The data is downloaded successfully after the prerequisite files have been generated and the “SEDAC” applications have been authorized. References "],["health-data-integration.html", "Health Data Integration", " Health Data Integration This unit provides guidance and code for integrating environmental and health data in climate change and health research, both at the individual level and at the population level. "],["chapter-link-to-census.html", "6 Linkage to Census Units 6.1 Introduction 6.2 Tutorial 6.3 Concluding Remarks", " 6 Linkage to Census Units Linking Geocoded Addresses to Census Units and Social Determinants of Health Data in R Date Modified: September 13, 2023 Author: Lara P. Clark Key Terms: Data Integration, Social Determinants of Health, Geocoded Address, GeoID, Geographic Unit Programming Language: R 6.1 Introduction This tutorial provides example code in R: To link geocoded addresses (i.e., geographic coordinates) to the specific US Census geographic units (e.g., tract) in which they are located. To link data from the Agency for Healthcare Research and Quality (AHRQ) Social Determinants of Health Database (17) to those geocoded addresses. 6.1.1 Motivation Linking geocoded addresses to US Census geographic units is a common step in environmental health data integration workflows. First, using geographic information systems (GIS) software, the geocoded addresses are mapped to the specific Census geographic units (e.g., Census tracts) in which they are located. Second, the Census geographic unit identifying code (or, geoID) is matched to each geocoded address. The result is a table of geocoded addresses and their corresponding Census geoIDs. The Census geoIDs can then serve as the basis for linking additional data to each geocoded address. Many types of data with importance for environmental health applications are available by Census geoID. Specifically, the Census collects and provides data by Census geoID for various social determinants of health (SDOH). Such Census SDOH data describe poverty, race/ethnicity, language, housing, and other socioeconomic characteristics. Increasingly, other data providers (e.g., other government agencies, research institutions, community science groups) are making their data available by Census geoID to help facilitate linkages with SDOH data. Such data cover various environment, climate, health, and built environment characteristics. The following table lists example environmental health data sources readily available by Census geoID from US federal agencies. Example Environmental Health Data Sources Data source Geographic units Example topics AHRQ Social Detrminants of Health Database County, ZIP code, tract Demographic characteristics, housing and transportation characteristics, food access, healthcare characteristics EPA Environmental Justice Screening and Mapping Tool Block group Air pollution, hazardous waste, flood risk, wildfire risk, environmental justice indices CDC National Environmental Public Health Tracking Network State, county Heat, sunlight and ultraviolet exposure, built environment characteristics, asthma, heat-related illnesses 6.1.2 Background The Census defines geographic boundaries at various spatial scales. Common examples in environmental health workflows include (from coarsest to finest spatial scale): states, counties, tracts, block groups, and blocks. These boundaries are completely non-overlapping, such that block boundaries are nested within block groups, which are nested within tracts, which are nested within counties, which are nested within states. The Census defines the boundaries of tracts and block groups to scale with local population: that is, in areas with higher population density (e.g., urban cores), these geographic units have finer spatial scale (i.e., smaller land area per unit) and in areas with lower population density (e.g., rural areas), the geographic units have coarser spatial scale (i.e., larger land area per unit). As a result, the spatial scale of geographic units varies substantially across the US. The shape of geographic units also varies substantially: the Census defines these boundaries to follow political boundaries (e.g., state boundaries) as well as physical features (e.g., roads, rivers), which often have irregular shapes. The variable shape and spatial scale of Census geographic units is in contrast with spatial grids– in which each geographic unit (i.e., grid cell) has the same shape and spatial resolution. Such spatial grids are common for environment and climate data. ZIP Code Tabulation Areas (ZCTAs), which represent the geographic areas used by the US Postal Service for ZIP codes, are another common boundary used in environmental health workflows. ZCTAs have no spatial relationship with block groups, tracts, counties, or states: that is, ZCTAs can cross or overlap those other geographic boundaries. Like other Census geographic boundaries, ZCTAs vary in spatial resolution and shape across the US. The spatial scale of ZCTAs is, on average, finer than counties but coarser than tracts. The following figure illustrates the variable spatial scale and shape of each type of geographic boundary: Census Geographic Units in Durham County, North Carolina, in 2010 The Census assigns a unique identifying code, or geoID, to each geographic unit. This Census geoID is also referred to as a FIPS (Federal Information Processing Series) code. The following table describes common types of Census geographic units and the structure of their geoIDs. Common Census Geographic Units in the United States Geographic Unit Total Units1 GeoID Description and Structure Example Unit GeoID Example Unit Name State 50 2-digit state (S) code = SS 09 Connecticut (CT) County 3143 5-digit code = 2-digit state (S) code + 3-digit county (C) code = SSCCC 09007 Middlesex County, CT Zip Code Tabulation Area (ZCTA) 33,642 5-digit ZCTA (Z) code = ZZZZZ 06480 ZCTA for Portland, CT Tract 84,414 11-digit code = 2-digit state (S) code + 3 digit county (C) code + 6-digit tract (T) code = SSCCCTTTTTT 09007560100 Tract 560100 in Middlesex County, CT Block group 239,780 12-digit code = 2-digit state (S) code + 3 digit county (C) code + 6-digit tract (T) code + 1-digit block group (G) code = SSCCCTTTTTTG 090075601001 Block group 1 in Tract 560100 in Middlesex County, CT Block 8,132,968 15-digit code = 2-digit state (S) code + 3 digit county (C) code + 6-digit tract (T) code + 4-digit block (B) code (which contains the block group code as its first digit) = SSCCCTTTTTTBBBB 090075601001004 Block 1004 in Tract 560100 in Middlesex County, CT For states, counties, tracts, block groups, and blocks: the geoIDs for the finer spatial scale units are constructed from the geoIDs for the coarser spatial scale units in which they are located. This makes it possible to extract, for example, the tract geoID from the block geoID based on digit number. 6.1.3 Considerations The following are important considerations for linking geocoded addresses to Census geographic boundaries for environmental health applications. Temporal Considerations: The Census periodically updates its geographic boundaries, such as tracts, block groups, and blocks. For example, the Census may add, remove, or modify specific geographic units (and associated geoIDs) as the population changes over time. Thus, it is important to select the correct years (or, vintages) of Census geographic boundaries needed for linking each data source of interest. For example, linking SDOH data provided for year-2010 vintage Census tract boundaries would require the year-2010 vintage of Census tract geoIDs for each geocoded address. Spatial Scale Considerations: The spatial scale of tracts, block groups, and blocks varies substantially across the US, with finer spatial scale in urban areas and coarser scale in rural areas. Thus, it is important to select the Census geographic boundary with appropriate spatial resolution for the specific environmental health application. Privacy Considerations: Various web-based geocoding tools support look-up of Census geoIDs by street addresses or geographic coordinates. However, such web-based tools require sending geolocation information over the internet, which can risk exposing private geolocation information. To meet data protection guidelines for environmental health applications, this tutorial instead uses a fully offline approach for handling geolocation information. 6.1.4 Outline This tutorial includes the following steps: Install R packages Prepare geocoded addresses Access Census geographic boundaries Link geocoded addresses to Census geographic boundaries Link AHRQ SDOH data to geocoded addresses 6.2 Tutorial 6.2.1 Install R Packages This tutorial uses the R packages sf (1,8), tidyverse (18), tigris (19), and tmap (20). The following code installs and loads these packages: # install required packages install.packages(c(&quot;sf&quot;, &quot;tidyverse&quot;, &quot;tigris&quot;, &quot;tmap&quot;)) # load required packages library(sf) library(tidyverse) library(tigris) library(tmap) 6.2.2 Prepare Geocoded Addresses The first step is to prepare the geocoded addresses (i.e., geographic coordinates or latitude/longitude) for mapping in R. For this tutorial, we will use sample public data to represent geocoded addresses of, for example, participants in a health cohort study. This sample public data will be the coordinates for the city halls of the five largest cities in North Carolina. These coordinates were identified by searching Google Maps. The following code reads these coordinates into a table in R with columns for id, latitude, and longitude: # create a table of sample public geocoded addresses geo_addresses_tbl &lt;- tibble( id = c( &quot;01-charlotte&quot;, &quot;02-raleigh&quot;, &quot;03-greensboro&quot;, &quot;04-durham&quot;, &quot;05-winston-salem&quot; ), latitude = c( 35.21599030178876, 35.78019493350421, 36.07391214865624, 35.99607916214782, 36.09512361249636 ), longitude = c( -80.80170873958926, -78.64278743612566, -79.7883534467845, -78.89907326845271, -80.24283630316438 ) ) # view the table print(geo_addresses_tbl) Here is the table produced by the code above: # A tibble: 5 × 3 # id latitude longitude # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 01-charlotte 35.2 -80.8 # 2 02-raleigh 35.8 -78.6 # 3 03-greensboro 36.1 -79.8 # 4 04-durham 36.0 -78.9 # 5 05-winston-salem 36.1 -80.2 Next, we’ll transform this table to an explicitly spatial data type: simple features (sf). This will allow us to use the point locations for spatial analysis using the sf package in R. To do this, we will need to specify the coordinate reference system (CRS) used for the city hall coordinates. For this example, we retrieved the city hall coordinates from Google Maps, which uses the World Geodetic System 1984 (WGS84) CRS. If you need to find the coordinate reference system information for your geolocation data, here are a few places to look: Metadata for geolocation data Documentation for geocoding method (e.g., geocoding software) or geolocation data collection method (e.g., GPS device) General documentation for data provider There are multiple formats available in R for specifying the CRS: proj4, well known text (wkt), and European Petroleum Survey Group (EPSG) format (21). Here, we will specify the WGS84 CRS using the EPSG format, which is a 4-digit numeric identifying code. We can look up the EPSG code for WGS84 by searching a spatial reference catalog. There, we find that the EPSG code is4326. The following code transforms the table of city hall locations to a simple features object: # transform table to simple features geo_addresses_sf &lt;- sf::st_as_sf(geo_addresses_tbl, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = &quot;EPSG:4326&quot; ) # view simple features print(geo_addresses_sf) Here is the description of the simple features object produced by the code above: # Simple feature collection with 5 features and 1 field # Geometry type: POINT # Dimension: XY # Bounding box: xmin: -80.80171 ymin: 35.21599 xmax: -78.64279 ymax: 36.09512 # Geodetic CRS: WGS 84 # # A tibble: 5 × 2 # id geometry # * &lt;chr&gt; &lt;POINT [°]&gt; # 1 01-charlotte (-80.80171 35.21599) # 2 02-raleigh (-78.64279 35.78019) # 3 03-greensboro (-79.78835 36.07391) # 4 04-durham (-78.89907 35.99608) # 5 05-winston-salem (-80.24284 36.09512) Now, we can see that each city hall has associated geometry in point format. We can also confirm that the correct CRS (WGS84) is now associated with the city hall locations. 6.2.3 Access Census Geographic Boundaries The second step is to prepare the Census geographic boundaries for mapping in R. For this tutorial, we’ll use the tigris package to load the Census tract boundaries for North Carolina in year-2010 (i.e., year-2010 vintage). Using an R package like tigris to load the Census geographic boundaries will help keep the workflow reproducible by documenting all of the steps in R. The following code reads the North Carolina 2010 Census tract boundaries into R as simple features: # download Census tracts in North Carolina in 2010 as simple features nc_tracts_2010_sf &lt;- tigris::tracts(state = &quot;NC&quot;, year = 2010) # view the first several rows of the Census tracts simple features head(nc_tracts_2010_sf) Here is the description of the simple features object produced by the code above: # Simple feature collection with 6 features and 14 fields # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -80.07571 ymin: 34.80499 xmax: -79.45918 ymax: 35.18342 # Geodetic CRS: NAD83 # STATEFP10 COUNTYFP10 TRACTCE10 GEOID10 NAME10 NAMELSAD10 MTFCC10 # 1 37 153 970100 37153970100 9701 Census Tract 9701 G5020 # 2 37 153 970200 37153970200 9702 Census Tract 9702 G5020 # 3 37 153 970800 37153970800 9708 Census Tract 9708 G5020 # 4 37 153 970900 37153970900 9709 Census Tract 9709 G5020 # 5 37 153 971000 37153971000 9710 Census Tract 9710 G5020 # 6 37 153 971100 37153971100 9711 Census Tract 9711 G5020 # # FUNCSTAT10 ALAND10 AWATER10 INTPTLAT10 INTPTLON10 # 1 S 246281647 2106825 +35.0503203 -079.6180454 # 2 S 457736198 7835811 +35.0967892 -079.8225512 # 3 S 139358521 2752112 +34.8508484 -079.8201950 # 4 S 23311020 78240 +34.8785679 -079.7346295 # 5 S 49233222 188190 +34.9395795 -079.6628977 # 6. S 161136716 948938 +34.8751742 -079.6567146 # geometry COUNTYFP STATEFP # 1 MULTIPOLYGON (((-79.56729 3... 153 37 # 2 MULTIPOLYGON (((-79.71753 3... 153 37 # 3 MULTIPOLYGON (((-79.76773 3... 153 37 # 4 MULTIPOLYGON (((-79.76773 3... 153 37 # 5 MULTIPOLYGON (((-79.69038 3... 153 37 # 6 MULTIPOLYGON (((-79.5684 34... 153 37 Each tract has associated geometry in polygon format plus 14 additional attributes (i.e., variables, columns). Importantly, the Census tract geoID (i.e., 11-digit identifying code) for year-2010 is stored in the column GEOID10. We can see that the CRS listed above for the Census tract boundaries (NAD83) is different from the CRS for the city hall locations (WGS84). This will be important for the linkage step. Next, we can view the geometry of the tracts by creating a map: # create a map of the Census tracts nc_tracts_2010_map &lt;- tmap::tm_shape(nc_tracts_2010_sf) + tmap::tm_polygons(lwd = 0.5) # view the map print(nc_tracts_2010_map) Census Tracts in North Carolina in 2010 Other types and vintages of Census geographic boundaries are available through tigris and the related tidycensus package (22). In most cases, these boundaries are available for recent years (i.e., 1990 to present) and are accessed by state (i.e., users can download geographic boundaries for one state at a time, in separate files). Census geographic boundaries are also available to download by state for years 2007 to present via the Census TIGER/Lines website (23). Historic Census geographic boundaries (i.e., 1910 to present) are available through IPUMS NHGIS (24). Additionally, IPUMS NHGIS provides many boundaries at the national scale (i.e., such that users can download geographic boundaries for the entire US in a single file). Here is an example workflow for accessing historic Census geographic boundaries via IPUMS NHGIS in R (25). 6.2.4 Link Geocoded Addresses to Census Geographic Boundaries The third step is to link each geocoded addresses to the geoID of the Census geographic unit that contains it. To do this, we’ll first need to prepare the geocoded addresses and Census geographic boundaries in the same CRS. The following code transforms the CRS of the geocoded addresses to match the the CRS of the Census geographic boundaries (NAD83) and then maps them together: # transform city hall locations to match CRS of Census tracts geo_addresses_crs_sf &lt;- sf::st_transform(geo_addresses_sf, crs = sf::st_crs(nc_tracts_2010_sf) ) # create a map of the Census tracts with the city hall locations linkage_map &lt;- tmap::tm_shape(nc_tracts_2010_sf) + tmap::tm_polygons(lwd = 0.5) + tmap::tm_shape(geo_addresses_crs_sf) + tmap::tm_dots( col = &quot;red&quot;, size = 0.25 ) # view the map print(linkage_map) Census Tracts in North Carolina in 2010 (Grey) with Sample Geocoded Addresses (Red) Now that the geocoded addresses and Census tracts are in the same CRS, we can link each geocoded address to the Census tract that contains it using a spatial join. The following code produces a table of geocoded addresses linked to Census tract geoIDs: # link geocoded addresses to the Census tracts that contain them geo_addresses_linkage_sf &lt;- sf::st_join(geo_addresses_sf, nc_tracts_2010_sf, left = TRUE) # create linked table with geocoded addresses id paired with Census tract geoID geo_addresses_linkage_tbl &lt;- sf::st_drop_geometry(geo_addresses_linkage_sf) %&gt;% dplyr::rename(geoid_tract_2010 = GEOID10) %&gt;% dplyr::select(id, geoid_tract_2010) # write linked table to CSV file readr::write_csv(geo_addresses_linkage_tbl, &quot;city_hall_census_tract_2010_linkage.csv&quot;) # view the linked table print(geo_addresses_linkage_tbl) # A tibble: 5 × 2 # id geoid_tract_2010 # &lt;chr&gt; &lt;chr&gt; # 1 01-charlotte 37119001100 # 2 02-raleigh 37183050100 # 3 03-greensboro 37081010800 # 4 04-durham 37063002200 # 5 05-winston-salem 37067000100 6.2.5 Link AHRQ SDOH Data to Geocoded Addresses The fourth step is to link the AHRQ SDOH data to each geocoded address based on the Census geoID. To start, we’ll need to prepare the AHRQ SDOH data for Census tracts. This data is available to download as an Excel (XLSX) spreadsheet on the AHRQ SDOH website as shown in this screenshot: Screenshot of AHRQ SDOH Website (September 13, 2023) Before linking, we’ll need to check the vintage of Census tracts used in the AHRQ SDOH data. To do this, we can review the AHRQ SDOH Data Source Documentation (accessed through the link shown in the website screenshot above). There we find the following information: The SDOH Database 2009 file has census tracts based on the 2000 census tract boundaries. The SDOH Database for 2010-2019 SDOH files uses 2010 census tract boundaries, and the 2020 file uses 2020 census tract boundaries. Based on this information, we can use the geoIDs for year-2010 vintage Census tracts we prepared for the geocoded addresses in the previous step to link the AHRQ SDOH data for years 2010-2019. If we would instead prefer to link the AHRQ SDOH data for year 2009 or 2020, we would first need to repeat the previous steps to link the geoIDs for year-2000 vintage or year-2020 vintage Census tracts, respectively, to the geocoded addresses. For this example, we will link the AHRQ SDOH data for Census tracts in 2010 to our sample geocoded addresses on the basis of the year-2010 Census tract geoID. The SDOH AHRQ data for Census tracts in 2010 is provided as an Excel file. Because Excel files can contain multiple sheets, we’ll need to first download and open the Excel file to understand which sheet(s) to read into R: Screenshot of First Sheet in Excel File (September 14, 2023) Screenshot of Second Sheet in Excel File (September 14, 2023) We find that the Excel file has two sheets: Layout and Data. The Layout sheet contains the data dictionary. The Data sheet contains &gt;300 columns of SDOH data by geoID for the &gt;70,000 Census tracts in the US in 2010. The following code downloads the Excel file and reads its Data sheet into a table in R: # download Excel file using the URL provided in the screenshot above sdoh_tracts_2010_url &lt;- &quot;https://www.ahrq.gov/downloads/sdoh/sdoh_2010_tract_1_0.xlsx&quot; download.file(sdoh_tracts_2010_url, destfile = &quot;sdoh_2010_tract_1_0.xlsx&quot;) # read the &quot;Data&quot; sheet of the Excel file into a table R # note that you may need to provide a complete filepath for the Excel file sdoh_tracts_2010_tbl &lt;- readxl::read_xlsx(&quot;sdoh_2010_tract_1_0.xlsx&quot;, sheet = &quot;Data&quot;) # view the table head(sdoh_tracts_2010) # A tibble: 6 × 355 # YEAR TRACTFIPS COUNTYFIPS STATEFIPS STATE COUNTY REGION # &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; # 1 2010 01001020100 01001 01 Alabama Autauga County South # 2 2010 01001020200 01001 01 Alabama Autauga County South # 3 2010 01001020300 01001 01 Alabama Autauga County South # 4 2010 01001020400 01001 01 Alabama Autauga County South # 5 2010 01001020500 01001 01 Alabama Autauga County South # 6 2010 01001020600 01001 01 Alabama Autauga County South # ℹ 348 more variables: TERRITORY &lt;dbl&gt;, ACS_TOT_POP_WT &lt;dbl&gt;, # ACS_TOT_POP_US_ABOVE1 &lt;dbl&gt;, ACS_TOT_POP_ABOVE5 &lt;dbl&gt;, # ACS_TOT_POP_ABOVE15 &lt;dbl&gt;, ACS_TOT_POP_ABOVE16 &lt;dbl&gt;, … # ℹ Use `colnames()` to see all variable names We can see that the variable TRACTFIPS contains the 11-digit geoID for Census tracts in character format, which matches the geoID format we prepared in the previous step . For linkages based on geoID, it is important to check that the geoID is in the same format across data sources. We recommend using character format (rather than numeric format) for Census geoIDs. This will preserve padded zeroes in geoIDs (e.g., state geoID for Alabama is 01 rather than 1) such that each type of geoID has the expected number of digits (e.g., 2-digits for state geoID). For this example, we can choose to link the following sample of SDOH variables identified by exploring the Layout sheet: ACS_PCT_INC50: Percentage of population with income to poverty ratio under 0.50 POS_DIST_ED_TRACT: Distance in miles to the nearest emergency department, calculated using population weighted tract centroids ACS_PCT_HU_NO_VEH: Percentage of housing units with no vehicle available The following code joins those SDOH data variables to the geocoded addresses based on the Census tract geoID: # rename geoID in SDOH table to match the geoID in the geocoded addresses table # select the SDOH variables of interest sdoh_tracts_2010_tbl &lt;- sdoh_tracts_2010_tbl %&gt;% dplyr::rename(geoid_tract_2010 = TRACTFIPS) %&gt;% dplyr::select(geoid_tract_2010, ACS_PCT_INC50, POS_DIST_ED_TRACT, ACS_PCT_HU_NO_VEH) # join the SDOH table to the geocoded addresses based on geoID geo_addresses_sdoh_tbl &lt;- dplyr::left_join(geo_addresses_linkage_tbl, sdoh_tracts_2010_tbl, by = &quot;geoid_tract_2010&quot;) # view the resulting linked table print(geo_addresses_sdoh_tbl) # A tibble: 5 × 5 # id geoid_tract_2010 ACS_PCT_INC50 POS_DIST_ED_TRACT # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 01-charlotte 37119001100 3.68 0.85 # 2 02-raleigh 37183050100 6.77 2.8 # 3 03-greensboro 37081010800 6.16 0.99 # 4 04-durham 37063002200 25.9 2.62 # 5 05-winston-salem 37067000100 9.01 3.35 # ACS_PCT_HU_NO_VEH # &lt;dbl&gt; # 1 7.24 # 2 25.5 # 3 16.4 # 4 7.63 # 5 16.5 Now, we have a linked table of geocoded addresses (by individual id) linked to the SDOH data. 6.3 Concluding Remarks This tutorial demonstrates how to link geocoded addresses for individuals to Census geographic boundaries and then to SDOH data available for those Census geographic boundaries. Additional tabular data available for Census geographic boundaries can then be readily linked to further develop an integrated dataset for individuals. Such integrated datasets can be used to investigate relationships between SDOH and health outcomes for individuals. References "],["chapter-fhir-pit.html", "7 A FHIR PIT Tutorial 7.1 Introduction 7.2 Tutorial 7.3 Considerations 7.4 Concluding Remarks", " 7 A FHIR PIT Tutorial FHIR PIT: HL7 Fast Healthcare Interoperability Resources Patient data Integration Tool: A Tutorial Date Modified: July 8, 2024 Authors: Juan Garcia , Kara Fecho , Hong Yi Key Terms: Data Integration, Electronic Health Record, Exposure, FHIR, Geocoded Address Programming Languages: R, Bash 7.1 Introduction This tutorial provides example code in R: To set up FHIR PIT. To analyze the results from the FHIR PIT output. 7.1.1 Disclaimer The patient dataset used in this tutorial is synthetic. When using actual patient or participant datasets containing Protected Health Information (PHI), one must run FHIR PIT within a secure enclave and abide by all federal and institutional regulations. 7.1.2 Motivation Environmental exposures are increasingly recognized as important to consider when conducting human subjects research. Unfortunately, associating environmental exposures data with subject-level data is challenging due to the complexity of the data and the varied spatiotemporal resolutions. FHIR PIT is an open-source tool to link electronic health record (EHR) data in FHIR format with environmental exposures data derived from public sources. See Section Considerations for tools to support other common data models (CDMs). 7.1.3 Background FHIR PIT (HL7 Fast Healthcare Interoperability Resources Patient data Integration Tool) is an open-source data integration pipeline to support subject-level research on social and environmental determinants of health and disease (26). In essence, the pipeline is a sequence of transformations configured from a YAML file (27). The transformation steps join FHIR patient records with environmental exposures estimates using patient geocodes (e.g., primary residence) and date(s) of healthcare visits as the spatiotemporal links. At the end of each linkage step, FHIR PIT saves the linked tables to an intermediate folder to support rapid re-execution, should any given step fail to complete (e.g., lack of disk space). To support multiple healthcare visits, FHIR PIT vectorizes the FHIR patient records by grouping FHIR domains (e.g., Condition, Laboratory, Medication, Procedure) and counting events within a domain on a per-day basis. The features associated with the patient are then grouped by user-defined study periods (e.g., year) and aggregated with user-defined statistics (e.g., counts, mean, median, first measurement, last measurement). FHIR PIT was developed to support ICEES (Integrated Clinical and Environmental Exposures Service) (28,29). However, the tool is not specific to ICEES, but rather can be used to integrate any source of patient FHIR files with any source of environmental exposures data. In this tutorial, we use synthetic FHIR files and randomly sampled environmental exposures data derived from several public sources: US Environmental Protection Agency (EPA) airborne pollutant exposures data - The US EPA maintains collections of model-derived estimates of airborne pollutant exposure data such as PM2.5 and ozone at varying spatial and temporal resolutions. FHIR PIT uses the data to calculate, for example, average daily PM2.5 exposure and maximum daily ozone exposure over a defined study period. US Department of Transportation (DOT), Federal Highway Administration (FHA), Highway Performance Monitoring System (HPMS) major roadway/highway exposures data - The US DOT maintains a variety of metrics on roadway data. FHIR PIT uses the data to calculate point estimates of primary residential distance from the nearest major roadway or highway. US Census Bureau TIGER/line roadway data - The US Census Bureau also maintains metrics on roadway data. FHIR PIT uses the data to supplement the US DOT data. US Census Bureau American Community Survey (ACS) socio-economic exposures data - The US Census Bureau’s ACS is a 5-year survey sample from the decennial nationwide US census. FHIR PIT uses a subset of the available ACS survey estimates of socio-economic exposures, including survey estimates on residential density, household median income, and household access to health insurance. NC Department of Environmental Quality (DEQ) concentrated animal feeding operations (CAFO) exposures data - North Carolina’s DEQ maintains data on the location of all registered CAFOs across the state. FHIR PIT uses the data to calculate point estimates of primary residential distance from the nearest CAFO. NC Department of Environmental Quality (DEQ) landfill exposures data - North Carolina’s DEQ maintains data on the location of all registered active and inactive landfills across the state. FHIR PIT uses the data to calculate point estimates of primary residential distance from the nearest landfill. 7.1.4 Outline This tutorial includes the following steps: Setup Run FHIR PIT Analyze results 7.2 Tutorial # Loading library library(ggplot2) 7.2.1 Setup Install Docker. Follow instructions in Docker installation documentation for your operating system. Assign WORK_DIR the directory you would like to save FHIR PIT and its outputs to. work_dir &lt;- &quot;/ADD/YOUR/FILEPATH/HERE&quot; Sys.setenv(WORK_DIR = work_dir) Clone FHIR PIT into the WORK_DIR directory. mkdir -p $WORK_DIR git clone --recursive https://github.com/ExposuresProvider/FHIR-PIT.git $WORK_DIR 7.2.2 Execute FHIR-PIT echo &quot;$WORK_DIR/data/output&quot; Run FHIR PIT in a docker container with -v. This will allow the output data to persist and be easily accessible by users on the host. docker run -m=4g -v &quot;$WORK_DIR/data/output&quot;:&quot;/FHIR-PIT/data/output&quot; renci/fhir-pit:1.0 7.2.3 Access and Analyze Results The main outputs to consider are in ICEES, ICEES2 and ICEES2_dei directories (named in reference to ICEES, but not specific to ICEES). Each directory contains one or more CSV files with information at different granularities. year_ &lt;- 2010 basepath &lt;- file.path(work_dir, &quot;data&quot;, &quot;output&quot;) icees2dei_patient_url &lt;- file.path(basepath, &quot;icees2_dei&quot;, sprintf(&quot;%spatient_deidentified&quot;, year_)) The ICEES directory contains one CSV file per subject. Each CSV file consists of the subject’s visits, concatenated with the corresponding environmental exposure estimates for that day and that subject’s location. If a subject has multiple visits per day, then the transformation “PreprocPerPatSeriesToVector” step aggregates multiple daily visits by counting how many times a drug or diagnosis occurred. The directory for each subject is indexed by the “patient_num” column. The ICEES2 directory contains a single CSV file with the aggregation of all subjects grouped by (subject, study period). Lastly, the ICEES2DEI directory contains the same aggregated CSV file, but the data have been stripped of all PHI per HIPAA Safe Harbor method (30). The fully deidentified file then abides by all federal regulations surrounding privacy and security, although institutional regulations may remain. For exposition purposes, we focus on the deidentified ICEES2DEI CSV file and reorder its columns. icees2dei_colorder &lt;- scan(&quot;icees2dei_patient_column_order.txt&quot;, what = &quot;&quot;, sep = &quot;\\n&quot;) icees2dei &lt;- read.csv(icees2dei_patient_url, header = TRUE)[, icees2dei_colorder] icees2dei # index year AvgDailyPM2.5Exposure AvgDailyPM2.5Exposure_StudyAvg # 1 0 2010 7.949999 8.645932 # 2 1 2010 10.520382 8.645932 # 3 2 2010 22.711028 12.472791 # 4 3 2010 22.134923 10.000803 # AvgDailyPM2.5Exposure_StudyMax MaxDailyPM2.5Exposure # 1 38.89938 20.42399 # 2 38.89938 16.50489 # 3 49.20912 35.83996 # 4 50.76701 37.09866 # MaxDailyPM2.5Exposure_StudyAvg MaxDailyPM2.5Exposure_StudyMax # 1 14.07015 54.65602 # 2 14.07015 54.65602 # 3 20.50664 71.36617 # 4 16.74545 67.37352 # AvgDailyOzoneExposure AvgDailyOzoneExposure_StudyAvg # 1 36.97466 46.99595 # 2 18.62635 46.99595 # 3 55.75696 38.58856 # 4 38.33320 40.13805 # AvgDailyOzoneExposure_StudyMax MaxDailyOzoneExposure # 1 128.36066 42.67887 # 2 128.36066 28.94073 # 3 69.68106 94.79951 # 4 76.71777 67.85281 # MaxDailyOzoneExposure_StudyAvg MaxDailyOzoneExposure_StudyMax # 1 61.60031 198.8417 # 2 61.60031 198.8417 # 3 55.08151 103.9576 # 4 55.48645 115.6936 # AvgDailyPM2.5Exposure_2 MaxDailyOzoneExposure_2 AvgDailyCOExposure_2 # 1 9.274049 36.54099 213.2262 # 2 9.498967 35.70597 358.0131 # 3 8.238118 39.11024 212.4620 # 4 8.300655 38.53758 193.7690 # AvgDailyNOExposure_2 AvgDailyNO2Exposure_2 AvgDailyNOxExposure_2 # 1 3.048349 11.539853 14.58820 # 2 14.655994 22.662306 37.31830 # 3 2.493993 10.630901 13.12489 # 4 1.840359 8.475516 10.31587 # AvgDailySO2Exposure_2 AvgDailyAcetaldehydeExposure_2 # 1 1.956961 0.6987113 # 2 3.789237 0.9583015 # 3 1.613538 0.8120486 # 4 1.097606 0.7321192 # AvgDailyFormaldehydeExposure_2 AvgDailyBenzeneExposure_2 ObesityBMI # 1 0.9163324 NA 0 # 2 1.3879702 NA 0 # 3 1.0863108 NA 0 # 4 0.9295859 NA 0 # TotalEDVisits TotalInpatientVisits TotalEDInpatientVisits Sex2 Sex # 1 0 0 0 Male Male # 2 0 0 0 Male Male # 3 0 0 0 Male Male # 4 0 0 0 Male Male # Race Ethnicity MajorRoadwayHighwayExposure RoadwayDistanceExposure # 1 Unknown Unknown 227.9604 12.42057 # 2 Unknown Unknown 2098.2448 301.82455 # 3 Unknown Unknown 336.5802 252.79614 # 4 Unknown Unknown 460.0862 462.22059 # RoadwayType RoadwayAADT RoadwaySpeedLimit RoadwayLanes CAFO_Exposure # 1 NA NA NA NA 574089.1 # 2 NA NA NA NA 567777.9 # 3 NA NA NA NA 472817.7 # 4 NA NA NA NA 572968.6 # Landfill_Exposure EstResidentialDensity # 1 819888.5 NA # 2 813577.6 NA # 3 732120.1 NA # 4 826018.3 NA # EstProbabilityHighSchoolMaxEducation EstProbabilityNoHealthIns # 1 NA NA # 2 NA NA # 3 NA NA # 4 NA NA # EstProbabilityHouseholdNonHispWhite EstProbabilityESL # 1 NA NA # 2 NA NA # 3 NA NA # 4 NA NA # EstProbabilityNonHispWhite EstHouseholdIncome EstResidentialDensity25Plus # 1 NA NA NA # 2 NA NA NA # 3 NA NA NA # 4 NA NA NA # EstProbabilityNoAuto ur Estradiol TesticularDysfunctionDx ProstateCancerDx # 1 NA NA 0 0 0 # 2 NA NA 0 0 0 # 3 NA NA 0 0 0 # 4 NA NA 0 0 0 # AnxietyDx Formoterol FibromyalgiaDx Metaproterenol Indacaterol # 1 0 0 0 0 0 # 2 0 0 0 0 0 # 3 0 0 0 0 0 # 4 0 0 0 0 0 # OvarianDysfunctionDx Fluoxetine PneumoniaDx Triptorelin Omalizumab # 1 0 0 0 0 0 # 2 0 0 0 0 0 # 3 0 0 0 0 0 # 4 0 0 0 0 0 # Mepolizumab Fluticasone Sertraline Budesonide AlcoholDependenceDx # 1 0 0 0 0 0 # 2 0 0 0 0 0 # 3 0 0 0 0 0 # 4 0 0 0 0 0 # Flunisolide Nandrolone Venlafaxine Androstenedione Arformoterol # 1 0 0 0 0 0 # 2 0 0 0 0 0 # 3 0 0 0 0 0 # 4 0 0 0 0 0 # MenopauseDx Diphenhydramine Paroxetine DrugDependenceDx DepressionDx # 1 0 0 0 0 0 # 2 0 0 0 0 0 # 3 0 0 0 0 0 # 4 0 0 0 0 0 # Mometasone AutismDx Progesterone Prednisone Escitalopram CroupDx Goserelin # 1 0 0 0 0 0 0 0 # 2 0 0 0 0 0 0 0 # 3 0 0 0 0 0 0 0 # 4 0 0 0 0 0 0 0 # Cetirizine Citalopram CoughDx EndometriosisDx TesticularCancerDx # 1 0 0 0 0 0 # 2 0 0 0 0 0 # 3 0 0 0 0 0 # 4 0 0 0 0 0 # UterineCancerDx Fexofenadine Salmeterol Prasterone Duloxetine Estrogen # 1 0 0 0 0 0 0 # 2 0 0 0 0 0 0 # 3 0 0 0 0 0 0 # 4 0 0 0 0 0 0 # PregnancyDx Ciclesonide ObesityDx Testosterone Beclomethasone # 1 0 0 0 0 0 # 2 0 0 0 0 0 # 3 0 0 0 0 0 # 4 0 0 0 0 0 # OvarianCancerDx ReactiveAirwayDx Medroxyprogresterone DiabetesDx # 1 0 0 0 0 # 2 0 0 0 0 # 3 0 0 0 0 # 4 0 0 0 0 # Hydroxyzine AlopeciaDx Propranolol Theophylline KidneyCancerDx AsthmaDx # 1 0 0 0 0 0 0 # 2 0 0 0 0 0 0 # 3 0 0 0 0 0 0 # 4 0 0 0 0 0 0 # Metformin Albuterol Tamoxifen Estropipate Leuprolide CervicalCancerDx # 1 0 0 0 0 0 0 # 2 0 0 0 0 0 0 # 3 0 0 0 0 0 0 # 4 0 0 0 0 0 0 # Histrelin Ipratropium Trazodone AgeStudyStart Active_In_Year # 1 0 0 0 14 1 # 2 0 0 0 17 1 # 3 0 0 0 20 1 # 4 0 0 0 17 1 The deidentified ICEES2DEI CSV file may be used for further analysis. For instance, below we plot AvgDailyPM2.5Exposure vs MaxDailyOzoneExposure to examine the relationship between maximum daily ozone exposure and average daily PM2.5 exposure, noting a strong correlation is not expected for this tutorial, as we randomly sampled the exposures data. ggplot(icees2dei, aes(y = AvgDailyPM2.5Exposure, x = MaxDailyOzoneExposure)) + geom_point(color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) 7.3 Considerations FHIR PIT must be run within a secure enclave when working with real patient datasets, and all federal and institutional regulations surrounding patient privacy and data security must be met. FHIR PIT performs linkages between patient datasets and environmental exposures datasets using geocodes (i.e., patient primary residence), study period, and healthcare visit dates. FHIR PIT currently assumes a patient or subject resides in a single geolocation across a defined study period. We plan to adapt FHIR PIT to address mobility by, for example, assessing school-aged children’s home versus public school exposures. Note that the FHIR PIT application may be memory-intensive, depending on the size of the input datasets. For the sample input data, this tutorial requires approximately 4 GB RAM to run successfully, but the memory requirements may be greater with large input datasets. Finally, FHIR PIT, by name and function, ingests FHIR files as the preferred CDM. For users who are working with the PCORnet CDM, we offer two tools to map PCORnet to FHIR: pcornet-to-fhir (https://github.com/RENCI/tx-pcornet-to-fhir) and CAMP FHIR (https://github.com/NCTraCSIDSci/camp-fhir). CAMP FHIR additionally maps i2b2 and OMOP to FHIR. Both tools are openly available. 7.4 Concluding Remarks This tutorial demonstrates how to execute FHIR PIT and analyze its output. The patient dataset used in this tutorial is synthetic and intended for demonstration purposes only. The environmental datasets used in this tutorial are randomly sampled from much larger datasets derived from public sources. Users may substitute the synthetic patient dataset and sample exposures datasets with their own datasets. To add your own data, please update the ‘data/input’ directory. For assistance with FHIR PIT or access to our environmental exposure datasets, please contact fhir-pit@renci.org. References "],["case-studies.html", "Case Studies", " Case Studies This unit provides example case studies that analyze integrated wildfire-related data and other environmental exposures data with health outcomes data. Please note that the CHORDS Toolkit is a work in progress. This unit is currently in development. "],["appendix.html", "Appendix", " Appendix "],["chapter-user-profiles.html", "A User Profiles Index Clinical Data Manager Clinical Researcher Clinician/Medical Professional Community Health Worker Educator Epidemiologist Geospatial Analyst Public Health Official Social &amp; Behavioral Scientist Student Translational Researcher ", " A User Profiles The CHORDS Toolkit chapters have been developed for various user profiles. This appendix describes the user profiles and suggests relevant chapters for each. Index Clinical Data Manager | Clinical Researcher | Clinician/Medical Professional | Community Health Worker | Educator | Epidemiologist | Geospatial Analyst | Public Health Official | Social &amp; Behavioral Scientist | Student | Translational Researcher Clinical Data Manager What is my area of expertise? Advanced knowledge of health data systems Familiarity with some data sources for exposure and sociodemographic information Advanced data collection, management, and analysis skills What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Readily computed sociodemographic metrics Exposure and sociodemographic measures that are directly comparable with related measures from other studies Relevant Chapters: NASA EarthData Download Linkage to Census Units A FHIR PIT Tutorial Clinical Researcher What is my area of expertise? In-depth knowledge of health data systems Background knowledge of the key exposures and risk factors that influence the mechanism of the health outcome General knowledge of the main data sources for exposure, health, and sociodemographic information Advanced data collection, management, and analysis skills Statistical methods to examine population health trends What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Exposure measures that are directly comparable with related measures from other studies Methods to integrate measures across different spatial geographies Methods to readily assess known and predicted impacts of exposures on human health Relevant Chapters: A FHIR PIT Tutorial Clinician/Medical Professional What is my area of expertise? Advanced knowledge of health conditions and mechanisms of disease Background knowledge of key exposures and risk factors that influence the mechanism of the health outcome Familiarity with some data sources for exposure and sociodemographic information Advanced knowledge of health data systems What resources might I need? Readily computed exposure metrics Readily computed sociodemographic metrics Health data that capture information on the diseases or conditions of interest Exposure, sociodemographic, and health measures that are directly comparable with related measures from other studies Methods for collecting, managing, and analyzing data Strategies for interpreting analytical results Relevant Chapters: Geospatial Data Foundations Unit A FHIR PIT Tutorial Linkage to Census Units Community Health Worker What is my area of expertise? Familiarity with general exposures and related health outcomes of interest Familiarity with some data sources for exposure, health, and sociodemographic information Communication of important health information to the public and other stakeholders What resources might I need? Readily computed exposure metrics Readily computed sociodemographic metrics Health data that capture information on the diseases or conditions of interest Key findings or strategies for interpreting analytical results Data visualizations and other related tools Relevant Chapters: Linkage to Census Units Educator What is my area of expertise? In-depth knowledge of health data systems Familiarity with some data sources for exposure, health, and sociodemographic information Advanced data collection, management, and analysis skills What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Readily computed sociodemographic metrics Exposure and sociodemographic measures that are directly comparable with related measures from other studies Relevant Chapters: NASA EarthData Download Epidemiologist What is my area of expertise? Background knowledge of the key risk factors that influence the mechanism of the health outcome General knowledge of the main sources of data for exposures and health information Fundamental data collection, management, and analysis skills Statistical methods to examine population health trends and patterns Interpretation of analytical findings for broader public health applications What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Exposure measures that are directly comparable with related measures from other studies Health data that capture information on the disease or condition of interest Methods to integrate measures across different spatial geographies Methods to readily identify known and predicted impacts of exposures on human health Geospatial Analyst What is my area of expertise? Background knowledge of the key sources and patterns of exposure of interest General knowledge of the main sources of geospatial data for exposures Advanced data collection, management, and analysis skills Characterization and assessment of exposures What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Exposure measures that are directly comparable with related measures from other studies Methods to integrate measures across different spatial geographies Methods to calculate and predict exposures from multiple sources and time periods Relevant Chapters: NASA EarthData Download A FHIR PIT Tutorial Public Health Official What is my area of expertise? Familiarity with the exposures and typical health outcomes of interest Familiarity with general data collection, management, and analysis skills Communication of important health information to the public and other stakeholders What resources might I need? Readily computed exposure metrics Readily computed sociodemographic metrics Health data that capture information on the diseases or conditions of interest Key findings or strategies for interpreting analytical results Data visualizations and other related tools Social &amp; Behavioral Scientist What is my area of expertise? Background knowledge of the key exposures and risk factors that influence the mechanism of the health outcome General knowledge of the main data sources for exposure, health, and sociodemographic information Advanced data collection, management, and analysis skills Statistical methods to examine population health trends Interpretation of analytical findings for broader public health applications What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Exposure measures that are directly comparable with related measures from other studies Health data that capture information on the disease or condition of interest Methods to integrate measures across different spatial geographies Methods to readily identify known and predicted impacts of exposures on human health Relevant Chapters: Geospatial Data Foundations Unit A FHIR PIT Tutorial Student What is my area of expertise? Background knowledge of the typical health outcomes of interest and key exposures and risk factors influencing these health outcomes General knowledge of the main data sources for exposure, health, and sociodemographic information Fundamental data collection, management, and analysis skills Familiarity with statistical methods to examine population health trends What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Exposure measures that are directly comparable with related measures from other studies Health data that capture information on the disease or condition of interest Methods to integrate measures across different spatial geographies Methods for collecting, managing, and analyzing data Relevant Chapters: Geospatial Data Foundations Unit Linkage to Census Units Translational Researcher What is my area of expertise? Background knowledge of the key exposures and risk factors that influence the mechanism of the health outcome General knowledge of the main data sources for exposure, health, and sociodemographic information Advanced data collection, management, and analysis skills Statistical methods to examine population health trends Translating analytical findings into interventions and treatments What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Exposure measures that are directly comparable with related measures from other studies Health data that capture information on the disease or condition of interest Methods to integrate measures across different spatial geographies Methods to readily assess known and predicted impacts of exposures on human health "],["chords-glossary.html", "B CHORDS Glossary Index", " B CHORDS Glossary Key terms used in the CHORDS Toolkit are defined below and in the NIEHS Climate Change and Human Health Glossary. Index A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X | Y | Z E Electronic Health Record (EHR) The digital files describing a patient’s medical history that are maintained by a health provider. EHRs contain information such as demographics, progress notes, diagnoses, medications, and test results. Context: EHRs are common data sources in various environmental health applications. Categories: Health-Related, Health Outcomes Related Terms: FHIR F FHIR A technical standard for health care data exchange. Source: HL7. Context: FHIR-based tools can be used to process data from EHRs for use in environmental health applications. Categories: Health-Related, Health Outcomes Related Terms: Electronic Health Record G Geocoded Address The geographic coordinates (i.e., latitude and longitude) associated with a street address (e.g., home or work location). Context: Geocoded addresses are commonly used as a basis for geospatial exposure assessment for individuals. Categories: Research Methods Related Terms: Geocoding Geocoding The process of converting place names or street addresses to geographic coordinates (i.e., latitude and longitude). Context: Geocoding is a common data processing step in geospatial exposure assessment. Categories: Research Methods Related Terms: Geocoded Address Geographic Unit (Areal Unit) A specific place defined by administrative or political geographic boundaries (e.g., state, census tract, country, postal code), environmental boundaries (e.g., watershed) or a grid (e.g., grid cell in a 1 km grid). Context: Data for social and environmental determinants of health are often provided for different types of geographic units (e.g., census tracts, grid cells). Categories: Research Methods GeoID A unique identifying code for a place or geographic unit (e.g., census tract, city, postal code, land parcel, watershed). Context: GeoID is commonly used as a basis for data integration at the individual and population level. Categories: Research Methods Related Terms: Geographic Unit, Data Integration Geospatial Data Data that includes information about specific times and places on the Earth. Context: Geospatial data can be used to assess individual and population level exposures to environmental and social determinants of health. Categories: Research Methods "],["all-references-appendix.html", "C References", " C References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
