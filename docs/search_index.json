[["index.html", "The CHORDS Toolkit for Health and Geospatial Exposures Research Introduction About CHORDS About This Toolkit", " The CHORDS Toolkit for Health and Geospatial Exposures Research Introduction Research into the effects of the environment on human health at the level of individuals and populations entails pulling together a combination of health data and data on environmental exposures. One approach to gathering exposure information is to compute estimated exposure levels based on the location of individuals at different time points. For instance, based on location and time one can estimate the level of noise around a person, the level of pollutants present in the air, and the presence of parks and other green spaces. Estimating these exposures can be challenging, especially if one is not familiar with the sources of data used to make these estimations or the analysis methods needed to compute the estimated exposures. This can be especially challenging for time varying exposures, exposures that depend on complex models such as the composition and dispersion of chemicals in the air, and for researchers seeking to study exposure effects at the individual level. This resource seeks to aid researchers in adopting geospatial data-based exposures into their research by providing guides, tools, and example code. About CHORDS The Climate and Health Outcomes Research Data Systems (CHORDS) program provides resources aimed at making it easier for researchers to study the effects of external, place-based environmental exposures on health outcomes. The CHORDS resources include a web-based data catalog, standardized data sets, and this toolkit. About This Toolkit Getting Started This toolkit provides guides, tools, and example code in R. The CHORDS Toolkit GitHub repository provides the underlying code and data for this book. The CHORDS toolkit has been developed to support different types of users, such as students, clinicians, and data managers. Please see the User Profile Appendix for descriptions and suggested relevant toolkit chapters for each user profile. This toolkit does require familiarity with the R programming language and the use of R for working with, visualizing, and analyzing scientific data. The TAME Toolkit Chapter 1 provides resources for getting started with R for environmental health research. The toolkit consists of a series of chapters organized into the following units: Foundations: This unit provides background, guidance and example code for working with different types of geospatial data common in environmental health research. This unit is intended as a starting point for users with less familiarity with geospatial data analysis methods in environmental health. Wildfire Data: This unit provides guidance for working with different types of wildfire-related data in climate change and health research. Other Environmental Data: This unit provides guidance and code for working with specific sources of environmental data common in environmental health research for characterizing environmental exposures as well as social determinants of health. Health Data Integration: This unit provides guidance and code for integrating environmental and health data in climate change and health research, both at the individual level and at the population level. Case Studies: This unit provides example case studies that analyze integrated wildfire-related data with health outcomes data. The following toolkit chapters are currently in development: Unit Chapter Topic Status Foundations Mapping Geospatial Data In Development Foundations Calculating Geospatial Covariates In Development Funding This resource was supported by the National Institutes of Health (NIH) from the National Institute of Environmental Health Sciences (NIEHS), by the NIH Office of Data Science Strategy (ODSS) Data Scholar Program, by the Patient Centered Outcomes Research Trust Fund (PCORTF) and the Department of Health and Human Services (DHHS) Office of Assistant Secretary for Planning and Evaluation. Authors Suggested Citation Test Citation (CHORDS 2024) Test Citation (Hernangomez 2023) References "],["foundations.html", "Foundations", " Foundations This unit provides guidance and example code for working with different types of geospatial data common in environmental health research. "],["chapter-intro-spatial-data-analysis.html", "1 Spatial Data Analysis 1.1 Introduction 1.2 Point Data with sf 1.3 Polygon Data 1.4 Raster Data with terra 1.5 Additional Resources 1.6 References", " 1 Spatial Data Analysis Access, Import, and Primary Analyses with Environmental Data in R Date Modified: February 1, 2024 Authors: Mitchell Manware, Kyle P Messier Key Terms: Geospatial Data Programming Language: R 1.1 Introduction 1.1.1 Motivation Environmental health research relies on various types of data to accurately measure, model, and predict exposures. Environmental data are often spatial (related to the surface of the Earth), temporal (related to specific time/period of time), or spatio-temporal (related to the surface of the Earth for a specific time/period of time). These data are at the core of environmental health research, but the steps between identifying a spatial data set or variable and using it to help answer a research question can be challenging. This vignette is designed to introduce the necessary steps for conducting analyses with spatial data in R. It will introduce R packages that are equipped to handle spatial data, and will demonstrate how to access, import, and analyze three different types of spatial data. The vignette will focus primarily on spatial data, but some aspects of temporal and spatio-temporal data will also be discussed. 1.1.2 Objectives Users will learn about the following topics related to spatial data in R: Point, polygon, and raster data types Downloading data from a URL Importing data Checking data type, structure, and class Reclassifying data Computing summary and zonal statistics Plotting individual and multiple data sets 1.1.3 Data Types This vignette will cover how to access, import, and analyze point, polygon, and raster spatial data types. The details of what constitutes each unique spatial data type, however, will not be covered. For detailed descriptions of each type of spatial data, please see Simple Features for R for point and polygon data types, and Introduction to Raster Data for raster data. 1.1.4 Data Sources The exploratory analyses performed in this vignette utilize free and publicly available environmental data. The code chunks are designed to access each specific file used for the exploratory analyses, but a description of each data source and data set is available below. Exploratory analyses data sources Data Data Type Producer Link PM2.5 Daily Observations Point Environmental Protection Agency (EPA) https://aqs.epa.gov/aqsweb/airdata/download_files.html Wildfire Smoke Plumes Polygon National Oceanic and Atmospheric Administration (NOAA) https://www.ospo.noaa.gov/Products/land/hms.html United States Cartographic Boundary Polygon United States Census Bureau https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html Land Surface Temperature Raster National Oceanic and Atmospheric Administration (NOAA) https://psl.noaa.gov/data/gridded/data.narr.html 1.1.5 Packages Various R packages can be used to create, import, analyze, and export spatial data. If you have not used these packages previously, they may not be installed on your machine. The following chunk of code installs and imports the packages required to conduct the exploratory analyses in this vignette. Installing and importing new packages may required R to restart. vignette_packages &lt;- c( &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;ggpubr&quot;, &quot;sf&quot;, &quot;terra&quot;, &quot;tidyterra&quot;, &quot;utils&quot; ) for (v in seq_along(vignette_packages)) { if (vignette_packages[v] %in% installed.packages() == FALSE) { install.packages(vignette_packages[v]) } } library(dplyr) library(ggplot2) library(ggpubr) library(sf) library(terra) library(tidyterra) library(utils) ggplot2 and ggpubr The ggplot2 and ggpubr packages will be used throughout the vignette for creating publication quality plots. Please see ggplot2: Elegant Graphics for Data Analysis (3e) and ggpubr: ‘ggplot2’ Based Publication Ready Plots for in depth descriptions of the syntax and functionality utilized by these packages. The exploratory analyses performed in this vignette are designed for educational purposes only. The results of the following analyses are not peer-reviewed findings, nor are they based on any hypotheses. 1.2 Point Data with sf Air pollution monitoring data from the United States Environmental Protection Agency (EPA) will be used to demonstrate using point data with the sf package. 1.2.1 Access, download, and unzip To download data with the utils::download.file() function, define two variables. One variable to store the website URL where the data exists and a second to store the file path for where the file should be saved. Multiple chunks of code in this vignette will contain / YOUR FILE PATH /. To run the code on your machine, substitute / YOUR FILE PATH / with the file path where you would like to store the vignette data. url_epa &lt;- &quot;https://aqs.epa.gov/aqsweb/airdata/daily_88101_2021.zip&quot; destination_epa &lt;- &quot;/ YOUR FILE PATH /epa_data.zip&quot; download.file( url_epa, destination_epa ) The file downloaded from the EPA website is a zip file. Zip files need to be unzipped (decompressed) in order to access the data within. Unzip the EPA air pollution file with utils::unzip(). Unzipping a .zip file will decompress the contents within. Spatial data sets can be very large (ie. &gt; 1GB ), so check the size of the data before unzipping on your machine. The numeric value size of the file is listed under Length. unzip(&quot;/ YOUR FILE PATH /epa_data.zip&quot;, list = TRUE ) After inspecting the file size, unzip epa_data.zip. unzip(&quot;/ YOUR FILE PATH /epa_data.zip&quot;) Inspecting the file with utils::unzip(list = TRUE) returned the size of the file, but also the name of the data file of interest. The desired data file can also be identified with list.files(). Other file names may be returned if / YOUR FILE PATH / is a directory with other contents (ie. Desktop or Documents). list.files(&quot;/ YOUR FILE PATH /&quot;) 1.2.2 Import Now that the contents of the zip file have been saved on your machine and the data file of interest has been identified, import the data with sf::st_read(). pm &lt;- st_read(&quot;/ YOUR FILE PATH /daily_88101_2021.csv&quot;) The previous chunk of code returned a Warning: message. This warning informs the user that the imported data does not have native spatial features, so the data was imported as a data.frame. 1.2.3 Inspect structure Inspect the structure of pm to see its class, column names, column classes, and the first two (specified by vec.len = 2) data points from each column. str(pm, vec.len = 2 ) 1.2.4 Subset Checking the data’s structure shows that pm is a very large data set. Each of the variables convey important information related to air pollution monitoring, but not all will be utilized in these exploratory analyses. The data set can be reduced to include only the variables of interest with the subset() function. The select = argument indicates which variables to be retained in the new data set. Re-running str(pm) after running the subset shows that all observations (n = 590208) of the desired variables (n = 8) have been retained. pm &lt;- subset(pm, select = c( State.Code, County.Code, Site.Num, Latitude, Longitude, State.Name, Date.Local, Arithmetic.Mean )) Re-running str(pm) after subsetting the data set shows that all all observations (n = 590208) of the variables of interest (n = 8) have been retained. str(pm, vec.len = 2 ) 1.2.5 Reclassify The str() function showed the class of each variable within the data set. All of the retained variables are of class character, indicated by : chr and the quotations around each observation (\"01\" \"01\" ...). The class of a variable depends on the information conveyed by the data stored within that variable. For example, character is an appropriate class for the pm$State.Name variable because each observation is a character string labeling in which state the monitor was located. Alternatively, character is not appropriate for the pm$Arithmetic.Mean or pm$Date.Local variables because each observation is a numeric decimal or time-referenced date, respectively. The as. functions can be used for reclassifying data. Reclassify pm$Arithmetic.Mean as a number, and pm$Date.Local as a date. pm$Arithmetic.Mean &lt;- as.numeric(pm$Arithmetic.Mean) pm$Date.Local &lt;- as.Date(pm$Date.Local) After running the as. functions, ensure that the two variables have been reclassified to the desired classes. class(pm$Arithmetic.Mean) class(pm$Date.Local) 1.2.6 Convert to sf object With the variables of interest selected and reclassified, pm can be converted to spatially defined sf object. The sf::st_as_sf() function creates a $geometry field based on the latitude and longitude coordinates contained within pm. The coords = c() argument specifies the column names which contain the coordinate values. The columns containing coordinate values will not always be “Latitude” and “Longitude”. Use str() to see column names and identify which contain the coordinate values. pm_sf &lt;- st_as_sf(pm, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;) ) Inspect the classes of pm_sf and pm_sf$geometry to see their differences, and how each are different than class(pm). class(pm_sf) class(pm_sf$geometry) class(pm_sf) returned both \"sf\" and \"data.frame\", indicating that it contains both spatial and non-spatial data. 1.2.7 Coordinate reference system and projection The coordinate reference system of an sf object can be checked with sf::st_crs(). st_crs(pm_sf) The previous chunk of code shows that pm_sf does not have a native coordinate reference system. The same function, sf::st_crs(), can be used to assign a coordinate reference system to an sf object. For this example, the World Geodesic System 1984 (WGS 84) will be used (EPSG code: 4326). st_crs(pm_sf) &lt;- 4326 st_crs(pm_sf) An sf object with a coordinate reference system can be transformed (projected) into a different coordinate reference system with sf::st_transform(). The area of interest for these exploratory analyses is the coterminous United States, so the Albers Equal Area projected coordinate system will be used (EPSG code: 5070). An sf object without an assigned coordinate reference system cannot be transformed. sf::st_crs() must be used to assign a coordinate reference system to an sf object that does not have one. pm_sf &lt;- st_transform( pm_sf, 5070 ) 1.2.8 Plot Plotting spatial data is important for visualizing and analyzing patterns in the data. Initialize a plot for the locations of each air pollution monitoring station with ggplot2::ggplot(). Identifying the data set to be plotted within the geom_sf() argument informs the function that the data is an sf object. ggplot() + geom_sf(data = pm_sf) + ggtitle(&quot;Air Pollution Monitor Locations&quot;) + theme_pubr() + theme(plot.title = element_text(hjust = 0.5)) + grids() The plot shows the distribution of monitoring locations, and roughly depicts the outline of the United States due to the large number of monitors. The plot does not, however, convey any information about the concentration of PM2.5 measured by each monitor. Inspect the summary statistics of the PM2.5 measurements before creating any plots to visualize the data. summary(pm_sf$Arithmetic.Mean) sd(pm_sf$Arithmetic.Mean) After inspecting the summary statistics, create a histogram of the PM2.5 concentration measurements to visualize the distribution of the data. The histogram is not a spatially defined plot, so the data set to be plotted is identified within ggplot(). ggplot( data = pm_sf, aes(Arithmetic.Mean) ) + geom_histogram( fill = &quot;blue&quot;, binwidth = 5 ) + ggtitle( expression(&quot;PM&quot;[2.5] * &quot; Concentration Measurements&quot;) ) + xlab(expression(&quot;PM&quot;[2.5] * &quot; Concentration (µg/m&quot;^3 * &quot;)&quot;)) + ylab(&quot;Number of Measurements&quot;) + theme_pubr() + theme(plot.title = element_text(hjust = 0.5)) + grids() 1.2.9 Calculate annual mean A common summary statistic of interest to researchers is the mean over a certain period of time. For this example, we will calculate the mean PM2.5 concentration for each monitoring location for the year 2021. To do this, a unique identification code must be created for each monitoring location. The pm_sf$Monitor.ID variable can be created by concatenating each monitor’s state code, county code, and site number together into a single character string. pm_sf$Monitor.ID &lt;- paste0( pm_sf$State.Code, pm_sf$County.Code, pm_sf$Site.Num ) Each unique monitor identification code should be matched with a unique monitor location geometry. To ensure that each monitor location has a unique identification code, check that there are an equal number of unique geometries and identification codes. length(unique(pm_sf$Monitor.ID)) == length(unique(pm_sf$geometry)) Now that each monitor location as a unique identification code, we can calculate the mean PM2.5 concentration measured at each monitoring location. Functions and syntax from the dplyr package will be used to do this. For more on the dplyr package, please see Introduction to dplyr. The group_by(Monitor.ID, ) argument specifies that an annual mean should be calculated for each unique Monitor.ID. Including State.Name in this argument retains the column in the new pm_mean data set, but does not influence the calculation of the annual mean. pm_mean &lt;- pm_sf %&gt;% group_by(Monitor.ID, State.Name) %&gt;% summarise(Annual.Mean = mean(Arithmetic.Mean)) Inspect the summary statistics of pm_mean. summary(pm_mean$Annual.Mean) sd(pm_mean$Annual.Mean) Create a plot which shows the distribution of monitoring locations, and color each point according to the monitor’s annual mean concentration of PM2.5. ggplot() + geom_sf( data = pm_mean, aes(color = Annual.Mean) ) + scale_color_viridis_b( expression(&quot;PM&quot;[2.5] * &quot; Concentration (µg/m&quot;^3 * &quot;)&quot;) ) + ggtitle( expression(&quot;Annual Mean PM&quot;[2.5] * &quot; Concentration&quot;) ) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() Now the plot depicts both spatial and non-spatial data. 1.2.10 Compare highest annual means A close visual inspection of the previous plot shows a few monitoring locations in the southwestern region of the United States with very high (&gt; 20 µm/m3) annual mean concentrations of PM2.5. To investigate the differences between the monitors with the highest and lowest annual mean concentrations, create a subset of pm_sf with only the three highest and lowest monitors. To do this, first identify the monitors with the highest and lowest annual mean concentrations. min_monitors &lt;- pm_mean %&gt;% arrange(Annual.Mean) %&gt;% head(n = 3) max_monitors &lt;- pm_mean %&gt;% arrange(Annual.Mean) %&gt;% tail(n = 3) Next, create a variable storing only the unique identification codes of these six monitors. min_max_monitors_id &lt;- c( min_monitors$Monitor.ID, max_monitors$Monitor.ID ) Finally, subset the pm_sf data set according to the monitor identification codes stored in min_max_monitors_id. pm_min_max &lt;- subset(pm_sf, subset = Monitor.ID == min_max_monitors_id ) The resulting pm_min_max data set contains data for only six monitoring locations. Check the unique monitor identification codes that constitute the new data set. unique(pm_min_max$Monitor.ID) The temporal trend of PM2.5 concentrations measured at each of these locations in 2021 can be depicted with ggplot::geom_line(). ggplot( data = pm_min_max, aes( x = Date.Local, y = Arithmetic.Mean, group = Monitor.ID, color = Monitor.ID ) ) + geom_line() + ggtitle(&quot;Minimum and Maximum Monitors&quot;) + xlab(&quot;Date&quot;) + ylab(expression(&quot;PM&quot;[2.5] * &quot; Concentrations (µg/m&quot;^3 * &quot;)&quot;)) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) Alternatively, the ggplot2::geom_boxplot() function compares the median, interquartile range, and outliers of the monitors’ measurements. ggplot( data = pm_min_max, aes( x = Monitor.ID, y = Arithmetic.Mean, fill = Monitor.ID ) ) + geom_boxplot() + xlab(&quot;Monitor ID&quot;) + ylab(expression(&quot;PM&quot;[2.5] * &quot; Concentrations (µg/m&quot;^3 * &quot;)&quot;)) + theme_pubr(legend = &quot;none&quot;) 1.3 Polygon Data Wildfire smoke plume coverage data from the United States National Oceanic and Atmospheric Administration (NOAA) will be used to demonstrate using polygon data. This section will cover polygon data with both the sf and terra packages separately, but the steps for accessing, downloading, and unzipping the data is the same for both packages. 1.3.1 Access, download, and unzip The website URL where the NOAA wildfire smoke plume data exists is date-specific, meaning there is a unique URL for each daily data set. For the purpose of these exploratory analyses, wildfire smoke plume data from September 1, 2023 will be used. Define three variables for day, month, and year according to the date of interest. day &lt;- &quot;01&quot; month &lt;- &quot;09&quot; year &lt;- &quot;2023&quot; The utils::download.file() function downloads the file according to the defined URL and destination file. url_noaa &lt;- paste0( &quot;https://satepsanone.nesdis.noaa.gov/pub/FIRE/web/HMS/Smoke_Polygons&quot;, &quot;/Shapefile/&quot;, year, &quot;/&quot;, month, &quot;/hms_smoke&quot;, year, month, day, &quot;.zip&quot; ) destination_noaa &lt;- paste0( &quot;/ YOUR FILE PATH /noaa_smoke&quot;, year, month, day, &quot;.zip&quot; ) download.file( url_noaa, destination_noaa ) The file downloaded from the NOAA website is a .zip file. Zip files need to be unzipped (decompressed) in order to access the data within. Unzip the NOAA wildfire smoke plume coverage file with utils::unzip(). Unzipping a .zip file will decompress the contents within. Spatial data sets can be very large (ie. &gt; 1GB), so check the size of the data before unzipping on your machine. The numeric value size of each file is listed under Length. unzip(&quot;/ YOUR FILE PATH /noaa_smoke20230901.zip&quot;, list = TRUE ) After inspecting the file sizes, unzip noaa_smoke20230901.zip. unzip(&quot;/ YOUR FILE PATH /noaa_smoke20230901.zip&quot;) Inspecting the file with utils::unzip(list = TRUE) returned the size of the file, but also the name of the data file of interest. The desired data file can also be identified with list.files(). list.files(&quot;/ YOUR FILE PATH /&quot;) Listing the contents of the unzipped file reveals four individual files. The data to be imported is stored in the hms_smoke20230901.shp, but the other files contain important information for the .shp file. Deleting any of the supporting files (ie. *.dbf, *.prj, or *.shx) will disrupt the data import. 1.3.2 Polygon Data with sf This section will focus on exploratory analyses with polygon data using the sf package. Import Now that the contents of the zip file have been saved on your machine and the data file of interest has been identified, import the data with sf::st_read(). Although the supporting files are required to import a shapefile, only the file ending in .shp needs to be imported smoke_sf &lt;- st_read(&quot;/ YOUR FILE PATH /hms_smoke20230901.shp&quot;) Importing hms_smoke20230901.shp does not return a Warning: message because the data set has native spatial features, and is therefore imported as an sf object. Inspect structure Inspect the structure of smoke_sf to see its class, column names, column classes, and the first two (specified by vec.len = 2) data points. str(smoke_sf, vec.len = 2 ) As mentioned previously, the smoke_sf data set has native spatial features. These are reflected by the data set having classes of sf and data.frame, and the $geometry feature. class(smoke_sf) Reclassify The main parameter of interest in this data set is $Density, which discretely categorizes each wildfire smoke plume as “Light”, “Medium”, or “Heavy”. Checking its class shows that $Density is class character. class(smoke_sf$Density) Nominal data, data without fixed order or rank system, can be stored as class character (ie. State names). However, it is best to store ordinal data as class factor for conducting analyses in R. Converting data from class character to class factor can be done with factor(). The levels = c() argument in the function specifies both the level names and the ranked order of the levels. smoke_sf$Density &lt;- factor(smoke_sf$Density, levels = c(&quot;Light&quot;, &quot;Medium&quot;, &quot;Heavy&quot;) ) Check the class of $Density again to ensure proper reclassification. class(smoke_sf$Density) Coordinate reference system and projection Check the coordinate reference system of an sf object with sf::st_crs(). st_crs(smoke_sf) smoke_sf has a native coordinate reference system which was imported during the sf::st_read() step. The area of interest for these exploratory analyses is the coterminous United States, so we can transform smoke_sf to the Albers Equal Area projected coordinate system (EPSG code: 5070). smoke_sf &lt;- st_transform( smoke_sf, 5070 ) Plot (single) With the data prepared, plot the wildfire smoke plume polygons with ggplot2::ggplot(). Now that the parameters of interest and coordinate reference system have been prepared, create a plot with ggplot2::ggplot(). Identifying the data set to be plotted within the geom_sf() arguement informs the function that the data is an sf object. ggplot() + geom_sf( data = smoke_sf, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + ggtitle(&quot;Wildfire Smoke Plumes&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() The wildfire smoke plume polygons are clearly visible and colored according to their individual smoke density classification. This plot, however, is difficult to interpret for two reasons. First, there are multiple polygons for each smoke density classification. Multiple borders and overlapping polygons with the same smoke density type can be confusing. To make the polygons more clear, individual polygons for each smoke density classification can be combined. For the purposes of these exploratory analyses, the satellite travelling direction and time of collection will be ignored. Union Individual polygons can be unioned (combined) into one multi-part polygon with sf::st_union. The group_by(Density) argument specifies that the polygons should be combined based on the value stored in $Density. Adding the Date = paste0(... argument within the dplyr::summarise() function creates a parameter to store the date based on the year, month, and day of the data (defined in 2.0 Access, download, and unzip). smoke_sf_density &lt;- smoke_sf %&gt;% group_by(Density) %&gt;% summarise( geometry = st_union(geometry), Date = paste0( year, month, day ) ) The resulting data set contains three multi-polygons, a column for the smoke plume classification, and a column for the date. smoke_sf_density Creating a new plot with smoke_sf_density. ggplot() + geom_sf( data = smoke_sf_density, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + ggtitle(&quot;Wildfire Smoke Plumes (unioned)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() The plot is still difficult to interpret because it lacks geospatial context. The grid lines provide latitude and longitude references, but physical or geopolitical boundaries can help show where the wildfire smoke plumes are relative to the study area of interest. To provide geospatial context to the wildfire smoke plume polygons, we can add the United States’ state boundary polygons to the plot. Plot (multiple) The steps taken to access, download, unzip, and import the United States’ state boundary data are the same as those taken for the wildfire smoke plume coverage data. Refer to sections 2.0 Access, download, and unzip, and 2.1.1 Import for detailed descriptions. url_states &lt;- &quot;https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip&quot; destination_states &lt;- &quot;/ YOUR FILE PATH /states.zip&quot; download.file( url_states, destination_states ) unzip(&quot;/ YOUR FILE PATH /states.zip&quot;, list = TRUE ) unzip(&quot;/ YOUR FILE PATH /states.zip&quot;) list.files(&quot;/ YOUR FILE PATH /&quot;) states &lt;- st_read(&quot;/ YOUR FILE PATH /cb_2018_us_state.shp&quot;) Inspect the structure of states_sf. str(states_sf, vec.len = 2 ) For the purpose of these exploratory analyses, only the coterminous (CONUS) United States will be used. Subset states_sf to remove Alaska, Hawaii, and the United States’ territories. remove &lt;- c( &quot;Alaska&quot;, &quot;Hawaii&quot;, &quot;Puerto Rico&quot;, &quot;United States Virgin Islands&quot;, &quot;Commonwealth of the Northern Mariana Islands&quot;, &quot;Guam&quot;, &quot;American Samoa&quot; ) conus_sf &lt;- subset( states_sf, !NAME %in% remove ) Check the coordinate reference system. st_crs(conus_sf) When analyzing multiple spatial data sets together, all data sets must have the same coordinate reference system or projected coordinate system. Transform conus_sf to match the projected coordinate system of the smoke_sf_density data set. conus_sf &lt;- st_transform( conus_sf, st_crs(smoke_sf_density) ) Plot the coterminous United States’s state boundaries. ggplot() + geom_sf(data = conus_sf) + ggtitle(&quot;Coterminous United States&#39; State Boundaries&quot;) + theme_pubr() + theme(plot.title = element_text(hjust = 0.5)) + grids() With the wildfire smoke plume and coterminous United States polygons imported and prepared, ensure that they have the same coordinate reference system. st_crs(smoke_sf_density) == st_crs(conus_sf) Create a plot which shows the distribution of wildfire smoke plumes over the coterminous United States’ state boundaries. ggplot() + geom_sf( data = smoke_sf_density, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + geom_sf( data = conus_sf, fill = &quot;transparent&quot; ) + ggtitle(&quot;Wildfire Smoke Plumes (with states)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() This plot provides important geospatial context for understanding where the wildfire smoke plumes are in relation to the study area of interest. Crop The sf::st_crop() function can be used to reduce the extent of a set of polygons to a specific rectangle, typically the bounding box of another spatial data set. In this example we can crop the smoke_sf_density polygons to the bounding box surrounding the coterminous United States. smoke_sf_crop &lt;- st_crop( smoke_sf_density, conus_sf ) Plot the cropped wildfire smoke plume polygons and the coterminous United States’ state boundaries. ggplot() + geom_sf( data = smoke_sf_crop, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + geom_sf( data = conus_sf, fill = &quot;transparent&quot; ) + ggtitle(&quot;Wildfire Smoke Plumes (cropped)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() 1.3.3 Polygon Data with terra This section will focus on exploratory analyses with polygon data using the terra package. Import Now that the contents of the zip files have been saved on your machine and the data files of interest have been identified, import both the wildfire smoke plume coverage data and the United States’ state boundary data with terra::vect(). See sections 2.0 Access, download, and unzip and 2.1.7 Plot (multiple) for obtaining the wildfire smoke plume coverage and United States’ state boundary data sets, respectively. smoke_t &lt;- vect(&quot;/ YOUR FILE PATH /hms_smoke20230901.shp&quot;) states_t &lt;- vect(&quot;/ YOUR FILE PATH /cb_2018_us_state_500k.shp&quot;) Inspect structure Inspect the structures of smoke_t and states_t to see their classes, column names, column classes. smoke_t states_t Both smoke_t and states_t have native spatial features. These are reflected by the type of spatial data in geometry:, and the spatial attributes extent: and coord. ref.: Reclassify The main parameter of interest in this data set is $Density, which discretely categorizes each wildfire smoke plume as “Light”, “Medium”, or “Heavy”. Checking its class shows that $Density is class character. class(smoke_t$Density) Nominal data, data without fixed order or rank system, can be stored as class character (ie. State names). However, it is best to store ordinal data as class factor for conducting analyses in R. Converting data from class character to class factor can be done with factor(). The levels = c() argument in the function specifies both the level names and the ranked order of the levels. smoke_t$Density &lt;- factor(smoke_t$Density, levels = c(&quot;Light&quot;, &quot;Medium&quot;, &quot;Heavy&quot;) ) Check the class of $Density again to ensure proper reclassification. class(smoke_t$Density) For the purpose of these exploratory analyses, only the coterminous (CONUS) United States will be used. Subset states_t to remove Alaska, Hawaii, and the United States’ territories. remove &lt;- c( &quot;Alaska&quot;, &quot;Hawaii&quot;, &quot;Puerto Rico&quot;, &quot;United States Virgin Islands&quot;, &quot;Commonwealth of the Northern Mariana Islands&quot;, &quot;Guam&quot;, &quot;American Samoa&quot; ) conus_t &lt;- subset( states_t, !states_t$NAME %in% remove ) Coordinate reference system and projection Check the coordinate reference systems of SpatVector objects with terra::crs(). crs(smoke_t, describe = TRUE ) crs(conus_t, describe = TRUE ) Both data sets have native coordinate reference systems which were imported during the terra::vect() step. The two data sets, however, have different coordinate reference systems from each other. The area of interest for these exploratory analyses is the coterminous United States, so we can project smoke_t and conus_t to the Albers Equal Area projected coordinate system (EPSG code: 5070). smoke_t &lt;- project( smoke_t, &quot;EPSG:5070&quot; ) conus_t &lt;- project( conus_t, &quot;EPSG:5070&quot; ) Although both data sets were transformed to the same projected coordinate system, it is important to ensure that all data sets have the same coordinate reference system or projected coordinate system. crs(smoke_t) == crs(conus_t) Plot (multiple) Plot both data sets together in one plot with ggplot2::ggplot(). Now that the parameters of interest and coordinate reference systems have been prepared, create a plot with ggplot2::ggplot(). Identifying the data sets to be plotted within the geom_spatvector() arguments informs the function that the data are SpatVector objects. ggplot() + geom_spatvector( data = smoke_t, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot; ) + ggtitle(&quot;Wildfire Smoke Plumes (with states)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() The wildfire smoke plume polygons are clearly visible and colored according to their individual smoke density classification. The plot, however, is difficult to interpret because there are multiple polygons for each smoke density classification. Multiple borders and overlapping polygons with the same smoke density type can be confusing. To make the polygons more clear, individual polygons for each smoke density classification can be combined. For the purposes of these exploratory analyses, the satellite travelling direction and time of collection will be ignored. Aggregate Individual polygons an be aggregated (combined) into one multi-part polygon with terra::aggregate(). The by = \"Density\" argument specifies that the polygons should be combined based on the value stored in $Density. smoke_t_density &lt;- terra::aggregate(smoke_t, by = &quot;Density&quot;, dissolve = TRUE ) Aggregating the polygons based on the values stored in the $Density column can result in the other columns containing NA values. To remove these columns, subset smoke_t_density to remove $Satellite, $Start, and $End. smoke_t_density &lt;- smoke_t_density[ seq_len(nrow(smoke_t_density)), c(&quot;Density&quot;, &quot;agg_n&quot;) ] The resulting data set contains three multi-polygons, a column for the smoke plume classification, and a count of the number of individual polygons that were aggregated to create the multi-polygon. This last column, $agg_n is automatically calculated by the terra::aggregate() function. smoke_t_density Create a new plot with smoke_t_density. ggplot() + geom_spatvector( data = smoke_t_density, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot; ) + ggtitle(&quot;Wildfire Smoke Plumes (aggregated)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() Crop The terra::crop() function can be used to reduce SpatVector to an area determined by another SpatVector. In this example, we can crop the smoke_t_density polygons to the coterminous United States’ state boundaries. smoke_t_crop &lt;- terra::crop( smoke_t_density, conus_t ) Plot the cropped wildfire smoke plume polygons and the coterminous United States’ state boundaries. ggplot() + geom_spatvector( data = smoke_t_crop, aes(fill = Density) ) + scale_fill_manual(&quot;Smoke Density&quot;, values = c(&quot;lightgreen&quot;, &quot;lightgoldenrod&quot;, &quot;tomato&quot;) ) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot; ) + ggtitle(&quot;Wildfire Smoke Plumes (cropped)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.line = element_blank(), axis.ticks = element_blank(), axis.text = element_blank() ) Zonal statistics Looking closely at the previous plot, it is clear that wildfire smoke plumes cover each state differently. The terra package can be used to identify which states are covered by each classification of wildfire smoke plumes. The terra::relate() function can be used to identify spatial relationships between two SpatVector objects. The relation = \"intersects\" argument logically identifies if any portion of each state is or is not covered by each of the three wildfire smoke plume classification multi-polygons. The output of terra::relate() is a wide matrix. The nested data.frame() and t() wrappers convert the output from a wide matrix to a long data frame, which is required to combine the results with the conus_t data set. conus_smoke &lt;- data.frame( t( relate(smoke_t_density, conus_t, relation = &quot;intersects&quot; ) ) ) Set the column names of conus_smoke to match the smoke density classifications. The order of the columns in conus_smoke are based on the ordered factor levels in smoke_t_density$Density (see 2.2.3 Reclassify). colnames(conus_smoke) &lt;- c(&quot;Light&quot;, &quot;Medium&quot;, &quot;Heavy&quot;) Combine the wildfire smoke plume indicator data frame with the the coterminous United States’ state boundaries data. conus_t &lt;- cbind( conus_t, conus_smoke ) The conus_t data set now contains separate columns indicating the presence or absence of “Light”, “Medium”, and “Heavy” wildfire smoke plumes for each coterminous state. names(conus_t) Plot (for Loop) A for loop can be used to create indicator plots for each wildfire smoke plume classification. The layout of a for loop can look complicated, but it simply applies the same set of functions to a given list of inputs. The list of inputs must first be created. As the goal is to plot each of the wildfire smoke plume density classifications, create a character vector of the three classification names. This “list of inputs” must first be created. Store the three wildfire smoke plume classifications in a vector of class character. dens_c &lt;- c(&quot;Light&quot;, &quot;Medium&quot;, &quot;Heavy&quot;) Create a for loop that creates a plot for each wildfire smoke plume density stored within dens_c. Code line 1 tells the for loop to apply the following functions to each observation in dens_c. Code lines 3 through 9 define the plotting colors based on the wildfire smoke plume classification (dens_c[d]). As in previous plots, “Light” smoke plumes will be colored green, “Medium” smoke plumes will be covered yellow, and “Heavy” smoke plumes will be colored red. Code lines 12 through 32 create the plot based on the wildfire smoke plume classification (dens_c[d]), and previously defined plotting colors (color_values). for (d in seq_along(dens_c)) { # define color palette based on smoke density if (dens_c[d] == &quot;Light&quot;) { color_values &lt;- c(&quot;lightgrey&quot;, &quot;lightgreen&quot;) } else if (dens_c[d] == &quot;Medium&quot;) { color_values &lt;- c(&quot;lightgrey&quot;, &quot;lightgoldenrod&quot;) } else if (dens_c[d] == &quot;Heavy&quot;) { color_values &lt;- c(&quot;lightgrey&quot;, &quot;tomato&quot;) } # create plot print( ggplot() + geom_spatvector( data = conus_t, aes_string(fill = dens_c[d]) ) + scale_fill_manual( paste0( dens_c[d], &quot; Smoke Plume Coverage Present&quot; ), values = color_values ) + theme_pubr(legend = &quot;bottom&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.line = element_blank(), axis.ticks = element_blank(), axis.text = element_blank() ) ) } 1.4 Raster Data with terra Air temperature data from the United States National Oceanic and Atmospheric Administration’s (NOAA) North American Regional Reanalysis (NARR) data set will be used to demonstrate using raster data with the terra package. 1.4.1 Access and download The website URL where the NOAA NARR exists is year-specific, meaning there is a unique URL for each annual data set. For the purpose of these exploratory analyses, air temperature data from the year 2021 will be used Define the variable year according to the year of interest. year &lt;- &quot;2021&quot; The utils::download.file() function downloads the file according to the defined URL and destination file. # specify the URL where data is stored based on year variable of interest url_narr &lt;- paste0( &quot;https://downloads.psl.noaa.gov//Datasets/NARR/Dailies/monolevel/air.2m.&quot;, year, &quot;.nc&quot; ) # specify where to save downloaded data destination_narr &lt;- paste0( &quot;/ YOUR FILE PATH /narr_air2m_&quot;, year, &quot;.nc&quot; ) # download the data download.file( url_narr, destination_narr ) Identify the desired data file with utils::list.files() list.files(&quot;/ YOUR FILE PATH /&quot;) The file downloaded from NOAA’s NARR data set is an .nc, or netCDF, file. NetCDF files are common for raster data, and do not need to be unzipped. 1.4.2 Import Now that the data file of interest has been downloaded and identified, import the data with terra::rast(). narr &lt;- rast(paste0( &quot;/ YOUR FILE PATH /narr_air2m_&quot;, year, &quot;.nc&quot; )) 1.4.3 Inspect structure Inspect the structure of narr to see its class, dimensions, variables, and layer names. narr When working with raster data, the dimensions of the raster refer to the number of rows (nrow) and columns (ncol) of grid cells that make up the raster. Similarly, the number of layers in the raster object (nlyr) represents the number of observations of the data. These can also be inspected individually with nrow(), ncol(), and nlyr(), respectively. nrow(narr) ncol(narr) nlyr(narr) 1.4.4 Rename The narr data set contains 365 layers, one for each daily observation of air temperature in 2021. When working with raster data that contains multiple layers, it is important to know and recognize the naming structure of each layer. In this case, the layer names are air_ followed by the day of the year (ie. January 1 = air_1). names(narr)[1:5] Renaming raster layers can be useful for calculating summary statistics or when combining rasters with potentially identical layer names. Using the time() and gsub() functions, the layers can be renamed according to their date. names(narr) &lt;- paste0( &quot;air_&quot;, gsub( &quot;-&quot;, &quot;&quot;, as.character(time(narr)) ) ) Check the layer names again to ensure proper renaming. names(narr)[1:5] 1.4.5 Coordinate reference system and projection Check the coordinate reference system of a SpatRaster object with terra::crs(). crs(narr, describe = TRUE ) narr has a native coordinate reference system, but it is unnamed and was not identifiable by terra::rast(). The area of interest for these exploratory analyses is the coterminous United States, so we can project narr to the Albers Equal Area projected coordinate system (EPSG code: 5070). narr &lt;- project( narr, &quot;EPSG:5070&quot; ) We want to create plots with and analyze the air temperature data as it relates to the coterminous United States’ state boundaries. Subset states_t to remove Alaska, Hawaii, and the United States’ territories. See sections 2.0 Access, download, and unzip and 2.2.1 Import for obtaining the states_t data set. remove &lt;- c( &quot;Alaska&quot;, &quot;Hawaii&quot;, &quot;Puerto Rico&quot;, &quot;United States Virgin Islands&quot;, &quot;Commonwealth of the Northern Mariana Islands&quot;, &quot;Guam&quot;, &quot;American Samoa&quot; ) conus_t &lt;- subset( states_t, !states_t$NAME %in% remove ) Project conus_t to the Albers Equal Area projected coordinate system (EPSG code: 5070). conus_t &lt;- project( conus_t, &quot;EPSG:5070&quot; ) Ensure that both data sets have the same coordinate reference system. crs(conus_t) == crs(narr) 1.4.6 Plot (multiple) Now that the data sets and coordinate reference systems have been prepared, create a plot with ggplot2::gglot(). Identifying the data sets to be plotted within the geom_spatraster() and geom_spatvector() argument informs the function that the narr and conus_t data sets are SpatRaster and SpatVector objects, respectively. Only the first layer of the narr data set will be plotted. ggplot() + geom_spatraster(data = narr$air_20210101) + scale_fill_continuous( type = &quot;viridis&quot;, na.value = NA, &quot;Temperature (°K)&quot; ) + ggtitle(&quot;Air Temperature&quot;) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot;, color = &quot;black&quot; ) + theme_pubr(legend = &quot;bottom&quot;) + theme(plot.title = element_text(hjust = 0.5)) + grids() 1.4.7 Crop The terra::crop() function can be used to reduce a SpatRaster to an area determined by SpatVector polygons. The mask = TRUE argument crops to the border of the polygons, whereas mask = FALSE crops to the bounding box surrounding the polygons. In this example, crop the narr data to the coterminous United States’ state boundaries. narr_crop &lt;- crop(narr, conus_t, mask = TRUE ) Plot the cropped temperature data and the coterminous United States’ state boundaries. Only the first layer of the narr data set will be plotted. ggplot() + geom_spatraster(data = narr_crop$air_20210101) + scale_fill_continuous( type = &quot;viridis&quot;, na.value = NA, &quot;Temperature (°K)&quot; ) + ggtitle(&quot;Air Temperature (cropped)&quot;) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot;, color = &quot;black&quot; ) + theme_pubr(legend = &quot;bottom&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.line = element_blank(), axis.ticks = element_blank(), axis.text = element_blank() ) 1.4.8 Units The previous plot depicts the 2m air temperature data in Kelvin. To convert the units to degrees Celsius, simply subtract the values in narr_crop by 273.15. This subtraction will be applied to every grid cell within every layer of narr_crop. narr_crop_c &lt;- narr_crop - 273.15 1.4.9 Summary statistics Similar to mathematical operations, calculating summary statistics is very straight forward with raster data. For this example, calculate the annual mean 2m air temperature and range of 2m air temperatures at each grid cell, and save these as a new layer in narr_crop_c narr_crop_c$mean &lt;- mean(narr_crop_c) narr_crop_c$range &lt;- max(narr_crop_c) - min(narr_crop_c) Inspect the results of the mean and range calculations. summary(narr_crop_c$mean) summary(narr_crop_c$range) With the summary statistic layers prepared, create a plot with ggplot2::ggplot(). Identifying the data sets to be plotted within the geom_spatraster() and geom_spatvector() argument informs the function that the narr_crop_c and conus_t data sets are SpatRaster and SpatVector objects, respectively. Additionally, the facet_wrap(~lyr) argument creates a plot for each layer specified in geom_spatraster(data = narr_crop_c[[c(\"mean\", \"range\")]]). ggplot() + geom_spatraster(data = narr_crop_c[[c(&quot;mean&quot;, &quot;range&quot;)]]) + scale_fill_continuous( type = &quot;viridis&quot;, na.value = NA, &quot;Temperature (°C)&quot; ) + facet_wrap(~lyr) + ggtitle(&quot;Air Temperature&quot;) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot;, color = &quot;black&quot; ) + theme_pubr(legend = &quot;bottom&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.line = element_blank(), axis.ticks = element_blank(), axis.text = element_blank() ) 1.4.10 Zonal statistics Looking closely at the previous plot, it is clear that the annual mean and range of temperatures differ between states. The terra package can be used to calculate zonal statistics of a SpatRaster object based on SpatVector polygons. The terra::zonal() function can be used to calculate the average annual temperature in each state based on the annual grid cell temperatures stored in narr_crop_c$mean. conus_t$MEAN &lt;- zonal(narr_crop_c$mean, conus_t, fun = &quot;mean&quot; ) Plot the state-specific annual mean temperatures with ggplot2::ggplot(). Identifying the data set to be plotted within geom_spatvector() argument informs the function that the data is a SpatVector object. ggplot() + geom_spatvector( data = conus_t, aes(fill = MEAN) ) + scale_fill_continuous( type = &quot;viridis&quot;, na.value = NA, &quot;Temperature (°C)&quot; ) + ggtitle(&quot;Air Temperature (state mean)&quot;) + theme_pubr(legend = &quot;bottom&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.line = element_blank(), axis.ticks = element_blank(), axis.text = element_blank() ) 1.4.11 Reclassify Raster data is most often continuous numeric data. Sometimes, however, it is important to classify the continuous numeric raster data into discrete classes. For this example, we will reclassify the annual mean temperature data into three discrete classes: &lt;10°C, 10°C - 20°C, &gt;20°C. The first step in the reclassification process is to create a matrix storing the “from”, “to”, and “becomes” values. As the names imply, the “from” and “to” values identify the discrete ranges to be reclassified, and “becomes” is the new value that data within this range will take (ie. “from” 0 “to” 5 “becomes” 1 means that values ranging from 0 to 5 will be reclassified as 1). Create the reclassification matrix. from &lt;- c( -Inf, 10, 20 ) to &lt;- c( 10, 20, Inf ) becomes &lt;- 1:3 reclass &lt;- matrix( c( from, to, becomes ), ncol = 3 ) Now that the reclassification matrix has been prepared, reclassify the annual mean temperatures. The right = TRUE argument indicates that intervals are open on the left and closed on the right (ie. (0,10] becomes 1). narr_reclass &lt;- classify(narr_crop_c$mean, rcl = reclass, right = TRUE ) Although narr_reclass contains the reclassified mean air temperature data, the data is still continuously numeric. The following chunk of code converts the narr_reclass$mean layer from numeric to character based on the defined levels. narr_reclass contains the reclassified mean air temperature data, but the data is still numeric. The terra::set.cats() function assigns categories to numeric data based on values stored in a data frame. level_values &lt;- data.frame( c(1:3), c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;) ) colnames(level_values) &lt;- c(&quot;mean_continuous&quot;, &quot;mean_discrete&quot;) set.cats(narr_reclass, layer = &quot;mean&quot;, value = level_values ) Plot the discretely reclassified mean temperature data. ggplot() + geom_spatraster(data = narr_reclass$mean_discrete) + scale_fill_viridis_d(&quot;&quot;, labels = c( &quot;≤10°&quot;, &quot;10°&lt; &amp; ≤20°&quot;, &quot;20°&lt;&quot;, &quot;&quot; ), na.value = NA ) + ggtitle(&quot;Air Temperature (reclassified)&quot;) + geom_spatvector( data = conus_t, fill = &quot;transparent&quot;, color = &quot;black&quot; ) + theme_pubr(legend = &quot;bottom&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.line = element_blank(), axis.ticks = element_blank(), axis.text = element_blank() ) 1.5 Additional Resources For additional resources pertaining to the packages used in this vignette, please see the following: Package ‘sf’ Package ‘terra’ Description of the methods in the terra package Package ‘dplyr’ Package ‘ggplot2’ Package ‘ggpubr’ 1.6 References H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. Hernangomez D (2023). tidyterra: tidyverse Methods and ggplot2 Helpers for terra Objects. https://doi.org/10.5281/zenodo.6572471, https://dieghernan.github.io/tidyterra/ Hijmans R (2023). terra: Spatial Data Analysis. R package version 1.7-39, https://CRAN.R-project.org/package=terra. Kassambara A (2023). ggpubr: ‘ggplot2’ Based Publication Ready Plots. R package version 0.6.0, https://CRAN.R-project.org/package=ggpubr. Pebesma, E., &amp; Bivand, R. (2023). Spatial Data Science: With Applications in R. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016 Pebesma, E., 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10 (1), 439-446, https://doi.org/10.32614/RJ-2018-009 R Core Team (2023). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/. Wickham H, François R, Henry L, Müller K, Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.2, https://CRAN.R-project.org/package=dplyr. "],["wildfire-data.html", "Wildfire Data", " Wildfire Data This unit provides guidance and code for working with different types of wildfire-related data in climate change and health research. "],["wildfire-chapter.html", "2 Wildfire Chapter", " 2 Wildfire Chapter Note: This is a blank test file. "],["other-environmental-data.html", "Other Environmental Data", " Other Environmental Data This unit provides guidance and code for working with various sources of environmental data common in environmental health research. "],["chapter-nasa-earthdata.html", "3 NASA EarthData Download 3.1 Introduction 3.2 NASA EarthData Account 3.3 References", " 3 NASA EarthData Download Date Modified: February 7, 2024 Author: Mitchell Manware Key Terms: Geospatial Data, Remote Sensing Programming Languages: R, Bash 3.1 Introduction 3.1.1 Motivation NASA’s Earth Observing System Data and Information System (EOSDIS) and its twelve Distributed Active Archive Centers (DAAC) are home to a wide range of open access Earth science data. Alaska Satellite Facility (ASF) DAAC Atmospheric Science Data Center (ASDC) Crustal Dynamics Data Information System (CDDIS) Global Hydrometeorology Resource Center (GHRC) Goddard Earth Sciences Data and Information Services Center (GES DISC) Land Processes DAAC (LP DAAC) Level 1 and Atmosphere Archive and Distribution System (LAADS) DAAC National Snow and Ice Data Center (NSIDC) DAAC Oak Ridge National Laboratory (ORNL) DAAC Ocean Biology DAAC (OB.DAAC) Physical Oceanography DAAC (PO.DAAC) Socioeconomic Data and Applications Center (SEDAC) See https://www.earthdata.nasa.gov/eosdis/daacs for more information. Many of the NASA EOSDIS data sets are relevant to environmental health research, and accessing the data from within R is important for reproducability and repeatable analyses. Although the data is openly accessible, users are required to register for an EarthData account and/or be logged in if you have an account. 3.1.2 Objectives Users will: Register for or log into a NASA EarthData Account Generate NASA EarthData Account prerequisite files Practice downloading data from a URL 3.2 NASA EarthData Account 3.2.1 Register or log in Visit https://urs.earthdata.nasa.gov/ to register for or log into a NASA EarthData account. NASA EarthData Account Landing Page 3.2.2 Approved applications After creating an account, navigate to “My Profile”(https://urs.earthdata.nasa.gov/profile), and then to “Applications &gt; Authorized Apps”. This “Authorized Apps” page specifies which NASA EarthData applications can use your login credentials. Authorize the applications from which you will be downloading data. NASA EarthData Approved Applications 3.2.3 Prerequisite files Downloading password-protected data from a URL requires user credentials. Without prerequisite files containing user credentials, the data will not be downloaded correctly. Without the prerequisite files the download step run without error, but trying to open the zip file will return an error. To demonstrate, try to download population density data from NASA’s Socioeconomic Data and Applications Center (SEDAC) archive center. Metric Population Density Year 2020 Resolution ~5km Format GeoTiff URL https://sedac.ciesin.columbia.edu/data/set/gpw-v4-population-density-rev11 Define the data URL and destination file. url &lt;- paste0( &quot;https://sedac.ciesin.columbia.edu/downloads/data/gpw-v4/gpw-v4-population-&quot;, &quot;density-rev11/gpw-v4-population-density-rev11_2020_2pt5_min_tif.zip&quot; ) destfile &lt;- paste0( &quot;/ YOUR FILE PATH /sedac_population_2020_5km.zip&quot; ) Run the download command using system() and unzip the file with unzip(). system( command = paste0( &quot;curl -n -c -b -LJ -o &quot;, destfile, &quot; --url &quot;, url ) ) unzip(destfile) As expected, the data was not downloaded. To download the password protected data with command line commands, we must generate the .netrc, .urs_cookies, and .dodsrc prerequisite files. The following steps return errors for Windows system users. File generation on Windows is currently in development. .netrc The following commands create the .netrc file, which contains your NASA EarthData Account credentials. First, set your working directory to the home directory. Setting a working directory differs between Mac/Linux and Windows machines. if (.Platform$OS.type == &quot;unix&quot;) { setwd(&quot;~/&quot;) } else if (.Platform$OS.type == &quot;windows&quot;) { setwd(&quot;C:/&quot;) } Create a file named .netrc with file.create(). file.create(&quot;.netrc&quot;) Open a connection to .netrc with sink(). Write the line machine urs... replacing YOUR_USERNAME and YOUR_PASSWORD with your NASA EarthData username and password, respectively. After writing the line, close the connection with sink() again. sink(&quot;.netrc&quot;) writeLines( &quot;machine urs.earthdata.nasa.gov login YOUR_USERNAME password YOUR_PASSWORD&quot; ) sink() Edit the settings so only you, the owner of the file, can read and write .netrc. system(&quot;chmod 0600 .netrc&quot;) After, check to ensure the file was created properly. file.exists(&quot;.netrc&quot;) TRUE readLines(&quot;.netrc&quot;) paste0( &quot;machine urs.earthdata.nasa.gov login YOUR_USERNAME password YOUR_PASSWORD&quot; ) .urs_cookies The following commands create the .urs_cookies file. First, set your working directory to the home directory. if (.Platform$OS.type == &quot;unix&quot;) { setwd(&quot;~/&quot;) } else if (.Platform$OS.type == &quot;windows&quot;) { setwd(&quot;C:/&quot;) } Create a file named .netrc with file.create(). file.create(&quot;.urs_cookies&quot;) After, check to ensure the file was created properly. file.exists(&quot;.urs_cookies&quot;) TRUE .dodsrc The following commands create the .urs_cookies file. First, set your working directory to the home directory. if (.Platform$OS.type == &quot;unix&quot;) { setwd(&quot;~/&quot;) } else if (.Platform$OS.type == &quot;windows&quot;) { setwd(&quot;C:/&quot;) } Create a file named “.dodsrc” with file.create() file.create(&quot;.dodsrc&quot;) Open a connection to .dodsrc with sink(). Write the lines beginning with HTTP., replacing YOUR_USERNAME and YOUR_PASSWORD with your NASA EarthData username and password, respectively. After writing the line, close the connection with sink() again. sink(&quot;.dodsrc&quot;) writeLines( paste0( &quot;HTTP.NETRC=YOUR_HOME_DIRECTORY/.netrc\\n&quot;, &quot;HTTP.COOKIE.JAR=YOUR_HOME_DIRECTORY/.urs_cookies&quot; ) ) sink() After, check to ensure the file was created properly. file.exists(&quot;.dodsrc&quot;) TRUE readLines(&quot;.dodsrc&quot;) paste0( c( &quot;HTTP.NETRC=YOUR_HOME_DIRECTORY/.netrc&quot;, &quot;HTTP.COOKIE.JAR=YOUR_HOME_DIRECTORY/.urs_cookies&quot; ) ) If working on a Windows machine, copy the .dodsrc file to the project working directory. Replace YOUR_WORKING_DIRECTORY with the absolute path to the project working directory. if (.Platform$OS.type == &quot;windows&quot;) { file.copy( &quot;C:/.dodsrc&quot;, &quot;YOUR_WORKING_DIRECTORY/.dodsrc&quot; ) } Enter these commands, as well as your username, password, and home directory, without error. Even a single misplaced character can disrupt the verification of your EarthData credentials. With the prerequisite files generated, try to download the SEDAC population data again. Be sure to authorize the “SEDAC” applications at “My Profile”(https://urs.earthdata.nasa.gov/profile) under “Applications &gt; Authorized Apps” before running the following command. system( command = paste0( &quot;curl -n -c ~/.urs_cookies -b .urs_cookies -LJ -o &quot;, destfile, &quot; --url &quot;, url ) ) unzip(destfile) The data is downloaded successfully after the prerequisite files have been generated and the “SEDAC” applications have been authorized. 3.3 References Center for International Earth Science Information Network - CIESIN - Columbia University. 2018. Gridded Population of the World, Version 4 (GPWv4): Population Density, Revision 11. Palisades, New York: NASA Socioeconomic Data and Applications Center (SEDAC). https://doi.org/10.7927/H49C6VHW. Accessed 7 February 2024. National Aeronautics and Space Administration (NASA). EOSDIS Distributed Active Archive Centers (DAAC). Accessed 3 January 2024. https://www.earthdata.nasa.gov/eosdis/daacs. National Aeronautics and Space Administration (NASA). How to Generate Earthdata Prerequisite Files. Accessed 3 January 2024. https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20Generate%20Earthdata%20Prerequisite%20Files. "],["health-data-integration.html", "Health Data Integration", " Health Data Integration This unit provides guidance and code for integrating environmental and health data in climate change and health research, both at the individual level and at the population level. "],["chapter-link-to-census.html", "4 Linkage to Census Units 4.1 Introduction 4.2 Tutorial 4.3 Concluding Remarks 4.4 Resources", " 4 Linkage to Census Units Linking Geocoded Addresses to Census Units and Social Determinants of Health Data in R Date Modified: September 13, 2023 Author: Lara P. Clark Key Terms: Data Integration, Social Determinants of Health, Geocoded Address, Geographic Unit, Geoid Programming Language: R 4.1 Introduction This tutorial provides example code in R: To link geocoded addresses (i.e., geographic coordinates) to the specific US Census geographic units (e.g., tract) in which they are located. To link data from the Agency for Healthcare Research and Quality (AHRQ) Social Determinants of Health Database to those geocoded addresses. 4.1.1 Motivation Linking geocoded addresses to US Census geographic units is a common step in environmental health data integration workflows. First, using geographic information systems (GIS) software, the geocoded addresses are mapped to the specific Census geographic units (e.g., Census tracts) in which they are located. Second, the Census geographic unit identifying code (or, geoid) is matched to each geocoded address. The result is a table of geocoded addresses and their corresponding Census geoids. The Census geoids can then serve as the basis for linking additional data to each geocoded address. Many types of data with importance for environmental health applications are available by Census geoid. Specifically, the Census collects and provides data by Census geoid for various social determinants of health (SDOH). Such Census SDOH data describe poverty, race/ethnicity, language, housing, and other socioeconomic characteristics. Increasingly, other data providers (e.g., other government agencies, research institutions, community science groups) are making their data available by Census geoid to help facilitate linkages with SDOH data. Such data cover various environment, climate, health, and built environment characteristics. Example environmental health data sources readily available by Census geoid from US federal agencies include the following: Data source Geographic units Example topics AHRQ Social Detrminants of Health Database County, ZIP code, tract Demographic characteristics, housing and transportation characteristics, food access, healthcare characteristics EPA Environmental Justice Screening and Mapping Tool Block group Air pollution, hazardous waste, flood risk, wildfire risk, environmental justice indices CDC National Environmental Public Health Tracking Network State, county Heat, sunlight and ultraviolet exposure, built environment characteristics, asthma, heat-related illnesses 4.1.2 Background The Census defines geographic boundaries at various spatial scales. Common examples in environmental health workflows include (from coarsest to finest spatial scale): states, counties, tracts, block groups, and blocks. These boundaries are completely non-overlapping, such that block boundaries are nested within block groups, which are nested within tracts, which are nested within counties, which are nested within states. The Census defines the boundaries of tracts, block groups, and blocks to scale with local population: that is, in areas with higher population density (e.g., urban cores), these geographic units have finer spatial scale (i.e., smaller land area per unit) and in areas with lower population density (e.g., rural areas), the geographic units have coarser spatial scale (i.e., larger land area per unit). As a result, the spatial scale of geographic units varies substantially across the US. The shape of geographic units also varies substantially: the Census defines these boundaries to follow political boundaries (e.g., state boundaries) as well as physical features (e.g., roads, rivers), which often have irregular shapes. The variable shape and spatial scale of Census geographic units is in contrast with spatial grids– in which each geographic unit (i.e., grid cell) has the same shape and spatial resolution. Such spatial grids are common for environment and climate data. ZIP Code Tabulation Areas (ZCTAs), which represent the geographic areas used by the US Postal Service for ZIP codes, are another common boundary used in environmental health workflows. ZCTAs have no spatial relationship with block groups, tracts, counties, or states: that is, ZCTAs can cross or overlap those other geographic boundaries. Like other Census geographic boundaries, ZCTAs vary in spatial resolution and shape across the US. The spatial scale of ZCTAs is, on average, finer than counties but coarser than tracts. The following figure illustrates the variable spatial scale and shape of each type of geographic boundary: Census geographic units in Durham County, North Carolina, in 2010 The Census assigns a unique identifying code, or geoid, to each geographic unit. This Census geoid is also referred to as a FIPS (Federal Information Processing Series) code. The following table describes common types of Census geographic units and the structure of their geoids: Geographic unit Total units1 Geoid desciption and structure Example unit geoid Example unit name State 50 2-digit state (S) code = SS 09 Connecticut (CT) County 3143 5-digit code = 2-digit state (S) code + 3-digit county (C) code = SSCCC 09007 Middlesex County, CT Zip Code Tabulation Area (ZCTA) 33,642 5-digit ZCTA (Z) code = ZZZZZ 06480 ZCTA for Portland, CT Tract 84,414 11-digit code = 2-digit state (S) code + 3 digit county (C) code + 6-digit tract (T) code = SSCCCTTTTTT 09007560100 Tract 560100 in Middlesex County, CT Block group 239,780 12-digit code = 2-digit state (S) code + 3 digit county (C) code + 6-digit tract (T) code + 1-digit block group (G) code = SSCCCTTTTTTG 090075601001 Block group 1 in Tract 560100 in Middlesex County, CT Block 8,132,968 15-digit code = 2-digit state (S) code + 3 digit county (C) code + 6-digit tract (T) code + 4-digit block (B) code (which contains the block group code as its first digit) = SSCCCTTTTTTGBBB 090075601001004 Block 1004 in Tract 560100 in Middlesex County, CT For states, counties, tracts, block groups, and blocks: the geoids for the finer spatial scale units are constructed from the geoids for the coarser spatial scale units in which they are located. This makes it possible to extract, for example, the tract geoid from the block geoid based on digit number. 4.1.3 Considerations The following are important considerations for linking geocoded addresses to Census geographic boundaries for environmental health applications. Temporal considerations: The Census periodically updates its geographic boundaries, such as tracts, block groups, and blocks. For example, the Census may add, remove, or modify specific geographic units (and associated geoids) as the population changes over time. Thus, it is important to select the correct years (or, vintages) of Census geographic boundaries needed for linking each data source of interest. For example, linking SDOH data provided for year-2010 vintage Census tract boundaries would require the year-2010 vintage of Census tract geoids for each geocoded address. Spatial scale considerations: The spatial scale of tracts, block groups, and blocks varies substantially across the US, with finer spatial scale in urban areas and coarser scale in rural areas. Thus, it is important to select the Census geographic boundary with appropriate spatial resolution for the specific environmental health application. Privacy considerations: Various web-based geocoding tools support look-up of Census geoids by street addresses or geographic coordinates. However, such web-based tools require sending geolocation information over the internet, which can risk exposing private geolocation information. To meet data protection guidelines for environmental health applications, this tutorial instead uses a fully offline approach for handling geolocation information. 4.1.4 Outline This tutorial includes the following steps: Install R packages Prepare geocoded addresses Access Census geographic boundaries Link geocoded addresses to Census geographic boundaries Link AHRQ SDOH data to geocoded addresses 4.2 Tutorial 4.2.1 Install R packages The following code installs and loads the packages used in this tutorial: # install required packages install.packages(c(&quot;sf&quot;, &quot;tidyverse&quot;, &quot;tigris&quot;, &quot;tmap&quot;)) # load required packages library(sf) library(tidyverse) library(tigris) library(tmap) 4.2.2 Prepare geocoded addresses The first step is to prepare the geocoded addresses (i.e., geographic coordinates or latitude/longitude) for mapping in R. For this tutorial, we will use sample public data to represent geocoded addresses of, for example, participants in a health cohort study. This sample public data will be the coordinates for the city halls of the five largest cities in North Carolina. The following code reads these coordinates into a table in R with columns for id, latitude, and longitude: # create a table of sample public geocoded addresses geo_addresses_tbl &lt;- tibble( id = c( &quot;01-charlotte&quot;, &quot;02-raleigh&quot;, &quot;03-greensboro&quot;, &quot;04-durham&quot;, &quot;05-winston-salem&quot; ), latitude = c( 35.21599030178876, 35.78019493350421, 36.07391214865624, 35.99607916214782, 36.09512361249636 ), longitude = c( -80.80170873958926, -78.64278743612566, -79.7883534467845, -78.89907326845271, -80.24283630316438 ) ) # view the table print(geo_addresses_tbl) Here is the table produced by the code above: # A tibble: 5 × 3 # id latitude longitude # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 01-charlotte 35.2 -80.8 # 2 02-raleigh 35.8 -78.6 # 3 03-greensboro 36.1 -79.8 # 4 04-durham 36.0 -78.9 # 5 05-winston-salem 36.1 -80.2 Next, we’ll transform this table to an explicitly spatial data type: simple features (sf). This will allow us to use the point locations for spatial analysis using the sf package in R. To do this, we will need to specify the coordinate reference system (CRS) used for the city hall coordinates. For this example, we retrieved the city hall coordinates from Google Maps, which uses the World Geodetic System 1984 (WGS84) CRS. If you need to find the coordinate reference system information for your geolocation data, here are a few places to look: Metadata for geolocation data Documentation for geocoding method (e.g., geocoding software) or geolocation data collection method (e.g., GPS device) General documentation for data provider There are multiple formats available in R for specifying the CRS : proj4, well known text (wkt), and European Petroleum Survey Group (EPSG) format. Here, we will specify the WGS84 CRS using the EPSG format, which is a 4-digit numeric identifying code. We can look up the EPSG code for WGS84 by searching a spatial reference catalog. There, we find that the EPSG code is4326. The following code transforms the table of city hall locations to a simple features object: # transform table to simple features geo_addresses_sf &lt;- sf::st_as_sf(geo_addresses_tbl, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = &quot;EPSG:4326&quot; ) # view simple features print(geo_addresses_sf) Here is the description of the simple features object produced by the code above: # Simple feature collection with 5 features and 1 field # Geometry type: POINT # Dimension: XY # Bounding box: xmin: -80.80171 ymin: 35.21599 xmax: -78.64279 ymax: 36.09512 # Geodetic CRS: WGS 84 # # A tibble: 5 × 2 # id geometry # * &lt;chr&gt; &lt;POINT [°]&gt; # 1 01-charlotte (-80.80171 35.21599) # 2 02-raleigh (-78.64279 35.78019) # 3 03-greensboro (-79.78835 36.07391) # 4 04-durham (-78.89907 35.99608) # 5 05-winston-salem (-80.24284 36.09512) Now, we can see that each city hall has associated geometry in point format. We can also confirm that the correct CRS (WGS84) is now associated with the city hall locations. 4.2.3 Access Census geographic boundaries The second step is to prepare the Census geographic boundaries for mapping in R. For this tutorial, we’ll use the tigris package to download the Census tract boundaries for North Carolina in year-2010 (i.e., year-2010 vintage). The following code reads the North Carolina 2010 Census tract boundaries into R as simple features: # download Census tracts in North Carolina in 2010 as simple features nc_tracts_2010_sf &lt;- tigris::tracts(state = &quot;NC&quot;, year = 2010) # view the first several rows of the Census tracts simple features head(nc_tracts_2010_sf) Here is the description of the simple features object produced by the code above: # Simple feature collection with 6 features and 14 fields # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -80.07571 ymin: 34.80499 xmax: -79.45918 ymax: 35.18342 # Geodetic CRS: NAD83 # STATEFP10 COUNTYFP10 TRACTCE10 GEOID10 NAME10 NAMELSAD10 MTFCC10 # 1 37 153 970100 37153970100 9701 Census Tract 9701 G5020 # 2 37 153 970200 37153970200 9702 Census Tract 9702 G5020 # 3 37 153 970800 37153970800 9708 Census Tract 9708 G5020 # 4 37 153 970900 37153970900 9709 Census Tract 9709 G5020 # 5 37 153 971000 37153971000 9710 Census Tract 9710 G5020 # 6 37 153 971100 37153971100 9711 Census Tract 9711 G5020 # # FUNCSTAT10 ALAND10 AWATER10 INTPTLAT10 INTPTLON10 # 1 S 246281647 2106825 +35.0503203 -079.6180454 # 2 S 457736198 7835811 +35.0967892 -079.8225512 # 3 S 139358521 2752112 +34.8508484 -079.8201950 # 4 S 23311020 78240 +34.8785679 -079.7346295 # 5 S 49233222 188190 +34.9395795 -079.6628977 # 6. S 161136716 948938 +34.8751742 -079.6567146 # geometry COUNTYFP STATEFP # 1 MULTIPOLYGON (((-79.56729 3... 153 37 # 2 MULTIPOLYGON (((-79.71753 3... 153 37 # 3 MULTIPOLYGON (((-79.76773 3... 153 37 # 4 MULTIPOLYGON (((-79.76773 3... 153 37 # 5 MULTIPOLYGON (((-79.69038 3... 153 37 # 6 MULTIPOLYGON (((-79.5684 34... 153 37 Each tract has associated geometry in polygon format plus 14 additional attributes (i.e., variables, columns). Importantly, the Census tract geoid (i.e., 11-digit identifying code) for year-2010 is stored in the column GEOID10. We can see that the CRS listed above for the Census tract boundaries (NAD83) is different from the CRS for the city hall locations (WGS84). This will be important for the linkage step. Next, we can view the geometry of the tracts by creating a map: # create a map of the Census tracts nc_tracts_2010_map &lt;- tmap::tm_shape(nc_tracts_2010_sf) + tmap::tm_polygons(lwd = 0.5) # view the map print(nc_tracts_2010_map) Census tracts in North Carolina in 2010 Other types and vintages of Census geographic boundaries are available through tigris and the related tidycensus package. In most cases, these boundaries are available for recent years (i.e., 1990 to present) and are accessed by state (i.e., users can download geographic boundaries for one state at a time, in separate files). Historic Census geographic boundaries (i.e., 1910 to present) are available through IPUMS NHGIS. Additionally, IPUMS NHGIS provides many boundaries at the national scale (i.e., such that users can download geographic boundaries for the entire US in a single file). Here is an example workflow for accessing historic Census geographic boundaries via IPUMS NHGIS in R. 4.2.4 Link geocoded addresses to Census geographic boundaries The third step is to link each geocoded addresses to the geoid of the Census geographic unit that contains it. To do this, we’ll first need to prepare the geocoded addresses and Census geographic boundaries in the same CRS. The following code transforms the CRS of the geocoded addresses to match the the CRS of the Census geographic boundaries (NAD83) and then maps them together: # transform city hall locations to match CRS of Census tracts geo_addresses_crs_sf &lt;- sf::st_transform(geo_addresses_sf, crs = sf::st_crs(nc_tracts_2010_sf) ) # create a map of the Census tracts with the city hall locations linkage_map &lt;- tmap::tm_shape(nc_tracts_2010_sf) + tmap::tm_polygons(lwd = 0.5) + tmap::tm_shape(geo_addresses_crs_sf) + tmap::tm_dots( col = &quot;red&quot;, size = 0.25 ) # view the map print(linkage_map) Census tracts in North Carolina in 2010 (grey) with sample geocoded addresses (red) Now that the geocoded addresses and Census tracts are in the same CRS, we can link each geocoded address to the Census tract that contains it using a spatial join. The following code produces a table of geocoded addresses linked to Census tract geoids: # link geocoded addresses to the Census tracts that contain them geo_addresses_linkage_sf &lt;- sf::st_join(geo_addresses_sf, nc_tracts_2010_sf, left = TRUE) # create linked table with geocoded addresses id paired with Census tract geoid geo_addresses_linkage_tbl &lt;- sf::st_drop_geometry(geo_addresses_linkage_sf) %&gt;% dplyr::rename(geoid_tract_2010 = GEOID10) %&gt;% dplyr::select(id, geoid_tract_2010) # write linked table to CSV file readr::write_csv(geo_addresses_linkage_tbl, &quot;city_hall_census_tract_2010_linkage.csv&quot;) # view the linked table print(geo_addresses_linkage_tbl) # A tibble: 5 × 2 # id geoid_tract_2010 # &lt;chr&gt; &lt;chr&gt; # 1 01-charlotte 37119001100 # 2 02-raleigh 37183050100 # 3 03-greensboro 37081010800 # 4 04-durham 37063002200 # 5 05-winston-salem 37067000100 4.2.5 Link AHRQ SDOH data to geocoded addresses The fourth step is to link the AHRQ SDOH data to each geocoded address based on the Census geoid. To start, we’ll need to prepare the AHRQ SDOH data for Census tracts. This data is available to download as an Excel (XLSX) spreadsheet on the AHRQ SDOH website as shown in this screenshot: Screenshot of AHRQ SDOH Website (September 13, 2023) Before linking, we’ll need to check the vintage of Census tracts used in the AHRQ SDOH data. To do this, we can review the AHRQ SDOH Data Source Documentation (accessed through the link shown in the website screenshot above). There we find the following information: The SDOH Database 2009 file has census tracts based on the 2000 census tract boundaries. The SDOH Database for 2010-2019 SDOH files uses 2010 census tract boundaries, and the 2020 file uses 2020 census tract boundaries. Based on this information, we can use the geoids for year-2010 vintage Census tracts we prepared for the geocoded addresses in the previous step to link the AHRQ SDOH data for years 2010-2019. If we would instead prefer to link the AHRQ SDOH data for year 2009 or 2020, we would first need to repeat the previous step to link the geoids for year-2000 vintage or year-2020 vintage Census tracts, respectively, to the geocoded addresses. For this example, we will link the AHRQ SDOH data for Census tracts in 2010 to our sample geocoded addresses on the basis of the year-2010 Census tract geoid. The SDOH AHRQ data for Census tracts in 2010 is provided as an Excel file. Because Excel files can contain multiple sheets, we’ll need to first download and open the Excel file to understand which sheet(s) to read into R: Screenshot of first sheet in Excel file (September 14, 2023) Screenshot of second sheet in Excel file (September 14, 2023) We find that the Excel file has two sheets: Layout and Data. The Layout sheet contains the data dictionary. The Data sheet contains &gt;300 columns of SDOH data by geoid for the &gt;70,000 Census tracts in the US in 2010. The following code downloads the Excel file and reads its Data sheet into a table in R: # download Excel file using the URL provided in the screenshot above sdoh_tracts_2010_url &lt;- &quot;https://www.ahrq.gov/downloads/sdoh/sdoh_2010_tract_1_0.xlsx&quot; download.file(sdoh_tracts_2010_url, destfile = &quot;sdoh_2010_tract_1_0.xlsx&quot;) # read the &quot;Data&quot; sheet of the Excel file into a table R # note that you may need to provide a complete filepath for the Excel file sdoh_tracts_2010_tbl &lt;- readxl::read_xlsx(&quot;sdoh_2010_tract_1_0.xlsx&quot;, sheet = &quot;Data&quot;) # view the table head(sdoh_tracts_2010) # A tibble: 6 × 355 # YEAR TRACTFIPS COUNTYFIPS STATEFIPS STATE COUNTY REGION # &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; # 1 2010 01001020100 01001 01 Alabama Autauga County South # 2 2010 01001020200 01001 01 Alabama Autauga County South # 3 2010 01001020300 01001 01 Alabama Autauga County South # 4 2010 01001020400 01001 01 Alabama Autauga County South # 5 2010 01001020500 01001 01 Alabama Autauga County South # 6 2010 01001020600 01001 01 Alabama Autauga County South # ℹ 348 more variables: TERRITORY &lt;dbl&gt;, ACS_TOT_POP_WT &lt;dbl&gt;, # ACS_TOT_POP_US_ABOVE1 &lt;dbl&gt;, ACS_TOT_POP_ABOVE5 &lt;dbl&gt;, # ACS_TOT_POP_ABOVE15 &lt;dbl&gt;, ACS_TOT_POP_ABOVE16 &lt;dbl&gt;, … # ℹ Use `colnames()` to see all variable names We can see that the variable TRACTFIPS contains the 11-digit geoid for Census tracts in character format, which matches the geoid format we prepared in the previous step . For linkages based on geoid, it is important to check that the geoid is in the same format across data sources. We recommend using character format (rather than numeric format) for Census geoids. This will preserve padded zeroes in geoids (e.g., state geoid for Alabama is 01 rather than 1) such that each type of geoid has the expected number of digits (e.g., 2-digits for state geoid). For this example, we can choose to link the following sample of SDOH variables identified by exploring the Layout sheet: ACS_PCT_INC50: Percentage of population with income to poverty ratio under 0.50 POS_DIST_ED_TRACT: Distance in miles to the nearest emergency department, calculated using population weighted tract centroids ACS_PCT_HU_NO_VEH: Percentage of housing units with no vehicle available The following code joins those SDOH data variables to the geocoded addresses based on the Census tract geoid: # rename geoid in SDOH table to match the geoid in the geocoded addresses table # select the SDOH variables of interest sdoh_tracts_2010_tbl &lt;- sdoh_tracts_2010_tbl %&gt;% dplyr::rename(geoid_tract_2010 = TRACTFIPS) %&gt;% dplyr::select(geoid_tract_2010, ACS_PCT_INC50, POS_DIST_ED_TRACT, ACS_PCT_HU_NO_VEH) # join the SDOH table to the geocoded addresses based on geoid geo_addresses_sdoh_tbl &lt;- dplyr::left_join(geo_addresses_linkage_tbl, sdoh_tracts_2010_tbl, by = &quot;geoid_tract_2010&quot;) # view the resulting linked table print(geo_addresses_sdoh_tbl) # A tibble: 5 × 5 # id geoid_tract_2010 ACS_PCT_INC50 POS_DIST_ED_TRACT # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 01-charlotte 37119001100 3.68 0.85 # 2 02-raleigh 37183050100 6.77 2.8 # 3 03-greensboro 37081010800 6.16 0.99 # 4 04-durham 37063002200 25.9 2.62 # 5 05-winston-salem 37067000100 9.01 3.35 # ACS_PCT_HU_NO_VEH # &lt;dbl&gt; # 1 7.24 # 2 25.5 # 3 16.4 # 4 7.63 # 5 16.5 Now, we have a linked table of geocoded addresses (by individual id) linked to the SDOH data. 4.3 Concluding Remarks This tutorial demonstrates how to link geocoded addresses for individuals to Census geographic boundaries and then to SDOH data available for those Census geographic boundaries. Additional tabular data available for Census geographic boundaries can then be readily linked to further develop an integrated dataset for individuals. Such integrated datasets can be used to investigate relationships between SDOH and health outcomes for individuals. 4.4 Resources R Packages used in this tutorial: sf: Pebesma, E. (2018). Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal, 10:1, 439-446. https://github.com/r-spatial/sf/; Pebesma, E., &amp; Bivand, R. (2023). Spatial Data Science: With Applications in R. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016 tidyverse: Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, LD., et al. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686 tigris: Walker, K., Rudis, B. (2023). tigris: Load Census TIGER/Line Shapefiles. R Package, Version 2.0.4, https://github.com/walkerke/tigris tmap: Tennekes, M. (2018). tmap: Thematic Maps in R. Journal of Statistical Software. 84:6, 1-39. https://doi.org/10.18637/jss.v084.i06 Data sources used in this tutorial: Agency for Healthcare Research and Quality (AHRQ). (2020). Social Determinants of Health Database. https://www.ahrq.gov/sdoh/data-analytics/sdoh-data.html Resources for accessing and linking Census geographic boundaries: Census Bureau. (2023). Geography Boundaries by Year. https://www.census.gov/programs-surveys/acs/geography-acs/geography-boundaries-by-year.html IPUMS NHGIS, University of Minnesota, https://www.nhgis.org Walker, K., Herman, M. (2023). tidycensus: Load US Census Boundary and Attribute Data as ‘tidyverse’ and ‘sf’-Ready Data Frames. R Package, Version 1.3.2, https://walker-data.com/tidycensus/ Walker, K. (2023). Analyzing US Census Data: Methods, Maps and Models in R. CRC Press. https://walker-data.com/census-r/index.html Total number of geographic units in the US in 2020 (Source: Census Bureau)↩︎ "],["chapter-chords-geo-exposures.html", "5 Linkage to Exposures 5.1 Introduction 5.2 CACES Air Pollution 5.3 Airport Proximity 5.4 Major Road Proximity", " 5 Linkage to Exposures Linking Geocoded Addresses to Various Geospatial Exposure Types Using NIEHS Tools in R Date Modified: August 10, 2023 Authors: Lara P. Clark, Kyle P. Messier, Sue Nolte, Charles Schmitt et al. Key Terms: Data Integration, Exposure, Exposure Assessment, Geocoded Address, Geospatial Data Programming Language: R 5.1 Introduction 5.1.1 Motivation Climate change and health research draws upon diverse types of open geospatial data to assess individuals’ environmental exposures, such as exposure to air pollution, green space, and extreme temperature. Calculating such environmental exposures using open geospatial data can be challenging and can require expertise from multiple disciplines, from geographic information science to exposure science to bioinformatics. Open source software can help reduce barriers and support broader use of open geospatial data for assessing environmental exposures using reproducible methods. 5.1.2 Approach This chapter describes NIEHS open source tools to link environmental exposures to individual health data based on geocoded addresses (i.e., geographic coordinates, latitude and longitude). These tools are intended to be accessible to researchers without training in geographic information science (GISc) or geographic information systems (GIS) software. These tools require basic or beginner level programming in R. The tools consist of code, standardized data, and documentation describing use for environmental health applications. Each tool calculates a selection of environmental exposure metrics based on a different source of open geospatial data with national (or approximately national) coverage for United States. Exposure metrics are calculated based on specified point locations (i.e., geocoded address or other geographic coordinates). For data sources that include temporal information, the exposure metrics are also calculated based on specified times. Output of the tool includes the calculated exposure metrics as well as information about data missingness and an optional log file. These tools are designed to be run completely offline as an approach to protect personal geolocation data. 5.1.3 Tools The following summarizes the available tools and planned tools: Tool Exposure Metrics Spatial Details Temporal Details Status Air pollution (CACES model) Annual average outdoor concentrations of ozone, particulate matter (PM2.5, PM10), sulfur dioxide, carbon monoxide, and nitrogen dioxide air pollution Census tracts in the contiguous US Yearly during 1979-2015 (varies by pollutant) Version 1.0 Airport proximity Distance to nearest, number within buffer distance, summary of distances (mean, mean of log, 25th, 50th and 75th percentiles) within buffer distance Points in the US Yearly during 1981-2020 Version 1.0 Major road proximity Distance to nearest, length in buffer distance Lines in the US Yearly during 2000-2018 Version 1.0 Air pollution (ACAG model) Annual average outdoor concentrations of particulate matter (PM2.5) and its components 0.01 degree grid in North America Yearly during 2000-2018 In Development Superfund sites Distance to nearest, number within buffer distance, summary of distances (mean, mean of log, 25th, 50th and 75th percentiles) within buffer distance Points in the US 2014 In Development 5.1.4 Additional Resources Other available tools to calculate environmental exposures for health applications: DeGAUSS geomarker software: Brokamp, C. (2018). DeGAUSS: Decentralized geomarker assessment for multi-site studies. Journal of Open Source Software, 3(30), 812. R packages used to develop the tools: sf: Pebesma, E. (2018). Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal, 10:1, 439-446. https://github.com/r-spatial/sf/; Pebesma, E., &amp; Bivand, R. (2023). Spatial Data Science: With Applications in R. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016 terra: Hijmans, R. J., Bivand, R., Pebesma, E., Sumner, M. D. (2024). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://CRAN.R-project.org/package=terra tidyverse: Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, LD., et al. (2019). terra: Spatial Data Analysis. R Package, Version 1.7-71, https://doi.org/10.21105/joss.01686 5.2 CACES Air Pollution The following are step-by-step instructions to calculate annual average air pollution exposure metrics using the Center for Air, Climate, &amp; Energy Solutions (CACES) land use regression models (Version 1.0). 5.2.1 Description CACES Air Pollution Model Data The CACES models are based on air pollution observations from United States (US) Environmental Protection Agency monitors and from satellites, as well as land cover and land use information (e.g., locations of roadways). CACES model predictions cover populated locations in the contiguous US (i.e., in the 48 contiguous states plus the District of Columbia) at the spatial resolution of US census tracts, for the following air pollutants and time-periods: Air pollutant Units Exposure metric Available years carbon monoxide (CO) ppm annual average concentration 1990-2015 nitrogen dioxide (NO2) ppb annual average concentration 1979-2015 ozone (O3) ppb May through September average of daily moving 8-hour maximum average concentration 1979-2015 particulate matter (PM2.5) µg m-3 annual average concentration 1999-2015 particulate matter (PM10) µg m-3 annual average concentration 1988-2015 sulfur dioxide (SO2) ppb annual average concentration 1979-2015 The following figure illustrates the spatial coverage (contiguous US) and spatial resolution (census tracts) for a single pollutant and time-period (NO2 in 2015): Illustration of CACES data (a) spatial coverage (contiguous United States) and (b) spatial scale (census tracts). Exposure Metrics This tool calculates long-term average air pollution exposure metrics for a specified list of receptor point locations (e.g., geocoded home addresses) during a specified time-period in years. This tool can be used to calculate average exposure metrics: for a single year or single range of years that is constant across all receptor locations (e.g., 2002, 2010 to 2015) for a single year or range of years that varies across receptor locations (e.g., year of birth, years of residence at geocoded home address) Exposure metrics can be calculated for all CACES pollutants or any subset of them. Output includes information about data missingness (e.g., whether a specified receptor point is located outside the coverage of the CACES data) as well as an optional log file. Recommended Uses This tool is recommended for the following uses: Comparisons of exposures across larger geographic regions in the contiguous US (e.g., across a metropolitan area or state). Note: This tool is based on census tract level air pollution model data and cannot be used to compare exposures for different locations within the same census tract. Comparisons of exposures incorporating multiple pollutants and/or years in a consistent manner Analyses focused on long-term average (e.g., annual average, multi-year average) exposures to air pollution Steps Install R and required packages Download tool Prepare receptor point data Run script in R Review output Cite data and tool 5.2.2 Install R and Required Packages Install R. Optionally, install RStudio. Then, install the following R packages: logr, tidyverse, sf. Follow R package installation instructions, or run the following code in R: install.packages(c(&quot;logr&quot;, &quot;tidyverse&quot;, &quot;sf&quot;)) 5.2.3 Download Tool Download and save the folder containing input data (input_source_caces.rds and input_census_tracts_2010.rds) and script (script_caces_exposures_for_points.R). To directly run the example scripts provided with these instructions in Step 4, do not change the file names within the folder. 5.2.4 Prepare Receptor Point Data Prepare a comma-separated values (CSV) file that contains a table of the receptor point locations (e.g., geocoded addresses, coordinates). Include each receptor as a separate row in the table, and include the following required columns: id: a unique and anonymous identifying code for each receptor. This can be in character (string) or numeric (double) format. id must be unique across all rows in the receptor point location table. It is not possible to use the same id for different time-points or locations within the same receptor point location table. latitude: the latitude of the receptor point location in decimal degrees format (range: -90 to 90) longitude: the longitude of the receptor point location in decimal degrees format (range: -180 to 180) To calculate exposure metrics for time-periods (year or range of years) that vary across the receptors (e.g., years of residence at geocoded addresses), include both of the following optional columns: time_start: the first year of the time-period in “YYYY” format (e.g., 2002 for year 2002) time_end: the last year of the time-period in “YYYY” format (e.g., 2003 for year 2003) To calculate exposure metrics for a single year that varies across the receptors, provide the same year for both time_start and time_end for each receptor. To calculate exposure metrics for a range of years that is constant across the receptors, provide the start year of the range as time_start for all receptors and the end year of the range as time_end for all receptors. To calculate exposure metrics for a single year that is constant across all receptors, specify year for exposure assessment using argument caces_year. If both caces_year and time_start and time_end are provided, the exposure assessment will be based on caces_year (ignoring time_start and time_end). The following table provides an example of the receptor point data format: id latitude longitude time_start time_end 1011A 39.00205369 -77.105578716 2002 2011 1012C 35.88480215 -78.877942573 2014 2015 1013E 39.43560788 -77.434847823 1990 1990 To directly run the example scripts provided with these instructions, save the receptor point data as input_receptor.csv in the folder. 5.2.5 Run Script in R Run the script script_caces_exposures_for_points.R to load the required functions in R. You can then use the function get_caces_for_points() to calculate the mean CACES model pollutant concentrations for the selected time period (in years) for each receptor point location. Description of Function get_caces_for_points() This function takes the receptor point data above and returns a data frame with the receptor id linked to the exposure estimates for selected pollutants and time periods as well as information about data missingness. Optionally, the function also writes a log file in the current R working directory. The function has the following arguments: Required Arguments receptor_filepath: specifies the file path to a CSV file containing the receptor point locations (described in Step 3). Note: The format for file paths in R can vary by operating system. source_caces_filepath: specifies the file path to a RDS file containing a data frame with the CACES air pollution estimates by census tract. This is the file input_source_caces.rds. source_census_tracts_2010_filepath: specifies the file path to a RDS file containing the simple features with the 2010 census tracts for the US. This is the file input_census_tracts_2010.rds. Optional Arguments receptor_crs: a coordinate reference system object (i.e., class is crs object in R) for the receptor point locations. Default is “EPSG:4269” (i.e., NAD83). caces_pollutants: list that specifies the subset of CACES pollutants to include. Default is all pollutants: \"co\", \"no2\", \"o3\", \"pm10\", \"pm25\", \"so2\". caces_year: specifies a single year (in “YYYY” format; e.g., 2003) for exposure assessment across all pollutants and receptors. Default is NULL. caces_year is required to be specified if time_start and time_end are not provided with the receptor point data. If both caces_year and time_start and time_end are provided, exposure assessment will be based on the single year specified by caces_year (i.e., ignoring time_start and time_end). caces_year must be NULL if using time_start and time_end to specify year(s) that vary across receptor points for exposure assessment. caces_year must be during 1999-2015 to return exposure estimates for all six pollutants, or during 1979-2015 for O3, NO2, and SO2, 1988-2015 for PM10, 1990-2015 for CO, or 1999-2015 for PM2.5. add_all_input_to_output: logical argument that specifies whether the output should include all columns included with receptor point locations (described in Step 3). TRUE returns all columns (i.e., including any time information and census tract identifying code) with output. FALSE returns only the anonymous receptor identifying code, exposure estimates, and data missingness flags with output. FALSE may be useful for meeting data de-identification requirements. Default is TRUE. write_log_to_file: logical argument that specifies whether a log should be written to file. TRUE will create a log file in the current working directory. Default is TRUE. print_log_to_console: logical argument that specifies whether a log should be printed to the console. TRUE will print a log to console. Default is TRUE. Example Use Below are two example scripts for using the function above to produce a CSV file with the CACES exposure estimates for each receptor point for ozone and nitrogen dioxide in year 2015 (using default options for all other optional arguments). The first example uses only R but requires editing the file paths. The second example requires RStudio and the here package but does not require editing file paths. Example 1: Base R # Load packages library(tidyverse) library(logr) library(sf) # Load functions source(&quot;/set/file/path/to/script_caces_exposures_for_points.R&quot;) # Get exposures caces_exposures &lt;- get_caces_for_points( receptor_filepath = &quot;/set/file/path/to/input_receptor.csv&quot;, source_caces_filepath = &quot;/set/file/path/to/input_source_caces.rds&quot;, source_census_tracts_2010_filepath = &quot;/set/file/path/to/input_census_tracts_2010.rds&quot;, caces_year = 2015, caces_pollutants = c(&quot;o3&quot;, &quot;no2&quot;) ) # Write exposures to CSV readr::write_csv(caces_exposures, file = &quot;/set/file/path/to/output_caces_exposures.csv&quot; ) Example 2: RStudio with here Package # Install here package (if needed) install.packages(&quot;here&quot;) # Load packages library(here) library(tidyverse) library(logr) library(sf) # Set location here::i_am(&quot;script_caces_exposures_for_points.R&quot;) # Load functions source(here::here(&quot;script_caces_exposures_for_points.R&quot;)) # Get exposures caces_exposures &lt;- get_caces_for_points( receptor_filepath = here(&quot;input_receptor.csv&quot;), source_caces_filepath = here(&quot;input_source_caces.rds&quot;), source_census_tracts_2010_filepath = here(&quot;input_census_tracts_2010.rds&quot;), caces_year = 2015, caces_pollutants = c(&quot;o3&quot;, &quot;no2&quot;) ) # Write exposures to CSV readr::write_csv(caces_exposures, file = here(&quot;output_caces_exposures.csv&quot;) ) 5.2.6 Review Output Log File After running the example script above, with the log file option selected, the log file will be available in the folder log in the current R working directory. Output Data After running the example script above, calculated exposure metrics for receptor locations will be available in the file output_caces_exposure.csv within the folder. This CSV file includes a row for each receptor with the following columns (as applicable): Identifiers id: the unique and anonymous identifying code for each receptor fips_tr_10: the identifying code (FIPS code) for the year 2010 census tract Calculated Exposure Metrics co: mean CACES model predicted concentration of outdoor annual average carbon monoxide (CO) air pollution (units: parts per million [ppm]) for the census tract that contains the receptor point location during the specified year(s) no2: mean CACES model predicted concentration of outdoor annual average nitrogen dioxide (NO2) air pollution (units: parts per billion [ppb]) for the census tract that contains the receptor point location during the specified year(s) o3: mean CACES model predicted concentration of outdoor annual average ozone (O3) air pollution (units: parts per billion [ppb]) for the census tract that contains the receptor point location during the specified year(s) pm25: mean CACES model predicted concentration of outdoor annual average particulate matter (PM2.5) air pollution (units: micrograms per cubic meter [µg m-3]) for the census tract that contains the receptor point location during the specified year(s) pm10: mean CACES model predicted concentration of outdoor annual average particulate matter (PM10) air pollution (units: micrograms per cubic meter [µg m-3]) for the census tract that contains the receptor point location during the specified year(s) so2: mean CACES model predicted concentration of outdoor annual average sulfur dioxide (SO2) air pollution (units: parts per billion [ppb]) for the census tract that contains the receptor point location during the specified year(s) Information on Data Missingness caces_flag_01: binary variable indicating whether the receptor point is located within a year 2010 US census tract: 1 indicates that receptor point is not located within a year 2010 US census tract. All exposure metrics for that receptor point will be reported as NA. 0 indicates that receptor point is located within a year 2010 US census tract caces_flag_02: binary variable indicating whether receptor point is located within a year 2010 US census tract but outside the spatial coverage of the CACES air pollution model: 1 indicates receptor point is located within a year 2010 US census tract but outside the spatial coverage of the CACES air pollution model. Examples include tracts in Alaska, Hawaii, or US territories, and tracts with no population recorded in the 2010 Decennial Census. All exposure metrics for that receptor point will be reported as NA. 0 indicates that receptor point is located within a year 2010 census tract within coverage of the CACES air pollution model caces_flag_03: binary variable indicating whether the specified time period for that receptor point is completely outside the coverage of the CACES air pollution model: 1 indicates that the specified time (year(s)) for that receptor point is completely outside the temporal coverage of CACES air pollution model for one or more of the selected pollutants. Exposure metrics will be reported as NA for one or more of the selected pollutants. 0 indicates that the specified time (year(s)) for that receptor point is not completely outside the temporal coverage of CACES air pollution model for any of the selected pollutants caces_flag_04: binary variable indicating whether the specified time-period (year(s)) for that receptor point is partly outside the temporal coverage of CACES air pollution model for one or more of the selected pollutants. 1 indicates that the the specified time-period (year(s)) for that receptor point is partly outside the temporal coverage of CACES air pollution model for one or more of the selected pollutants. Exposure metrics will be calculated based on the years with available data for that specified time-period. 0 indicates that the specified time-period (year(s)) for that receptor point is completely within the temporal coverage of CACES air pollution model for all of the selected pollutants. 5.2.7 Cite Data and Tool Please cite the following in any publications based on this tool: CACES Empirical Air Pollution Models (v1): Kim S.-Y.; Bechle, M.; Hankey, S.; Sheppard, L.; Szpiro, A. A.; Marshall, J. D. 2020. “Concentrations of criteria pollutants in the contiguous U.S., 1979 – 2015: Role of prediction model parsimony in integrated empirical geographic regression.” PLoS ONE 15(2), e0228535. DOI: 10.1371/journal.pone.0228535 Census Tract Spatial Boundaries: Steven Manson, Jonathan Schroeder, David Van Riper, Tracy Kugler, and Steven Ruggles. IPUMS National Historical Geographic Information System: Version 16.0 [Census Tract Shapefiles, 2010]. Minneapolis, MN: IPUMS. 2021. http://doi.org/10.18128/D050.V16.0 Please see the following for additional requirements: https://www.nhgis.org/citation-and-use-nhgis-data NIEHS Geospatial Toolbox: Citation to be determined. 5.3 Airport Proximity The following are step-by-step instructions to calculate proximity-based exposure metrics to aircraft landing facilities in the United States (US) using Federal Aviation Administration (FAA) aircraft landing facility data. 5.3.1 Description FAA Aircraft Landing Facility Data The FAA aircraft landing facility data includes a registry of point locations (i.e., coordinates) of arrival and departure of aircraft in the US. FAA provides the following categorization of aircraft landing facility types: airports, heliports, seaplane bases, gliderports, ultralights, and balloonports. FAA also provides the activation year for facilities starting in year 1981. The following figure illustrates the spatial coverage (all US states) and spatial scale (points) of the FAA aircraft landing facility data: Illustration of FAA aircraft facility data (a) spatial coverage (United States, including Alaska and Hawaii (not shown)) and (b) spatial scale (points). Exposure Metrics This tool calculates proximity-based (i.e., distance-based) exposure metrics for a specified list of receptor point locations (e.g., geocoded home addresses) to aircraft landing facilities in a specified year (during 1981-2020). This tool can be used to calculate the following proximity-based metrics within the US: Distance to nearest aircraft landing facility and identity of nearest aircraft landing facility (i.e., identifying codes, facility type, and activation year) Count of aircraft landing facilities within a specified buffer distance of receptor Summary metrics of distances to all aircraft landing facilities within a specified buffer distance of receptor (i.e., mean distance, mean of logarithm distance, and 25th, 50th, and 75th percentile distances) Proximity metrics can be calculated for all FAA aircraft landing facility types (i.e., airports, heliports, seaplane bases, gliderports, ultralights, and balloonports) or any subset of them. Output includes information about data missingness (e.g., whether a receptor location is near a US border) as well as an optional log file. Recommended Uses This tool is recommended for the following uses: Applications for which a proximity-based metric is appropriate. Note: This tool does not provide other relevant exposure information associated with aircraft landing facilities, such as traffic (e.g., annual count of arrivals/departures), noise levels, or air pollution levels. Applications for which most receptor point locations are not located near to a US border with Mexico or Canada. Note: Because this tool does not include aircraft landing facility data for Mexico or Canada, the tool may under predict proximity to aircraft landing facilities for receptor point locations in the US near a border with Mexico or Canada with nearby aircraft landing facilities across the border. This tool provides optional output information indicating whether a receptor point is located within a specified distance of a border. Steps Install R and required packages Download tool Prepare receptor point data Run script in R Review output Cite data and tool 5.3.2 Install R and Required Packages Install R. Optionally, install RStudio. Then, install the following R packages: logr, tidyverse, sf. Follow R package installation instructions, or run the following code in R: install.packages(c(&quot;logr&quot;, &quot;tidyverse&quot;, &quot;sf&quot;)) 5.3.3 Download Tool Download and save the folder containing input data (input_source_aircraft_facilities.rds and input_us_borders.rds) and script (script_aircraft_facility_proximity_for_points.R). To directly run the example scripts provided with these instructions in Step 4, do not change the file names within the folder. 5.3.4 Prepare Receptor Point Data Prepare a comma-separated values (CSV) file that contains a table of the receptor point locations (e.g., geocoded addresses, coordinates). Include each receptor as a separate row in the table, and include the following required columns: id: a unique and anonymous identifying code for each receptor. This can be in character (string) or numeric (double) format latitude: the latitude of the receptor point location in decimal degrees format (range: -90 to 90) longitude: the longitude of the receptor point location in decimal degrees format (range: -180 to 180) The following table provides an example of the receptor point data format: id latitude longitude 1011A 39.00205369 -77.105578716 1012C 35.88480215 -78.877942573 1013E 39.43560788 -77.434847823 To directly run the example scripts provided with these instructions, save the receptor point data as input_receptor.csv in the folder. 5.3.5 Run Script in R Run the script script_aircraft_facility_proximity_for_points.R to load the required functions in R. You can then use the function get_aircraft_facility_proximity_for_points() to calculate proximity-based exposure metrics for each receptor point location. Description of Function get_aircraft_facility_proximity_for_points() This function takes the receptor point data above and returns a data frame with the receptor identifying code linked to the selected aircraft landing facility proximity metrics for selected aircraft landing facility types and year (during 1981 to 2020) as well as information about data missingness. Optionally, the function also writes a log file in the current R working directory. The function has the following arguments: Required Arguments receptor_filepath: specifies the file path to a CSV file containing the receptor point locations (described in Step 3). Note: The format for file paths in R can vary by operating system. source_aircraft_facilities_filepath: specifies the file path to a RDS file containing a simple features object with the point locations of FAA aircraft landing facilities. This is the file input_source_aircraft_facilities.rds. us_borders_filepath: specifies the file path to a RDS file containing a simple features object with the US borders with Mexico and Canada. This is the file input_us_borders.rds. aircraft_year: specifies a single year (in YYYY format; e.g., 2003) for proximity-based exposure assessment across all pollutants. Default is NULL. Year must be during 1981 to 2020. Aircraft landing facilities activated after aircraft_year will be excluded from calculation of the proximity metrics. Optional Arguments buffer_distance_km: a numeric argument that specifies the buffer distance (units: kilometers [km]) to use in calculation of buffer-based proximity metrics. Default is 10 km. Must be between 0.001 km and 1000 km. Note: Larger buffer distance values may result in longer run-times for buffer-based proximity metrics. receptor_crs: a coordinate reference system object (i.e., class is crs object in R) for the receptor point locations. Default is \"EPSG:4269\" (i.e., NAD83). projection_crs: a projected coordinate reference system object (i.e., class is crs object in R) for use in exposure assessment. Default is \"ESRI:102008\" (i.e., North America Albers Equal Area Conic projection). aircraft_facility_type: list that specifies the subset of FAA aircraft landing facility types to include in the exposure assessment. Default is all types: \"airport\", \"heliport\", \"seaplane base\", \"gliderport\", \"ultralight\", \"balloonport\". proximity_metrics: list that specifies the subset of proximity-based exposure metrics to calculate. Default is all metrics: \"distance_to_nearest, \"count_in_buffer\", \"distance_in_buffer\". \"distance_to_nearest\": returns output with distance to nearest aircraft landing facility (units: km) and identity of nearest aircraft landing facility (i.e., identifying codes, facility type, and activation year) for each receptor \"count_in_buffer\": returns output with count of aircraft landing facilities within a specified buffer distance of each receptor \"distance_in_buffer\": returns output with summary metrics of distances to all aircraft landing facilities within the specified buffer distance of receptor (i.e., mean distance, mean of logarithm distance, and 25th, 50th, and 75th percentile distances to all aircraft landing facilities for each receptor) check_near_us_border: logical argument that specifies whether the function should identify receptor points that are within the buffer distance (i.e., specified by argument buffer_distance_km) of a US border with Canada or Mexico. TRUE returns a column with output (within_border_buffer) with a binary variable indicating receptor points within the buffer distance of a border. Default is TRUE. Note: The aircraft landing facility data covers only facilities located within the US. Thus, this tool may under predict proximity to aircraft landing facilities for receptor locations near a US border with Canada or Mexico. add_all_input_to_output: logical argument that specifies whether the output of the function should include all columns included with the input receptor data frame or not. TRUE returns all columns (i.e., including latitude and longitude) with output. FALSE returns only the anonymous receptor identifying code, proximity-based metrics, and data missingness information with output. FALSE may be useful for meeting data de-identification requirements. Default is TRUE. write_log_to_file: logical argument that specifies whether a log should be written to file. TRUE will create a log file in the current working directory. Default is TRUE. print_log_to_console: logical argument that specifies whether a log should be printed to the console. TRUE will print a log to console. Default is TRUE. Example Use Below are two example scripts for using the function above to produce a CSV file with the proximiity-based exposure estimates for each receptor to airports in year 2020 (using default options for all other optional arguments). The first example uses only R but requires editing the file paths. The second example requires RStudio and the here package but does not require editing file paths. Example 1: Base R # Load packages library(tidyverse) library(logr) library(sf) # Load functions source(&quot;/set/file/path/to/script_aircraft_facility_proximity_for_points.R&quot;) # Get exposures aircraft_proximity_metrics &lt;- get_aircraft_facility_proximity_for_points( receptor_filepath = &quot;/set/file/path/to/input_receptor.csv&quot;, source_aircraft_facilities_filepath = &quot;/set/file/path/to/input_source_aircraft_facilities.rds&quot;, us_borders_filepath = &quot;/set/file/path/to/input_us_borders.rds&quot;, aircraft_year = 2020, aircraft_facility_type = &quot;airport&quot; ) # Write exposures to CSV readr::write_csv(aircraft_proximity_metrics, file = &quot;/set/file/path/to/output_aircraft_proximity_metrics.csv&quot; ) Example 2: RStudio with here Package # Install here package (if needed) install.packages(&quot;here&quot;) # Load packages library(here) library(tidyverse) library(logr) library(sf) # Set location here::i_am(&quot;script_aircraft_facility_proximity_for_points.R&quot;) # Load functions source(here::here(&quot;script_aircraft_facility_proximity_for_points.R&quot;)) # Get exposures aircraft_proximity_metrics &lt;- get_aircraft_facility_proximity_for_points( receptor_filepath = here(&quot;input_receptor.csv&quot;), source_aircraft_facilities_filepath = here(&quot;input_source_aircraft_facilities.rds&quot;), us_borders_filepath = here(&quot;input_us_borders.rds&quot;), aircraft_year = 2020, aircraft_facility_type = &quot;airport&quot; ) # Write exposures to CSV readr::write_csv(aircraft_proximity_metrics, file = here(&quot;output_aircraft_proximity_metrics.csv&quot;) ) 5.3.6 Review Output Log File After running the example script above, with the log file option selected, the log file will be available in the folder log in the current R working directory. Output Data After running the example script above, calculated proximity-based exposure metrics for receptor locations will be available in the file output_aircraft_proximity_metrics.csv within the folder. This CSV file includes a row for each receptor with the following columns (as applicable): Identifiers id: the unique and anonymous identifying code for each receptor Calculated Proximity-Based Exposure Metrics Nearest Distance Metrics aircraft_nearest_distance_km: distance (units: km) to the nearest aircraft landing facility aircraft_nearest_id_site_num: the unique identifying FAA site number for the nearest aircraft landing facility. Consists of a numeric code followed by a letter indicating the aircraft landing facility type. For example, the site number for the Los Angeles International Airport is 01818.*A. The FAA site number can be used to link additional types of FAA data (e.g., annual operations) for further analyses. aircraft_nearest_id_loc: the unique identifying location code for the nearest aircraft landing facility. Consists of a 3 or 4 character alphanumeric code. For example, the location code for the Los Angeles International Airport is LAX. The location code can be used to link additional types of FAA data (e.g., annual operations) for further analyses. aircraft_nearest_fac_type: the type (i.e., airport, heliport, seaplane base, gliderport, ultralight, and balloonport) of the nearest aircraft landing facility aircraft_nearest_year_activation: the year of activation of the nearest aircraft landing facility. Activation year is available for all facilities starting in 1981. Count in Buffer Metrics aircraft_count_in_buffer: number of aircraft landing facilities within the specified buffer distance of each receptor Distance in Buffer Metrics aircraft_mean_distance_in_buffer: mean of distances (units: km) to all aircraft landing facilities within the specified buffer distance of receptor. NA indicates that no landing facilities are within the specified buffer distance of the receptor. Note: In cases with exactly one aircraft landing facility within the specified buffer distance, the value will be the distance to that aircraft landing facility. aircraft_log_mean_distance_in_buffer: mean of logarithm of distances (units: km) to all aircraft landing facilities within the specified buffer distance of receptor. NA indicates that no landing facilities are within the specified buffer distance of the receptor. Note: In cases with exactly one aircraft landing facility within the specified buffer distance, the value will be the logarithm of the distance to that aircraft landing facility. aircraft_p25_distance_in_buffer: 25th percentile of distances (units: km) to all aircraft landing facilities within the specified buffer distance of receptor, for cases with at least 10 aircraft landing facilities within the buffer distance. NA indicates that less than 10 aircraft landing facilities are within the buffer distance. aircraft_p50_distance_in_buffer: 50th percentile (i.e., median) of distances (units: km) to all aircraft landing facilities within the specified buffer distance of receptor, for cases with at least 10 aircraft landing facilities within the buffer distance. NA indicates that less than 10 aircraft landing facilities are within the buffer distance. aircraft_p75_distance_in_buffer: 75th percentile of distances (units: km) to all aircraft landing facilities within the specified buffer distance of receptor, for cases with at least 10 aircraft landing facilities within the buffer distance. NA indicates that less than 10 aircraft landing facilities are within the buffer distance. Information on Data Missingness within_border_buffer: binary variable indicating whether receptor point is located within the buffer distance (i.e., specified by argument buffer_distance_km) of a US border with Canada or Mexico: 1 indicates that receptor point is located within the buffer distance of a US border with Canada or Mexico. This indicates that the proximity-based metrics calculated by this tool may represent under predictions of the true proximity-based metrics (i.e., the nearest aircraft landing facility may be located in Canada or Mexico, outside the coverage of the US aircraft landing facility dataset). 0 indicates that receptor point is not located within the buffer distance of a US border with Canada or Mexico. 5.3.7 Cite Data and Tool Please cite the following in any publications based on this tool: Aircraft Landing Facility Data: US Federal Aviation Administration (FAA). Airport Data and Information Portal (ADIP). [Available: https://adip.faa.gov/agis/public/#/airportSearch/advanced]. Accessed: April 24, 2022. Homeland Infrastructure Foundation-Level Data (HIFLD) Geoplatform. Aircraft landing facilities geospatial data. [Available: https://hifld-geoplatform.opendata.arcgis.com/datasets/geoplatform::aircraft-landing-facilities/about]. Accessed: June 23, 2022. US Borders: Homeland Infrastructure Foundation-Level Data (HIFLD) Geoplatform. Canada and US border geospatial data. [Available: https://hifld-geoplatform.opendata.arcgis.com/datasets/geoplatform::canada-and-us-border/about]. Accessed: June 23, 2022. Homeland Infrastructure Foundation-Level Data (HIFLD) Geoplatform. Mexico and US border geospatial data. [Available: https://hifld-geoplatform.opendata.arcgis.com/datasets/geoplatform::mexico-and-us-border/about]. Accessed: June 23, 2022. NIEHS Geospatial Toolbox: Citation to be determined. 5.4 Major Road Proximity The following are step-by-step instructions to calculate proximity-based exposure metrics to major roadways in the United States (US) using US NASA (National Aeronautics and Space Administration) Socioeconomic Data and Applications Center (SEDAC) Global Roads Open Access Data Set (Version 1). 5.4.1 Description NASA SEDAC Roads Data The NASA SEDAC global data includes the locations of major roads (i.e., lines indicating roadway center lines) for the US in 2005. Major roads are categorized based on social and economic importance as follows: Major road classification Description Highways Limited access divided highways connecting major cities. Primary roads Other primary major roads between and into major cities as well as primary arterial roads. Secondary roads Other secondary roads between and into cities as well as secondary arterial roads. Other types of roads, such as tertiary roads, local roads, trails, and private roads, are not included. The following figure illustrates the spatial coverage (all US states and territories) and spatial scale (lines) of the SEDAC roads data: Illustration of NASA SEDAC major roads data (a) spatial coverage (United States, including Alaska, Hawaii, and US territories (not shown)) and (b) spatial scale (lines). Exposure Metrics This tool calculates proximity-based (i.e., distance-based) exposure metrics for a specified list of receptor point locations (e.g., geocoded home addresses) to major roads in year 2005. This tool can be used to calculate the following proximity-based metrics within the US: Distance to nearest major road and classification of nearest major road Length of road within a specified buffer distance of receptor These proximity-based metrics can be calculated for all available major road classifications (i.e., highways, primary roads, and secondary roads) or any subset of them. Output includes information about data missingness (e.g., whether a receptor location is near a US border) as well as an optional log file. Recommended Uses This tool is recommended for the following uses: Applications for which a proximity-based metric is appropriate. Note: This tool does not provide other relevant exposure information associated with roads, such as traffic, noise levels, or air pollution levels. Analyses focused on exposures related specifically to major roads. Note: This tool does not include data for other road classifications, such as local street networks or trails. Applications for which most receptor point locations are not located in communities with sections of tunneled or elevated highways. Note: This tool does not provide information about whether roads are at surface level (e.g., elevated, tunneled, etc.). Exposure implications of roadway proximity may differ depending on whether road is at surface level. Some urban highways have varying tunneled, surface-level, or elevated sections (e.g., tunneled sections of US Interstate 90 in Boston, Massachusetts, and in Seattle, Washington). Applications for which most receptor point locations are not located near to a US border with Mexico or Canada. Note: Because this tool does not include roadway data for Mexico or Canada, the tool may under predict proximity to major roads for receptor point locations in the US near a border with Mexico or Canada with nearby major roads across the border. This tool provides optional output information indicating whether a receptor point is located within a specified distance of a border. Steps Install R and required packages Download tool Prepare receptor point data Run script in R Review output Cite data and tool 5.4.2 Install R and required packages Install R. Optionally, install RStudio. Then, install the following R packages: logr, tidyverse, sf. Follow R package installation instructions, or run the following code in R: install.packages(c(&quot;logr&quot;, &quot;tidyverse&quot;, &quot;sf&quot;)) 5.4.3 Download Tool Download and save the folder containing input data (input_source_major_roads.rds and input_us_borders.rds) and script (script_major_road_proximity_for_points.R). To directly run the example scripts provided with these instructions in Step 4, do not change the file names within the folder. 5.4.4 Prepare Receptor Point Data Prepare a comma-separated values (CSV) file that contains a table of the receptor point locations (e.g., geocoded addresses, coordinates). Include each receptor as a separate row in the table, and include the following required columns: id: a unique and anonymous identifying code for each receptor. This can be in character (string) or numeric (double) format latitude: the latitude of the receptor point location in decimal degrees format (range: -90 to 90) longitude: the longitude of the receptor point location in decimal degrees format (range: -180 to 180) The following table provides an example of the receptor point data format: id latitude longitude 1011A 39.00205369 -77.105578716 1012C 35.88480215 -78.877942573 1013E 39.43560788 -77.434847823 To directly run the example scripts provided with these instructions, save the receptor point data as input_receptor.csv in the folder. 5.4.5 Run script in R Run the script script_major_road_proximity_for_points.R to load the required functions in R. You can then use the function get_major_road_proximity_for_points() to calculate proximity-based exposure metrics for each receptor point location. Description of Function get_major_road_proximity_for_points() This function takes the receptor point data above and returns a data frame with the receptor identifying code linked to the selected major road facility proximity metrics for selected raod class(es) as well as information about data missingness. Optionally, the function also writes a log file in the current R working directory. The function has the following arguments: Required Arguments receptor_filepath: specifies the file path to a CSV file containing the receptor point locations (described in Step 3). Note: The format for file paths in R can vary by operating system. source_major_roads_filepath: specifies the file path to a RDS file containing a simple features object with the line locations of NASA SEDAC major roads in the US. This is the file input_source_major_roads.rds. us_borders_filepath: specifies the file path to a RDS file containing a simple features object with the US borders with Mexico and Canada. This is the file input_us_borders.rds. Optional Arguments buffer_distance_km: a numeric argument that specifies the buffer distance (units: kilometers [km]) to use in calculation of buffer-based proximity metrics. Default is 1 km. Must be between 0.001 km and 1000 km. Note: Larger buffer distance values may result in longer run-times for buffer-based proximity metrics. receptor_crs: a coordinate reference system object (i.e., class is crs object in R) for the receptor point locations. Default is \"EPSG:4269\" (i.e., NAD83). projection_crs: a projected coordinate reference system object (i.e., class is crs object in R) for use in exposure assessment. Default is \"ESRI:102008\" (i.e., North America Albers Equal Area Conic projection). road_class_selection: list that specifies the subset of major road types to include in the exposure assessment. Default is all types: \"highway\", \"primary road\", \"secondary road\", \"unspecified\". proximity_metrics: list that specifies the subset of proximity-based exposure metrics to calculate. Default is all metrics: \"distance_to_nearest, \"length_in_buffer\". \"distance_to_nearest\": returns output with distance to nearest major road (units: km) and classification of nearest major road (e.g., highway) for each receptor \"length_in_buffer\": returns output with the length (units: km) of all major roads of the selected class(es) within the specified buffer distance of receptor check_near_us_border: logical argument that specifies whether the function should identify receptor points that are within the buffer distance (i.e., specified by argument buffer_distance_km) of a US border with Canada or Mexico. TRUE returns a column with output (within_border_buffer) with a binary variable indicating receptor points within the buffer distance of a border. Default is TRUE. Note: This tool includes only road data for US states and territories. Thus, this tool may under predict proximity to major roads for receptor locations near a US border with Canada or Mexico. add_all_input_to_output: logical argument that specifies whether the output of the function should include all columns included with the input receptor data frame or not. TRUE returns all columns (i.e., including latitude and longitude) with output. FALSE returns only the anonymous receptor identifying code, proximity-based metrics, and data missingness information with output. FALSE may be useful for meeting data de-identification requirements. Default is TRUE. write_log_to_file: logical argument that specifies whether a log should be written to file. TRUE will create a log file in the current working directory. Default is TRUE. print_log_to_console: logical argument that specifies whether a log should be printed to the console. TRUE will print a log to console. Default is TRUE. Example Use Below are two example scripts for using the function above to produce a CSV file with the proximity-based exposure estimates for each receptor to highways (using default options for all other optional arguments). The first example uses only R but requires editing the file paths. The second example requires RStudio and the here package but does not require editing file paths. Example 1: Base R # Load packages library(tidyverse) library(logr) library(sf) # Load functions source(&quot;/set/file/path/to/script_major_road_proximity_for_points.R&quot;) # Get proximity-based exposures major_road_proximity_metrics &lt;- get_major_road_proximity_for_points( receptor_filepath = &quot;/set/file/path/to/input_receptor.csv&quot;, source_major_roads_filepath = &quot;/set/file/path/to/input_source_major_roads.rds&quot;, us_borders_filepath = &quot;/set/file/path/to/input_us_borders.rds&quot;, road_class_selection = &quot;highway&quot; ) # Write exposures to CSV readr::write_csv(major_road_proximity_metrics, file = &quot;/set/file/path/to/output_major_road_proximity_metrics.csv&quot; ) Example 2: RStudio with here Package # Install here package (if needed) install.packages(&quot;here&quot;) # Load packages library(here) library(tidyverse) library(logr) library(sf) # Set location here::i_am(&quot;script_major_road_proximity_for_points.R&quot;) # Load functions source(here::here(&quot;script_major_road_proximity_for_points.R&quot;)) # Get exposures major_road_proximity_metrics &lt;- get_major_road_proximity_for_points( receptor_filepath = here(&quot;input_receptor.csv&quot;), source_major_roads_filepath = here(&quot;input_source_major_roads.rds&quot;), us_borders_filepath = here(&quot;input_us_borders.rds&quot;), road_class_selection = &quot;highway&quot; ) # Write exposures to CSV readr::write_csv(major_road_proximity_metrics, file = here(&quot;output_major_road_proximity_metrics.csv&quot;) ) 5.4.6 Review Output Log File After running the example script above, with the log file option selected, the log file will be available in the folder log in the current R working directory. Output Data After running the example script above, calculated proximity-based exposure metrics for receptor locations will be available in the file output_major_road_proximity_metrics.csv within the folder. This CSV file includes a row for each receptor with the following columns (as applicable): Identifiers id: the unique and anonymous identifying code for each receptor Calculated Proximity-Based Exposure Metrics Nearest Distance Metrics major_road_nearest_distance_km: distance (units: km) to the nearest major road major_road_nearest_road_class: the classification of the nearest major road segment. Length in Buffer Metrics major_road_length_in_buffer_km: length (units: km) of all major roads of the specified class(es) within the specified buffer distance of receptor. 0 indicates that no major roads are within the specified buffer distance of the receptor. Information on Data Missingness within_border_buffer: binary variable indicating whether receptor point is located within the buffer distance (i.e., specified by argument buffer_distance_km) of a US border with Canada or Mexico: 1 indicates that receptor point is located within the buffer distance of a US border with Canada or Mexico. This indicates that the proximity-based metrics calculated by this tool may represent under predictions of the true proximity-based metrics (i.e., the nearest major road may be located in Canada or Mexico, outside the coverage of the major road data included in this tool). 0 indicates that receptor point is not located within the buffer distance of a US border with Canada or Mexico. 5.4.7 Cite Data and Tool Please cite the following in any publications based on this tool: Major Roads Data: Center for International Earth Science Information Network - CIESIN - Columbia University, and Information Technology Outreach Services - ITOS - University of Georgia. (2013). Global Roads Open Access Data Set, Version 1 (gROADSv1). Palisades, New York: NASA Socioeconomic Data and Applications Center (SEDAC). [Available: https://doi.org/10.7927/H4VD6WCT.] Accessed October 24, 2022. US Borders: Homeland Infrastructure Foundation-Level Data (HIFLD) Geoplatform. Canada and US border geospatial data. [Available: https://hifld-geoplatform.opendata.arcgis.com/datasets/geoplatform::canada-and-us-border/about]. Accessed: June 23, 2022. Homeland Infrastructure Foundation-Level Data (HIFLD) Geoplatform. Mexico and US border geospatial data. [Available: https://hifld-geoplatform.opendata.arcgis.com/datasets/geoplatform::mexico-and-us-border/about]. Accessed: June 23, 2022. NIEHS Geospatial Toolbox: Citation to be determined. "],["case-studies.html", "Case Studies", " Case Studies This unit provides example case studies that analyze integrated wildfire-related data with health outcomes data. "],["chapter-case-ma-pm25.html", "6 Air Quality, Income, and Asthma 6.1 Introduction 6.2 Data Preparation and Integration 6.3 Data Exploration and Vizualization", " 6 Air Quality, Income, and Asthma Integrating and Analyzing Air Quality, Income and Asthma-Related Emergency Department Visits with Synthetic Patient Data in R Date Modified: February 21, 2024 Authors: Sue Nolte, Lara P. Clark, Charles Schmitt Key Terms: Exposure, Data Integration, Social Determinants of Health Programming Languages: R, Python 6.1 Introduction This case uses synthetic patient data to demonstrate a data integration pipeline for a study of asthma-related emergency department visits, air pollution, and social determinants of health. This case links air pollution data to the synthetic patients’ geocoded home addresses during shorter-term time periods (days) before asthma-related emergency department visits. 6.2 Data Preparation and Integration 6.2.1 Synthetic Patient Data Preparation Data set is from Synthetic patient and population health data from the state of Massachusetts. Download complete patients FHIR file from https://synthea.mitre.org/downloads 22G zip tar Developed python fhir_dataset.py to pull the patients information : patientid, lat, lon, birthdate, gender , start_date_time, end_date_time, code, code_display, start_date, end_date Total patients: 1 million patients sample result: ran synthea_1m_fhir_3_0_May_24/output_12 code =183478001 output file : patient_encounts_bycode_v2_12.csv QC dataset. for instance : start date &gt; end date ; misplace birth/death date to hospital stay date 6.2.2 Air Quality Data Linkage Using the start/end date from patients file run DeGauss pm2.5 model https://degauss.org/pm/ input file for pm2.5 from step 2 command line:docker run –rm -v $PWD:/tmp ghcr.io/degauss-org/pm:0.2.0 tmp/yourdatafile.csv The following columns been added for each row: pm_pred : predicted PM2.5 (micrograms per cubic meter) pm_se : standard error for predicted PM2.5 Output file: patient_encounts_bycode_v2_12_pm_0.2.0.csv The output file will contain one row per day between start_date and end_date for everyone. Lat(latitude) and lon (longitude) location. This means that the output file will likely contain many more rows than the input file 6.2.3 Census Geoid Linkage Run deGUAUSS census block group : https://degauss.org/census_block_group/ Produced a file with additional columns: • census_block_group_id_2010: identifier for 2010 block group • census_tract_id_2010: identifier for 2010 tract Output file: patient_encounts_bycode_v2_12_pm_0.2.0_census_block_group_0.6.0_2010.csv rename to:ms_patient_pm_census_v2.csv 6.3 Data Exploration and Vizualization 6.3.1 Mapping Linked Air Pollution Data library(readr) df &lt;- read_csv(&quot;./dataset/ms_patient_pm_census_v2.csv&quot;, show_col_types = FALSE) dim(df) ## [1] 2979 19 colnames(df) ## [1] &quot;patientid&quot; &quot;lat&quot; ## [3] &quot;lon&quot; &quot;birthdate&quot; ## [5] &quot;gender&quot; &quot;start_date_time&quot; ## [7] &quot;end_date_time&quot; &quot;code&quot; ## [9] &quot;code_display&quot; &quot;start_date&quot; ## [11] &quot;end_date&quot; &quot;date&quot; ## [13] &quot;year&quot; &quot;h3&quot; ## [15] &quot;h3_3&quot; &quot;pm_pred&quot; ## [17] &quot;pm_se&quot; &quot;census_block_group_id_2010&quot; ## [19] &quot;census_tract_id_2010&quot; Install packages: if (!require(&quot;shiny&quot;) || !require(&quot;tidycensus&quot;) || !require(&quot;tidyverse&quot;) || !require(&quot;viridis&quot;)) { install.packages(c(&quot;shiny&quot;, &quot;tidycensus&quot;, &quot;tidyverse&quot;, &quot;viridis&quot;)) } ## Loading required package: shiny ## Loading required package: tidycensus ## Loading required package: tidyverse ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ purrr 1.0.1 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.2 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors ## Loading required package: viridis ## ## Loading required package: viridisLite if (!require(&quot;plotly&quot;)) { install.packages(&quot;plotly&quot;, type = &quot;source&quot;) } ## Loading required package: plotly ## ## Attaching package: &#39;plotly&#39; ## ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## ## The following object is masked from &#39;package:graphics&#39;: ## ## layout if (!require(&quot;ggplot2&quot;)) { install.packages(&quot;ggplot2&quot;) } if (!require(&quot;maps&quot;) || !require(&quot;ggmap&quot;)) { install.packages(c(&quot;maps&quot;, &quot;ggmap&quot;)) } ## Loading required package: maps ## ## Attaching package: &#39;maps&#39; ## The following object is masked from &#39;package:viridis&#39;: ## ## unemp ## The following object is masked from &#39;package:purrr&#39;: ## ## map ## Loading required package: ggmap ## ℹ Google&#39;s Terms of Service: &lt;https://mapsplatform.google.com&gt; ## ℹ Please cite ggmap if you use it! Use `citation(&quot;ggmap&quot;)` for details. ## ## Attaching package: &#39;ggmap&#39; ## ## ## The following object is masked from &#39;package:plotly&#39;: ## ## wind Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. Load Libraries Data wrangling: # Check for missing values sum(is.na(df)) ## [1] 514 # If there are missing values, you can drop or fill them as per your requirement df &lt;- na.omit(df) # Drop rows with missing values Change column name for later map: colnames(df)[which(names(df) == &quot;lon&quot;)] &lt;- &quot;longitude&quot; colnames(df)[which(names(df) == &quot;lat&quot;)] &lt;- &quot;latitude&quot; dim(df) ## [1] 2722 19 Filter the rows based on the date range Use the subset() function to filter the rows based on the date range:2010 -2017to match 5 year sensus data filtered_df &lt;- subset(df, date &gt;= as.Date(&quot;2012-01-01&quot;) &amp; date &lt;= as.Date(&quot;2017-09-01&quot;)) dim(filtered_df) ## [1] 2124 19 write.csv(filtered_df, &quot;./dataset/asthma_small.csv&quot;) Combine information for display in tooltip # Combine the required information for hover tooltip into a new column filtered_df$tooltip &lt;- paste( &quot;PM2.5:&quot;, filtered_df$pm_pred, &quot;&lt;br&gt;&quot;, &quot;Hospital Date:&quot;, filtered_df$date, &quot;&lt;br&gt;&quot;, &quot;Patient Info:&quot;, filtered_df$code_display, &quot;&lt;br&gt;&quot;, &quot;gender :&quot;, filtered_df$gender ) Create Map Load the required libraries for plotting maps: library(ggplot2) library(maps) library(ggmap) Add map data to the base plot using the map_data() function: # Draw massachusetts map ma_map &lt;- map_data(&quot;state&quot;, region = &quot;massachusetts&quot;) # Create a base plot for Massachusetts p &lt;- ggplot() + geom_polygon( data = ma_map, aes(x = long, y = lat, group = group), fill = &quot;lightgray&quot;, color = &quot;black&quot; ) Plotting the Data Points Plot the data points on the map using the geom_point() function. Specify the longitude and latitude coordinates, and optionally, the color or size of the data points based on the PM2.5 values: Add PM2.5 data points with hover information p &lt;- p + geom_point(data = filtered_df, aes( x = longitude, y = latitude, color = pm_pred, text = tooltip ), size = 3) ## Warning in geom_point(data = filtered_df, aes(x = longitude, y = latitude, : ## Ignoring unknown aesthetics: text # you can run this block code on your local to have mouse hover text. # for Now it cannot # rander on R-studio connect # Convert the plot to a plotly object run &lt;- FALSE if (run == TRUE) { p &lt;- ggplotly(p) %&gt;% layout( hoverlabel = list(bgcolor = &quot;white&quot;), hovertemplate = paste( &quot;&lt;b&gt;%{text}&lt;/b&gt;&quot; ) ) } Draw the map print(p) 6.3.2 Mapping Linked Census Income Data Data Preparation Data set is from Synthetic patient and population health data from the state of Massachusetts Download complete patients FHIR file from https://synthea.mitre.org/downloads 22G zip tar Developed python fhir_dataset.py to pull the patients information : patientid, lat, lon, birthdate, gender , start_date_time, end_date_time, code, code_display, start_date, end_date Total patients: 1 million patients sample result: ran synthea_1m_fhir_3_0_May_24/output_12 code =183478001 output file : patient_encounts_bycode_v2_12.csv QC dataset. for instance : start date &gt; end date ; misplace birth/death date to hospital stay date Using the start/end date from patients file run DeGauss pm2.5 model https://degauss.org/pm/ input file for pm2.5 from step 2 command line:docker run –rm -v $PWD:/tmp ghcr.io/degauss-org/pm:0.2.0 tmp/yourdatafile.csv The following columns been added for each row: pm_pred : predicted PM2.5 (micrograms per cubic meter) pm_se : standard error for predicted PM2.5 Output file: patient_encounts_bycode_v2_12_pm_0.2.0.csv The output file will contain one row per day between start_date and end_date for everyone. Lat(latitude) and lon (longitude) location. This means that the output file will likely contain many more rows than the input file Run deGUAUSS census block group : https://degauss.org/census_block_group/ Produced a file with additional columns: • census_block_group_id_2010: identifier for 2010 block group • census_tract_id_2010: identifier for 2010 tract • command line:docker run –rm -v $PWD:/tmp ghcr.io/degauss-org/census_block_group:0.5.0 tmp/yourdatafile.csv Output file: yourdatafile_census_block_group_0.6.0_2010.csv rename to:ms_patient_pm_census_v2.csv install packages if (!require(&quot;shiny&quot;) || !require(&quot;tidycensus&quot;) || !require(&quot;tidyverse&quot;) || !require(&quot;viridis&quot;)) { install.packages(c(&quot;shiny&quot;, &quot;tidycensus&quot;, &quot;tidyverse&quot;, &quot;viridis&quot;)) } if (!require(&quot;plotly&quot;)) { install.packages(&quot;plotly&quot;, type = &quot;source&quot;) } if (!require(&quot;ggplot2&quot;)) { install.packages(&quot;ggplot2&quot;) install.packages(c(&quot;maps&quot;, &quot;ggmap&quot;)) } if (!require(&quot;maps&quot;) || !require(&quot;ggmap&quot;)) { install.packages(c(&quot;maps&quot;, &quot;ggmap&quot;)) } Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. ### Load libraries library(readr) df &lt;- read_csv(&quot;./dataset/ms_patient_pm_census_v2.csv&quot;, show_col_types = FALSE) dim(df) ## [1] 2979 19 colnames(df) ## [1] &quot;patientid&quot; &quot;lat&quot; ## [3] &quot;lon&quot; &quot;birthdate&quot; ## [5] &quot;gender&quot; &quot;start_date_time&quot; ## [7] &quot;end_date_time&quot; &quot;code&quot; ## [9] &quot;code_display&quot; &quot;start_date&quot; ## [11] &quot;end_date&quot; &quot;date&quot; ## [13] &quot;year&quot; &quot;h3&quot; ## [15] &quot;h3_3&quot; &quot;pm_pred&quot; ## [17] &quot;pm_se&quot; &quot;census_block_group_id_2010&quot; ## [19] &quot;census_tract_id_2010&quot; Get Census Income Data Apply census api key at https://api.census.gov/data/key_signup.html and then supply the key to the census_api_key() function to use it throughout your tidycensus session. Run r package tidycensus with the api key and the parameters: • income code = “C17002_001”,“C17002_002”,“C17002_003”,“C17002_004”, “C17002_005”,“C17002_006”,“C17002_007”, “C17002_008” • geography = tract • year = 2012 • state = MA • survey = acs5 Create a character vector named income_code containing eight elements. Each element represents a variable code related to income. income_code &lt;- c( &quot;C17002_001&quot;, &quot;C17002_002&quot;, &quot;C17002_003&quot;, &quot;C17002_004&quot;, &quot;C17002_005&quot;, &quot;C17002_006&quot;, &quot;C17002_007&quot;, &quot;C17002_008&quot; ) # Assign the result of the get_acs function to the variable tarr. The function retrieves # American Community Survey (ACS) data for the specified geography (tract), variables # (the income_code vector), state (Massachusetts with state code 25), geometry (TRUE to # include spatial information), survey (&quot;acs5&quot;), and year (2012). tarr &lt;- get_acs( geography = &quot;tract&quot;, variables = income_code, state = 25, geometry = FALSE, survey = &quot;acs5&quot;, year = 2012 ) ## Getting data from the 2008-2012 5-year ACS ## Warning: • You have not set a Census API key. Users without a key are limited to 500 ## queries per day and may experience performance limitations. ## ℹ For best results, get a Census API key at ## http://api.census.gov/data/key_signup.html and then supply the key to the ## `census_api_key()` function to use it throughout your tidycensus session. ## This warning is displayed once per session. # Write the census income data into csv file # modify if (FALSE) to if (TRUE) if you run the first time if (FALSE) { write.csv(tarr, &quot;./dataset/income_MA_mult_incomes.csv&quot;) } tarr column name : • Id – census track id • geoid, • name, • variable, • estimate, • moe Join the patient’s information + degauss pm 2.5 + census block dataset with census income dataset by census track id Data join output file : asthma_patients_pm_2_census_block_group_0.6.0_2010.csv dim(tarr) ## [1] 11824 5 colnames(tarr) ## [1] &quot;GEOID&quot; &quot;NAME&quot; &quot;variable&quot; &quot;estimate&quot; &quot;moe&quot; Link Census Income Data by Tract Geoid df column “census_block_group_id_2010” and tarr column “GEOID” are the census track id. Join these two data frames library(dplyr) merge &lt;- FALSE if (merge == TRUE) { merged_frame &lt;- df %&gt;% mutate(census_tract_id_2010 = as.character(census_tract_id_2010)) %&gt;% inner_join(tarr, by = c(&quot;GEOID&quot; = &quot;census_tract_id_2010&quot;)) } # left join produce the same result as inner_join left_merged_frame &lt;- df %&gt;% mutate(census_tract_id_2010 = as.character(census_tract_id_2010)) %&gt;% left_join(tarr, by = c(&quot;census_tract_id_2010&quot; = &quot;GEOID&quot;)) ## Warning in left_join(., tarr, by = c(census_tract_id_2010 = &quot;GEOID&quot;)): Detected an unexpected many-to-many relationship between `x` and `y`. ## ℹ Row 1 of `x` matches multiple rows in `y`. ## ℹ Row 11769 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## &quot;many-to-many&quot;` to silence this warning. # Write the merged frame data into csv file # modify if (FALSE) to if (TRUE) if you run the first time if (FALSE) { write.csv(left_merged_frame, &quot;./dataset/merged_income_MA_mult_incomes.csv&quot;) } library(dplyr) Identify County with Hightest Income in 2020 # Fetch ACS data for Massachusetts counties ma_counties &lt;- get_acs( geography = &quot;county&quot;, variables = &quot;B19013_001&quot;, state = &quot;MA&quot;, year = 2020 ) ## Getting data from the 2016-2020 5-year ACS # Sort the data by median household income in descending order ma_counties_sorted &lt;- ma_counties %&gt;% arrange(desc(estimate)) # Extract the county with the highest income highest_income_county &lt;- ma_counties_sorted$NAME[1] # Print the county with the highest income cat(&quot;The county with the highest income in Massachusetts is:&quot;, highest_income_county) ## The county with the highest income in Massachusetts is: Nantucket County, Massachusetts Map Income and Patient Data if (!require(&quot;maps&quot;) || !require(&quot;ggmap&quot;)) { install.packages(c(&quot;maps&quot;, &quot;ggmap&quot;)) library(maps) library(ggmap) } library(ggplot2) colnames(left_merged_frame) ## [1] &quot;patientid&quot; &quot;lat&quot; ## [3] &quot;lon&quot; &quot;birthdate&quot; ## [5] &quot;gender&quot; &quot;start_date_time&quot; ## [7] &quot;end_date_time&quot; &quot;code&quot; ## [9] &quot;code_display&quot; &quot;start_date&quot; ## [11] &quot;end_date&quot; &quot;date&quot; ## [13] &quot;year&quot; &quot;h3&quot; ## [15] &quot;h3_3&quot; &quot;pm_pred&quot; ## [17] &quot;pm_se&quot; &quot;census_block_group_id_2010&quot; ## [19] &quot;census_tract_id_2010&quot; &quot;NAME&quot; ## [21] &quot;variable&quot; &quot;estimate&quot; ## [23] &quot;moe&quot; # Draw Massachusetts map ma_map &lt;- map_data(&quot;state&quot;, region = &quot;massachusetts&quot;) # modify column&#39;s name colnames(left_merged_frame)[which(names(left_merged_frame) == &quot;lon&quot;)] &lt;- &quot;longitude&quot; colnames(left_merged_frame)[which(names(left_merged_frame) == &quot;lat&quot;)] &lt;- &quot;latitude&quot; # Optional Filter the rows based on the date range Use the subset() function to # filter the rows based on the date range:2012 -2017to match 5 year sensus data filtered_df &lt;- subset( left_merged_frame, date &gt;= as.Date(&quot;2012-09-01&quot;) &amp; date &lt;= as.Date(&quot;2017-09-01&quot;) ) # Create a base plot for Massachusetts p &lt;- ggplot() + geom_polygon( data = ma_map, aes(x = long, y = lat, group = group), fill = &quot;lightgray&quot;, color = &quot;black&quot; ) + # Plot the data points geom_point( data = filtered_df, aes(x = longitude, y = latitude, color = estimate), size = 3 ) + labs(title = &quot;Income vs patient Map&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + scale_color_gradient(low = &quot;blue&quot;, high = &quot;red&quot;, name = &quot;estimate&quot;) + theme_minimal() + # Adjust the map boundaries to focus on Massachusetts coord_cartesian(xlim = c(-73.5, -69.9), ylim = c(41.2, 42.9)) # Print the map print(p) Visualize Air Pollution and Health Information Using Bivariate Maps Set Up Bivariate Color Classes Each variate is divided into thirds (based on percentiles) and a joint classification for all 9 combinations is defined. require(latticeExtra) # uscancerrates, mapplot ## Loading required package: latticeExtra ## Loading required package: lattice ## ## Attaching package: &#39;latticeExtra&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## layer require(maps) # map require(classInt) # classIntervals, findCols ## Loading required package: classInt require(grid) # viewport, pushViewport ## Loading required package: grid require(pals) # brewer.blues, stevens.pinkgreen ## Loading required package: pals ## ## Attaching package: &#39;pals&#39; ## The following objects are masked from &#39;package:viridis&#39;: ## ## cividis, inferno, magma, plasma, turbo, viridis ## The following objects are masked from &#39;package:viridisLite&#39;: ## ## cividis, inferno, magma, plasma, turbo, viridis From asthma_patients_pm_2_census_block_group_0.6.0_2010.csv run python biv_cho_map.py call fcc api to get county FIPS and Name for each patient https://geo.fcc.gov/api/census/#!/block/get_block_find new data fileasthma_patients_pm_2_census_block_group_0.6.0_2010.csv library(readr) df_county &lt;- read.csv(&quot;./dataset/county.csv&quot;) head(df_county) ## X Unnamed..0 patientid lat lon ## 1 0 1 04a4d323-9115-49d8-b44f-f166c331c948 41.73322 -70.12610 ## 2 1 2 cea474c2-e254-4004-9687-332aba93ad1f 42.71344 -70.83298 ## 3 2 3 c3b51f04-41a8-4794-b195-02a9a6ace417 42.09928 -70.68884 ## 4 3 4 03329b9a-1f93-45a9-b73f-50039543b544 42.32161 -70.92707 ## 5 4 5 086a79cc-7ba9-4c18-ba01-7ee6085ae262 41.70227 -71.09501 ## 6 5 6 43f4fccc-e592-48a4-a513-3b792f9d6220 42.43578 -71.06075 ## start_date_time end_date_time birthdate gender code ## 1 2012-01-23 22:44:49 2012-01-23 22:44:49 1989-05-11 male 183478001 ## 2 2012-05-15 20:33:03 2012-05-15 20:33:03 1965-02-05 male 183478001 ## 3 2012-07-24 07:37:29 2012-07-24 07:37:29 1986-03-11 male 183478001 ## 4 2012-03-14 08:10:33 2012-03-14 08:10:33 1977-03-13 male 183478001 ## 5 2012-01-21 17:22:20 2012-01-21 17:22:20 1986-12-20 male 183478001 ## 6 2012-05-17 21:10:17 2012-05-17 21:10:17 1983-04-28 male 183478001 ## code_display start_date end_date date year ## 1 Emergency hospital admission for asthma 2012-01-23 2012-01-23 2012-01-23 2012 ## 2 Emergency hospital admission for asthma 2012-05-15 2012-05-15 2012-05-15 2012 ## 3 Emergency hospital admission for asthma 2012-07-24 2012-07-24 2012-07-24 2012 ## 4 Emergency hospital admission for asthma 2012-03-14 2012-03-14 2012-03-14 2012 ## 5 Emergency hospital admission for asthma 2012-01-21 2012-01-21 2012-01-21 2012 ## 6 Emergency hospital admission for asthma 2012-05-17 2012-05-17 2012-05-17 2012 ## h3 h3_3 pm_pred pm_se ## 1 882a310e69fffff 832a06fffffffff-832a31fffffffff 9.912325 1.858255 ## 2 882a3001c9fffff 832a30fffffffff 10.397400 3.698060 ## 3 882a315b1bfffff 832a06fffffffff-832a31fffffffff 9.668475 2.096010 ## 4 882a30283dfffff 832a30fffffffff 7.458575 1.795863 ## 5 882a33a143fffff 832a33fffffffff 7.068950 2.226050 ## 6 882a3075c3fffff 832a30fffffffff 10.952550 2.895613 ## census_block_group_id_2010 census_tract_id_2010 county_code county_name ## 1 250010108005 25001010800 25001 Barnstable County ## 2 250092231002 25009223100 25009 Essex County ## 3 250235062031 25023506203 25023 Plymouth County ## 4 250259801011 25025980101 25025 Suffolk County ## 5 250056425002 25005642500 25005 Bristol County ## 6 250173416003 25017341600 25017 Middlesex County df_census_block &lt;- read.csv(&quot;./dataset/asthma_patients_pm_2_census_block_group_0.6.0_2010.csv&quot;) head(df_census_block) ## patientid lat lon start_date_time ## 1 aeeed2fd-b274-4947-b4c4-36e6f8d6fe48 42.03064 -71.49793 2011-04-02T11:32:13Z ## 2 2ff614fb-652d-4f83-89db-f66942a75dd4 42.27744 -71.82262 2010-12-30T02:39:26Z ## 3 7aad32c1-35f2-4522-9a16-08122504f981 42.58764 -72.52155 2010-11-06T04:36:07Z ## 4 f78662b4-ab83-43ae-bdbf-04668d4ba1c5 42.47049 -70.92015 2010-11-07T15:25:16Z ## 5 d9419f62-2212-47f6-9537-854289b1ccaa 42.34944 -73.26774 2011-03-30T16:39:36Z ## 6 ae0265e4-e679-46b7-ad5f-f6fb3a25072d 42.72983 -71.15126 2010-10-19T09:14:56Z ## end_date_time birthdate gender code ## 1 2011-04-02T11:32:13Z 1980-05-22 female 183478001 ## 2 2010-12-30T02:39:26Z 1983-03-29 male 183478001 ## 3 2010-11-06T04:36:07Z 2003-04-15 female 183478001 ## 4 2010-11-07T15:25:16Z 1951-09-11 female 183478001 ## 5 2011-03-30T16:39:36Z 1999-06-25 male 183478001 ## 6 2010-10-19T09:14:56Z 1954-07-24 female 183478001 ## code_display start_date end_date date year ## 1 Emergency hospital admission for asthma 2011-04-02 2011-04-02 2011-04-02 2011 ## 2 Emergency hospital admission for asthma 2010-12-30 2010-12-30 2010-12-30 2010 ## 3 Emergency hospital admission for asthma 2010-11-06 2010-11-06 2010-11-06 2010 ## 4 Emergency hospital admission for asthma 2010-11-07 2010-11-07 2010-11-07 2010 ## 5 Emergency hospital admission for asthma 2011-03-30 2011-03-30 2011-03-30 2011 ## 6 Emergency hospital admission for asthma 2010-10-19 2010-10-19 2010-10-19 2010 ## h3 h3_3 pm_pred pm_se ## 1 882a33c6b1fffff 832a33fffffffff 4.738200 0.8280494 ## 2 882a33d99dfffff 832a33fffffffff 20.799600 5.4792723 ## 3 882a320d31fffff 832a32fffffffff 3.553875 1.1884528 ## 4 882a3076cdfffff 832a30fffffffff 5.397100 1.4094668 ## 5 882a14da25fffff 832a14fffffffff 4.359900 1.0133038 ## 6 882a3046e9fffff 832a30fffffffff 8.406375 2.6272480 ## census_block_group_id_2010 census_tract_id_2010 ## 1 250277471012 25027747101 ## 2 250277308012 25027730801 ## 3 250110407022 25011040702 ## 4 250092021013 25009202101 ## 5 250039131007 25003913100 ## 6 250092525022 25009252502 Get MA County Boundaries Aggregate the data by county, calculating the total number of patients and the mean pm2.5_pred: df_agg &lt;- df_county %&gt;% group_by(county_code) %&gt;% reframe( patient_count = n(), county_name, pm2_mean = mean(pm_pred, na.rm = TRUE) ) 6.3.3 Join Census Data with County Boundaries dim(df_agg) ## [1] 96 4 ma_counties_data &lt;- us_counties_MA %&gt;% left_join(df_agg, by = c(&quot;NAMELSAD&quot; = &quot;county_name&quot;)) head(ma_counties_data) ## Simple feature collection with 6 features and 15 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -72.31582 ymin: 42.00806 xmax: -71.47803 ymax: 42.72156 ## Geodetic CRS: NAD83 ## STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME NAMELSAD ## 1 25 027 00606940 0500000US25027 25027 Worcester Worcester County ## 2 25 027 00606940 0500000US25027 25027 Worcester Worcester County ## 3 25 027 00606940 0500000US25027 25027 Worcester Worcester County ## 4 25 027 00606940 0500000US25027 25027 Worcester Worcester County ## 5 25 027 00606940 0500000US25027 25027 Worcester Worcester County ## 6 25 027 00606940 0500000US25027 25027 Worcester Worcester County ## STUSPS STATE_NAME LSAD ALAND AWATER county_code patient_count ## 1 MA Massachusetts 06 3912627580 177366263 25027 16 ## 2 MA Massachusetts 06 3912627580 177366263 25027 16 ## 3 MA Massachusetts 06 3912627580 177366263 25027 16 ## 4 MA Massachusetts 06 3912627580 177366263 25027 16 ## 5 MA Massachusetts 06 3912627580 177366263 25027 16 ## 6 MA Massachusetts 06 3912627580 177366263 25027 16 ## pm2_mean geometry ## 1 5.862477 MULTIPOLYGON (((-72.31363 4... ## 2 5.862477 MULTIPOLYGON (((-72.31363 4... ## 3 5.862477 MULTIPOLYGON (((-72.31363 4... ## 4 5.862477 MULTIPOLYGON (((-72.31363 4... ## 5 5.862477 MULTIPOLYGON (((-72.31363 4... ## 6 5.862477 MULTIPOLYGON (((-72.31363 4... ggplot() + geom_sf(data = ma_counties_data, aes(fill = patient_count), color = NA) + scale_fill_viridis() + theme_void() + theme(legend.position = &quot;bottom&quot;) + labs(fill = &quot;Patient count&quot;) Set Color on Both Varaibles cols &lt;- stevens.pinkgreen nbins &lt;- 3 # categorize rates into 3 percentile bins brks_pm &lt;- classIntervals( log(ma_counties_data$pm2_mean), n = nbins, style = &quot;quantile&quot; ) ## Warning in classIntervals(log(ma_counties_data$pm2_mean), n = nbins, style = ## &quot;quantile&quot;): var has missing values, omitted in finding classes brks_patient &lt;- classIntervals( log(ma_counties_data$patient_count), n = nbins, style = &quot;quantile&quot; ) ## Warning in classIntervals(log(ma_counties_data$patient_count), n = nbins, : var ## has missing values, omitted in finding classes class_pm &lt;- findCols(brks_pm) print(class_pm) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 NA 3 3 3 3 3 3 3 3 ## [26] 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [51] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 ## [76] 3 3 3 3 3 3 3 NA 2 2 3 3 3 3 1 1 1 1 1 1 1 1 NA 3 class_patient &lt;- findCols(brks_patient) # convert x,y classes into a joint class x+3(y-1) ma_counties_data$class2 &lt;- class_pm + nbins * (class_patient - 1) # scatterplot of two variates showing bins plot(log(pm2_mean) ~ log(patient_count), data = ma_counties_data, col = cols()[ma_counties_data$class2], pch = 19, xlim = c(0, 6), ylim = c(0, 5) ) Map Air Pollution and Patient Counts # Check for missing values sum(is.na(ma_counties_data)) # If there are missing values, you can drop or fill them as per your requirement df &lt;- na.omit(ma_counties_data) # Drop rows with missing values m4 &lt;- mapplot(rownames(ma_counties_data) ~ class2, data = ma_counties_data, colramp = cols, breaks = seq(from = 0.5, by = 1, length = nbins * nbins + 1), xlab = &quot;&quot;, colorkey = FALSE, map = map(&quot;state&quot;, &quot;massachusetts&quot;, plot = FALSE, fill = TRUE, projection = &quot;tetra&quot; ), scales = list(draw = FALSE) ) print(nbins) matrix(1:(nbins * nbins), nrow = nbins) m4leg &lt;- levelplot(matrix(1:(nbins * nbins), nrow = nbins), axes = FALSE, col.regions = cols(), xlab = &quot;patient&quot;, ylab = &quot;pm_2.5&quot;, cuts = 8, colorkey = FALSE, scales = list(draw = 0) ) vp &lt;- viewport(x = .15, y = .25, width = .3, height = .3) pushViewport(vp) print(m4leg, newpage = FALSE) popViewport() suppressWarnings(print(m4)) table(apply(data_sample, 1, paste, collapse = &quot;&quot;)) m4leg &lt;- levelplot(matrix(1:(nbins * nbins), nrow = nbins), axes = FALSE, col.regions = cols(), xlab = &quot;patient&quot;, ylab = &quot;pm_2.5&quot;, cuts = 8, colorkey = FALSE, scales = list(draw = 0) ) # add the color legend m4leg &lt;- levelplot(matrix(1:(nbins * nbins), nrow = nbins), axes = FALSE, col.regions = cols(), xlab = &quot;patient&quot;, ylab = &quot;pm_2.5&quot;, cuts = 8, colorkey = FALSE, scales = list(draw = 0) ) vp &lt;- viewport(x = .15, y = .25, width = .3, height = .3) pushViewport(vp) print(m4leg, newpage = FALSE) popViewport() "],["appendix.html", "Appendix", " Appendix The following are included as appendices: User profile descriptions CHORDS glossary Data dictionary for NIEHS PEGS "],["chapter-user-profiles.html", "A1 User Profiles Clinical Data Manager Clinical Researcher Clinician/Medical Professional Community Health Worker Educator Epidemiologist Geospatial Analyst Public Health Official Social &amp; Behavioral Scientist Student Translational Researcher", " A1 User Profiles The CHORDS Toolkit chapters have been developed for various user profiles. This appendix describes the user profiles and suggests relevant chapters for each. Clinical Data Manager What is my area of expertise? Advanced knowledge of health data systems Familiarity with some data sources for exposure and sociodemographic information Advanced data collection, management, and analysis skills What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Readily computed sociodemographic metrics Exposure and sociodemographic measures that are directly comparable with related measures from other studies Relevant Chapters NASA EarthData Download Linkage to Exposures Clinical Researcher What is my area of expertise? In-depth knowledge of health data systems Background knowledge of the key exposures and risk factors that influence the mechanism of the health outcome General knowledge of the main data sources for exposure, health, and sociodemographic information Advanced data collection, management, and analysis skills Statistical methods to examine population health trends What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Exposure measures that are directly comparable with related measures from other studies Methods to integrate measures across different spatial geographies Methods to readily assess known and predicted impacts of exposures on human health Clinician/Medical Professional What is my area of expertise? Advanced knowledge of health conditions and mechanisms of disease Background knowledge of key exposures and risk factors that influence the mechanism of the health outcome Familiarity with some data sources for exposure and sociodemographic information Advanced knowledge of health data systems What resources might I need? Readily computed exposure metrics Readily computed sociodemographic metrics Health data that capture information on the diseases or conditions of interest Exposure, sociodemographic, and health measures that are directly comparable with related measures from other studies Methods for collecting, managing, and analyzing data Strategies for interpreting analytical results Relevant Chapters Spatial Data Analysis Linkage to Census Units Linkage to Exposures Community Health Worker What is my area of expertise? Familiarity with general exposures and related health outcomes of interest Familiarity with some data sources for exposure, health, and sociodemographic information Communication of important health information to the public and other stakeholders What resources might I need? Readily computed exposure metrics Readily computed sociodemographic metrics Health data that capture information on the diseases or conditions of interest Key findings or strategies for interpreting analytical results Data visualizations and other related tools Educator What is my area of expertise? In-depth knowledge of health data systems Familiarity with some data sources for exposure, health, and sociodemographic information Advanced data collection, management, and analysis skills What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Readily computed sociodemographic metrics Exposure and sociodemographic measures that are directly comparable with related measures from other studies Relevant Chapters NASA EarthData Download Epidemiologist What is my area of expertise? Background knowledge of the key risk factors that influence the mechanism of the health outcome General knowledge of the main sources of data for exposures and health information Fundamental data collection, management, and analysis skills Statistical methods to examine population health trends and patterns Interpretation of analytical findings for broader public health applications What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Exposure measures that are directly comparable with related measures from other studies Health data that capture information on the disease or condition of interest Methods to integrate measures across different spatial geographies Methods to readily identify known and predicted impacts of exposures on human health Geospatial Analyst What is my area of expertise? Background knowledge of the key sources and patterns of exposure of interest General knowledge of the main sources of geospatial data for exposures Advanced data collection, management, and analysis skills Characterization and assessment of exposures What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Exposure measures that are directly comparable with related measures from other studies Methods to integrate measures across different spatial geographies Methods to calculate and predict exposures from multiple sources and time periods Relevant Chapters NASA EarthData Public Health Official What is my area of expertise? Familiarity with the exposures and typical health outcomes of interest Familiarity with general data collection, management, and analysis skills Communication of important health information to the public and other stakeholders What resources might I need? Readily computed exposure metrics Readily computed sociodemographic metrics Health data that capture information on the diseases or conditions of interest Key findings or strategies for interpreting analytical results Data visualizations and other related tools Social &amp; Behavioral Scientist What is my area of expertise? Background knowledge of the key exposures and risk factors that influence the mechanism of the health outcome General knowledge of the main data sources for exposure, health, and sociodemographic information Advanced data collection, management, and analysis skills Statistical methods to examine population health trends Interpretation of analytical findings for broader public health applications What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Exposure measures that are directly comparable with related measures from other studies Health data that capture information on the disease or condition of interest Methods to integrate measures across different spatial geographies Methods to readily identify known and predicted impacts of exposures on human health Relevant Chapters Spatial Data Analysis Student What is my area of expertise? Background knowledge of the typical health outcomes of interest and key exposures and risk factors influencing these health outcomes General knowledge of the main data sources for exposure, health, and sociodemographic information Fundamental data collection, management, and analysis skills Familiarity with statistical methods to examine population health trends What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Exposure measures that are directly comparable with related measures from other studies Health data that capture information on the disease or condition of interest Methods to integrate measures across different spatial geographies Methods for collecting, managing, and analyzing data Relevant Chapters Spatial Data Analysis Linkage to Census Units Linkage to Exposures Translational Researcher What is my area of expertise? Background knowledge of the key exposures and risk factors that influence the mechanism of the health outcome General knowledge of the main data sources for exposure, health, and sociodemographic information Advanced data collection, management, and analysis skills Statistical methods to examine population health trends Translating analytical findings into interventions and treatments What resources might I need? High spatial and temporal resolution exposure data Readily computed exposure metrics Exposure measures that are directly comparable with related measures from other studies Health data that capture information on the disease or condition of interest Methods to integrate measures across different spatial geographies Methods to readily assess known and predicted impacts of exposures on human health "],["chords-glossary.html", "A2 CHORDS Glossary", " A2 CHORDS Glossary Key terms used in the CHORDS Toolkit are defined below and in the NIEHS Climate Change and Human Health Glossary. Geocoded Address The geographic coordinates (i.e., latitude and longitude) associated with a street address (e.g., home or work location). Context: Geocoded addresses are commonly used as a basis for geospatial exposure assessment for individuals. Categories: Research Methods Related Terms: Geocoding Geocoding The process of converting place names or street addresses to geographic coordinates (i.e., latitude and longitude). Context: Geocoding is a common data processing step in geospatial exposure assessment. Categories: Research Methods Related Terms: Geocoded Address Geographic Unit (Areal Unit) A specific place defined by administrative or political geographic boundaries (e.g., state, census tract, country, postal code), environmental boundaries (e.g., watershed) or a grid (e.g., grid cell in a 1 km grid). Context: Data for social and environmental determinants of health are often provided for different types of geographic units (e.g., census tracts, grid cells). Categories: Research Methods Geoid A unique identifying code for a place or geographic unit (e.g., census tract, city, postal code, land parcel, watershed). Context: Geoid is commonly used as a basis for data integration for individual-level or population-level analyses Categories: Research Methods Related Terms: Geocoding, Data Integration Geospatial Data Data that includes information about specific times and places on the Earth. Context: Geospatial data can be used to assess individual and population level exposures to environmental and social determinants of health. Categories: Research Methods "],["a3-pegs-data-dictionary.html", "A3 PEGS Data Dictionary", " A3 PEGS Data Dictionary [Page Note: add links to catalog for sources, add links to tools for tools] The following table provides a list of geospatial-based exposure measures used in the NIEHS Personalized Environment and Genes Study, including their sources and links to tools to use to compute these measures. The sources include: TRI = EPA Toxic Release Inventory ACAG = Atmospheric Composition Analysis Group CACES = Center for Air, Climate, &amp; Energy Solutions FAA = Federal Aviation Administration NCDEQ = North Carolina Department of Environmental Quality FCC = Federal Communications Commission NRC = Nuclear Regulatory Commission DOT = Department of Transportation EJI = Environmental Justice Index Merra2 = NASA Global Modeling and Assimilation Office variable_name description variable_source gis_latitude Latitude coordinate Geocoding of subject data hazards_pm25_ammonium_ugm3 ACAG estimated PM2.5 conponent: ammonium ion concentration (micrograms per cubic meter) ACAG hazards_benzene_1km Estimated xylene mass in kg using an isotropic sum of exponentially decaying mass equation with an exponential decay range of 1km (see Eq 1). TRI "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
